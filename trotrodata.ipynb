{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be0a12cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opendatasets in c:\\users\\cbotc\\repo\\trotronotebook\\.venv\\lib\\site-packages (0.1.22)\n",
      "Requirement already satisfied: pandas in c:\\users\\cbotc\\repo\\trotronotebook\\.venv\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\cbotc\\repo\\trotronotebook\\.venv\\lib\\site-packages (2.3.2)\n",
      "Requirement already satisfied: legacy-cgi in c:\\users\\cbotc\\repo\\trotronotebook\\.venv\\lib\\site-packages (2.6.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\cbotc\\repo\\trotronotebook\\.venv\\lib\\site-packages (from opendatasets) (4.67.1)\n",
      "Requirement already satisfied: kaggle in c:\\users\\cbotc\\repo\\trotronotebook\\.venv\\lib\\site-packages (from opendatasets) (1.7.4.5)\n",
      "Requirement already satisfied: click in c:\\users\\cbotc\\repo\\trotronotebook\\.venv\\lib\\site-packages (from opendatasets) (8.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\cbotc\\repo\\trotronotebook\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\cbotc\\repo\\trotronotebook\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\cbotc\\repo\\trotronotebook\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\cbotc\\repo\\trotronotebook\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\cbotc\\repo\\trotronotebook\\.venv\\lib\\site-packages (from click->opendatasets) (0.4.6)\n",
      "Requirement already satisfied: bleach in c:\\users\\cbotc\\repo\\trotronotebook\\.venv\\lib\\site-packages (from kaggle->opendatasets) (6.2.0)\n",
      "Requirement already satisfied: certifi>=14.05.14 in c:\\users\\cbotc\\repo\\trotronotebook\\.venv\\lib\\site-packages (from kaggle->opendatasets) (2025.8.3)\n",
      "Requirement already satisfied: charset-normalizer in c:\\users\\cbotc\\repo\\trotronotebook\\.venv\\lib\\site-packages (from kaggle->opendatasets) (3.4.3)\n",
      "Requirement already satisfied: idna in c:\\users\\cbotc\\repo\\trotronotebook\\.venv\\lib\\site-packages (from kaggle->opendatasets) (3.10)\n",
      "Requirement already satisfied: protobuf in c:\\users\\cbotc\\repo\\trotronotebook\\.venv\\lib\\site-packages (from kaggle->opendatasets) (6.31.1)\n",
      "Requirement already satisfied: python-slugify in c:\\users\\cbotc\\repo\\trotronotebook\\.venv\\lib\\site-packages (from kaggle->opendatasets) (8.0.4)\n",
      "Requirement already satisfied: requests in c:\\users\\cbotc\\repo\\trotronotebook\\.venv\\lib\\site-packages (from kaggle->opendatasets) (2.32.4)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in c:\\users\\cbotc\\repo\\trotronotebook\\.venv\\lib\\site-packages (from kaggle->opendatasets) (80.9.0)\n",
      "Requirement already satisfied: text-unidecode in c:\\users\\cbotc\\repo\\trotronotebook\\.venv\\lib\\site-packages (from kaggle->opendatasets) (1.3)\n",
      "Requirement already satisfied: urllib3>=1.15.1 in c:\\users\\cbotc\\repo\\trotronotebook\\.venv\\lib\\site-packages (from kaggle->opendatasets) (2.5.0)\n",
      "Requirement already satisfied: webencodings in c:\\users\\cbotc\\repo\\trotronotebook\\.venv\\lib\\site-packages (from kaggle->opendatasets) (0.5.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install required packages if not already installed\n",
    "%pip install opendatasets pandas numpy legacy-cgi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7ae6d3-1d64-4cfe-b289-bfba11f00a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TroTro Dataset Cleaning and Analysis ===\n",
      "Starting analysis at: 2025-08-10 12:06:00\n",
      "Downloading dataset...\n",
      "Skipping, found downloaded files in \".\\trotro\" (use force=True to force download)\n",
      "✓ Dataset downloaded successfully\n",
      "Using data directory: trotro\n",
      "Found 0 TXT files: []\n",
      "No TXT files found in the directory\n"
     ]
    }
   ],
   "source": [
    "# Enhanced TroTro Dataset Cleaning and Analysis\n",
    "import opendatasets as od\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=== TroTro Dataset Cleaning and Analysis ===\")\n",
    "print(f\"Starting analysis at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# 1. Download dataset with error handling\n",
    "def download_dataset():\n",
    "    \"\"\"Download the TroTro dataset from Kaggle\"\"\"\n",
    "    try:\n",
    "        dataset_url = 'https://www.kaggle.com/datasets/godfredaddaiamoako/trotro'\n",
    "        print(\"Downloading dataset...\")\n",
    "        od.download(dataset_url)\n",
    "        print(\"✓ Dataset downloaded successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠ Error downloading dataset: {e}\")\n",
    "        print(\"Please ensure you have Kaggle credentials configured\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# 2. Enhanced data cleaning function\n",
    "def clean_data(df, filename=\"\"):\n",
    "    \"\"\"Comprehensive data cleaning function\"\"\"\n",
    "    print(f\"\\nCleaning data for {filename}...\")\n",
    "    original_shape = df.shape\n",
    "    \n",
    "    # Create a copy to avoid modifying original\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # 1. Remove completely empty rows and columns\n",
    "    df_clean = df_clean.dropna(how='all').dropna(axis=1, how='all')\n",
    "    \n",
    "    # 2. Clean column names\n",
    "    df_clean.columns = df_clean.columns.str.strip().str.lower()\n",
    "    df_clean.columns = df_clean.columns.str.replace(' ', '_').str.replace(r'[^\\w]', '_', regex=True)\n",
    "    \n",
    "    # 3. Handle duplicates\n",
    "    initial_rows = len(df_clean)\n",
    "    df_clean = df_clean.drop_duplicates()\n",
    "    duplicates_removed = initial_rows - len(df_clean)\n",
    "    if duplicates_removed > 0:\n",
    "        print(f\"  - Removed {duplicates_removed} duplicate rows\")\n",
    "    \n",
    "    # 4. Clean text columns\n",
    "    text_columns = df_clean.select_dtypes(include=['object']).columns\n",
    "    for col in text_columns:\n",
    "        if col in df_clean.columns:\n",
    "            # Strip whitespace and handle common issues\n",
    "            df_clean[col] = df_clean[col].astype(str).str.strip()\n",
    "            df_clean[col] = df_clean[col].replace(['nan', 'NaN', 'None', ''], np.nan)\n",
    "            \n",
    "            # Clean special characters and normalize text\n",
    "            df_clean[col] = df_clean[col].str.replace(r'\\s+', ' ', regex=True)\n",
    "            \n",
    "    # 5. Handle missing values intelligently\n",
    "    missing_threshold = 0.7  # Drop columns with >70% missing data\n",
    "    for col in df_clean.columns:\n",
    "        missing_pct = df_clean[col].isnull().sum() / len(df_clean)\n",
    "        if missing_pct > missing_threshold:\n",
    "            print(f\"  - Dropped column '{col}' (>{missing_threshold*100}% missing)\")\n",
    "            df_clean = df_clean.drop(columns=[col])\n",
    "    \n",
    "    # 6. Fill remaining missing values based on data type\n",
    "    for col in df_clean.columns:\n",
    "        if df_clean[col].isnull().any():\n",
    "            if df_clean[col].dtype in ['int64', 'float64']:\n",
    "                # For numeric columns, use median\n",
    "                df_clean[col] = df_clean[col].fillna(df_clean[col].median())\n",
    "            else:\n",
    "                # For categorical columns, use mode or 'Unknown'\n",
    "                mode_val = df_clean[col].mode()\n",
    "                if len(mode_val) > 0:\n",
    "                    df_clean[col] = df_clean[col].fillna(mode_val[0])\n",
    "                else:\n",
    "                    df_clean[col] = df_clean[col].fillna('Unknown')\n",
    "    \n",
    "    # 7. Detect and handle potential date columns\n",
    "    potential_date_cols = [col for col in df_clean.columns if 'date' in col.lower() or 'time' in col.lower()]\n",
    "    for col in potential_date_cols:\n",
    "        try:\n",
    "            df_clean[col] = pd.to_datetime(df_clean[col], errors='coerce')\n",
    "            print(f\"  - Converted '{col}' to datetime\")\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # 8. Clean numeric columns\n",
    "    numeric_cols = df_clean.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_cols:\n",
    "        # Remove outliers using IQR method\n",
    "        Q1 = df_clean[col].quantile(0.25)\n",
    "        Q3 = df_clean[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        outliers_before = len(df_clean[(df_clean[col] < lower_bound) | (df_clean[col] > upper_bound)])\n",
    "        df_clean[col] = df_clean[col].clip(lower=lower_bound, upper=upper_bound)\n",
    "        if outliers_before > 0:\n",
    "            print(f\"  - Capped {outliers_before} outliers in '{col}'\")\n",
    "    \n",
    "    print(f\"  ✓ Cleaned data shape: {original_shape} → {df_clean.shape}\")\n",
    "    return df_clean\n",
    "\n",
    "# 3. Data analysis function\n",
    "def analyze_data(df, filename=\"\"):\n",
    "    \"\"\"Perform comprehensive data analysis\"\"\"\n",
    "    print(f\"\\n--- Analysis for {filename} ---\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    \n",
    "    # Missing values analysis\n",
    "    missing_data = df.isnull().sum()\n",
    "    if missing_data.sum() > 0:\n",
    "        print(\"\\nMissing values:\")\n",
    "        print(missing_data[missing_data > 0])\n",
    "    else:\n",
    "        print(\"\\n✓ No missing values!\")\n",
    "    \n",
    "    # Data types\n",
    "    print(f\"\\nData types:\")\n",
    "    print(df.dtypes.value_counts())\n",
    "    \n",
    "    # Numeric columns summary\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    if len(numeric_cols) > 0:\n",
    "        print(f\"\\nNumeric columns summary:\")\n",
    "        print(df[numeric_cols].describe())\n",
    "    \n",
    "    # Categorical columns info\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "    if len(categorical_cols) > 0:\n",
    "        print(f\"\\nCategorical columns (top 5 each):\")\n",
    "        for col in categorical_cols[:5]:  # Limit to first 5 to avoid clutter\n",
    "            print(f\"\\n{col}:\")\n",
    "            print(df[col].value_counts().head())\n",
    "    \n",
    "    # Display sample\n",
    "    print(f\"\\nSample data (first 5 rows):\")\n",
    "    print(df.head())\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 4. Main execution\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    \n",
    "    # Download dataset\n",
    "    if not download_dataset():\n",
    "        print(\"Failed to download dataset. Please check your Kaggle credentials.\")\n",
    "        return\n",
    "    \n",
    "    # Find data directory\n",
    "    data_dir = 'trotro/trotrolive-datasets'\n",
    "    if not os.path.exists(data_dir):\n",
    "        # Try alternative directory names\n",
    "        possible_dirs = [d for d in os.listdir('.') if 'trotro' in d.lower()]\n",
    "        if possible_dirs:\n",
    "            data_dir = possible_dirs[0]\n",
    "        else:\n",
    "            print(\"Could not find dataset directory\")\n",
    "            return\n",
    "    \n",
    "    print(f\"Using data directory: {data_dir}\")\n",
    "    \n",
    "    # Find TXT files\n",
    "    try:\n",
    "        data_files = [f for f in os.listdir(data_dir) if f.endswith('.txt')]\n",
    "        print(f\"Found {len(data_files)} TXT files: {data_files}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error accessing data directory: {e}\")\n",
    "        return\n",
    "    \n",
    "    if not data_files:\n",
    "        print(\"No TXT files found in the directory\")\n",
    "        return\n",
    "    \n",
    "    # Process each file\n",
    "    cleaned_dataframes = {}\n",
    "    \n",
    "    for file in data_files:\n",
    "        try:\n",
    "            file_path = os.path.join(data_dir, file)\n",
    "            print(f\"\\n{'='*50}\")\n",
    "            print(f\"Processing: {file}\")\n",
    "            print(f\"{'='*50}\")\n",
    "            \n",
    "            # Load data (try different separators for .txt files)\n",
    "            try:\n",
    "                # Try common separators for text files\n",
    "                df = pd.read_csv(file_path, sep='\\t')  # Tab-separated\n",
    "                print(f\"Original data loaded with tab separator: {df.shape}\")\n",
    "            except:\n",
    "                try:\n",
    "                    df = pd.read_csv(file_path, sep=',')  # Comma-separated\n",
    "                    print(f\"Original data loaded with comma separator: {df.shape}\")\n",
    "                except:\n",
    "                    try:\n",
    "                        df = pd.read_csv(file_path, sep='|')  # Pipe-separated\n",
    "                        print(f\"Original data loaded with pipe separator: {df.shape}\")\n",
    "                    except:\n",
    "                        # Try space-separated with flexible whitespace\n",
    "                        df = pd.read_csv(file_path, sep=r'\\s+', engine='python')\n",
    "                        print(f\"Original data loaded with space separator: {df.shape}\")\n",
    "            \n",
    "            # Check if data loaded properly\n",
    "            if df.empty:\n",
    "                print(f\"⚠ Warning: {file} appears to be empty or couldn't be parsed\")\n",
    "                continue\n",
    "            \n",
    "            # Clean data\n",
    "            df_clean = clean_data(df, file)\n",
    "            \n",
    "            # Analyze cleaned data\n",
    "            df_analyzed = analyze_data(df_clean, file)\n",
    "            \n",
    "            # Save cleaned data\n",
    "            cleaned_filename = f'cleaned_{file.replace(\".txt\", \".csv\")}'  # Save as CSV for easier handling\n",
    "            df_clean.to_csv(cleaned_filename, index=False)\n",
    "            print(f\"✓ Saved cleaned data to: {cleaned_filename}\")\n",
    "            \n",
    "            # Store for potential combination\n",
    "            cleaned_dataframes[file] = df_clean\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error processing {file}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # 5. Combine data if multiple files exist and have similar structure\n",
    "    if len(cleaned_dataframes) > 1:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(\"ATTEMPTING TO COMBINE DATASETS\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        try:\n",
    "            # Check if files have similar columns\n",
    "            all_columns = [set(df.columns) for df in cleaned_dataframes.values()]\n",
    "            common_columns = set.intersection(*all_columns)\n",
    "            \n",
    "            if len(common_columns) > 0:\n",
    "                print(f\"Found {len(common_columns)} common columns\")\n",
    "                \n",
    "                # Combine datasets using common columns\n",
    "                combined_dfs = []\n",
    "                for filename, df in cleaned_dataframes.items():\n",
    "                    df_subset = df[list(common_columns)].copy()\n",
    "                    df_subset['source_file'] = filename\n",
    "                    combined_dfs.append(df_subset)\n",
    "                \n",
    "                combined_data = pd.concat(combined_dfs, ignore_index=True)\n",
    "                \n",
    "                # Analyze combined data\n",
    "                print(\"\\nCombined dataset analysis:\")\n",
    "                analyze_data(combined_data, \"Combined Dataset\")\n",
    "                \n",
    "                # Save combined data\n",
    "                combined_data.to_csv('combined_trotro_data.csv', index=False)\n",
    "                print(\"✓ Saved combined data to: combined_trotro_data.csv\")\n",
    "                \n",
    "            else:\n",
    "                print(\"⚠ No common columns found - cannot combine datasets\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error combining datasets: {e}\")\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"DATA CLEANING COMPLETE!\")\n",
    "    print(f\"Finished at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"{'='*50}\")\n",
    "\n",
    "# Execute the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
