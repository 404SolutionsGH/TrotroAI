{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be0a12cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opendatasets in c:\\users\\cbotc\\repo\\trotronotebook\\.venv\\lib\\site-packages (0.1.22)\n",
      "Requirement already satisfied: pandas in c:\\users\\cbotc\\repo\\trotronotebook\\.venv\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\cbotc\\repo\\trotronotebook\\.venv\\lib\\site-packages (2.3.2)\n",
      "Requirement already satisfied: legacy-cgi in c:\\users\\cbotc\\repo\\trotronotebook\\.venv\\lib\\site-packages (2.6.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\cbotc\\repo\\trotronotebook\\.venv\\lib\\site-packages (from opendatasets) (4.67.1)\n",
      "Requirement already satisfied: kaggle in c:\\users\\cbotc\\repo\\trotronotebook\\.venv\\lib\\site-packages (from opendatasets) (1.7.4.5)\n",
      "Requirement already satisfied: click in c:\\users\\cbotc\\repo\\trotronotebook\\.venv\\lib\\site-packages (from opendatasets) (8.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\cbotc\\repo\\trotronotebook\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\cbotc\\repo\\trotronotebook\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\cbotc\\repo\\trotronotebook\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\cbotc\\repo\\trotronotebook\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\cbotc\\repo\\trotronotebook\\.venv\\lib\\site-packages (from click->opendatasets) (0.4.6)\n",
      "Requirement already satisfied: bleach in c:\\users\\cbotc\\repo\\trotronotebook\\.venv\\lib\\site-packages (from kaggle->opendatasets) (6.2.0)\n",
      "Requirement already satisfied: certifi>=14.05.14 in c:\\users\\cbotc\\repo\\trotronotebook\\.venv\\lib\\site-packages (from kaggle->opendatasets) (2025.8.3)\n",
      "Requirement already satisfied: charset-normalizer in c:\\users\\cbotc\\repo\\trotronotebook\\.venv\\lib\\site-packages (from kaggle->opendatasets) (3.4.3)\n",
      "Requirement already satisfied: idna in c:\\users\\cbotc\\repo\\trotronotebook\\.venv\\lib\\site-packages (from kaggle->opendatasets) (3.10)\n",
      "Requirement already satisfied: protobuf in c:\\users\\cbotc\\repo\\trotronotebook\\.venv\\lib\\site-packages (from kaggle->opendatasets) (6.31.1)\n",
      "Requirement already satisfied: python-slugify in c:\\users\\cbotc\\repo\\trotronotebook\\.venv\\lib\\site-packages (from kaggle->opendatasets) (8.0.4)\n",
      "Requirement already satisfied: requests in c:\\users\\cbotc\\repo\\trotronotebook\\.venv\\lib\\site-packages (from kaggle->opendatasets) (2.32.4)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in c:\\users\\cbotc\\repo\\trotronotebook\\.venv\\lib\\site-packages (from kaggle->opendatasets) (80.9.0)\n",
      "Requirement already satisfied: text-unidecode in c:\\users\\cbotc\\repo\\trotronotebook\\.venv\\lib\\site-packages (from kaggle->opendatasets) (1.3)\n",
      "Requirement already satisfied: urllib3>=1.15.1 in c:\\users\\cbotc\\repo\\trotronotebook\\.venv\\lib\\site-packages (from kaggle->opendatasets) (2.5.0)\n",
      "Requirement already satisfied: webencodings in c:\\users\\cbotc\\repo\\trotronotebook\\.venv\\lib\\site-packages (from kaggle->opendatasets) (0.5.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install required packages if not already installed\n",
    "%pip install opendatasets pandas numpy legacy-cgi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb7ae6d3-1d64-4cfe-b289-bfba11f00a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TroTro Dataset Cleaning and Analysis ===\n",
      "Starting analysis at: 2025-08-10 12:11:44\n",
      "Downloading dataset...\n",
      "Skipping, found downloaded files in \".\\trotro\" (use force=True to force download)\n",
      "✓ Dataset downloaded successfully\n",
      "Using data directory: trotro/trotrolive-datasets/accra\n",
      "Found 9 TXT files: ['agency.txt', 'calendar.txt', 'fare_attributes.txt', 'fare_rules.txt', 'routes.txt', 'shapes.txt', 'stops.txt', 'stop_times.txt', 'trips.txt']\n",
      "Found 0 CSV files: []\n",
      "Total data files to process: 9\n",
      "\n",
      "==================================================\n",
      "Processing: agency.txt\n",
      "==================================================\n",
      "Original TXT data loaded with tab separator: (91, 1)\n",
      "\n",
      "Cleaning data for agency.txt...\n",
      "  - Converted 'agency_id_agency_name_agency_url_agency_timezone' to datetime\n",
      "  ✓ Cleaned data shape: (91, 1) → (91, 1)\n",
      "\n",
      "--- Analysis for agency.txt ---\n",
      "Shape: (91, 1)\n",
      "Memory usage: 0.00 MB\n",
      "\n",
      "Missing values:\n",
      "agency_id_agency_name_agency_url_agency_timezone    91\n",
      "dtype: int64\n",
      "\n",
      "Data types:\n",
      "datetime64[ns]    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 5 rows):\n",
      "  agency_id_agency_name_agency_url_agency_timezone\n",
      "0                                              NaT\n",
      "1                                              NaT\n",
      "2                                              NaT\n",
      "3                                              NaT\n",
      "4                                              NaT\n",
      "✓ Saved cleaned data to: cleaned_agency.csv\n",
      "\n",
      "==================================================\n",
      "Processing: calendar.txt\n",
      "==================================================\n",
      "Original TXT data loaded with tab separator: (1, 1)\n",
      "\n",
      "Cleaning data for calendar.txt...\n",
      "  - Converted 'service_id_monday_tuesday_wednesday_thursday_friday_saturday_sunday_start_date_end_date' to datetime\n",
      "  ✓ Cleaned data shape: (1, 1) → (1, 1)\n",
      "\n",
      "--- Analysis for calendar.txt ---\n",
      "Shape: (1, 1)\n",
      "Memory usage: 0.00 MB\n",
      "\n",
      "Missing values:\n",
      "service_id_monday_tuesday_wednesday_thursday_friday_saturday_sunday_start_date_end_date    1\n",
      "dtype: int64\n",
      "\n",
      "Data types:\n",
      "datetime64[ns]    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 5 rows):\n",
      "  service_id_monday_tuesday_wednesday_thursday_friday_saturday_sunday_start_date_end_date\n",
      "0                                                NaT                                     \n",
      "✓ Saved cleaned data to: cleaned_calendar.csv\n",
      "\n",
      "==================================================\n",
      "Processing: fare_attributes.txt\n",
      "==================================================\n",
      "Original TXT data loaded with tab separator: (31, 1)\n",
      "\n",
      "Cleaning data for fare_attributes.txt...\n",
      "  ✓ Cleaned data shape: (31, 1) → (31, 1)\n",
      "\n",
      "--- Analysis for fare_attributes.txt ---\n",
      "Shape: (31, 1)\n",
      "Memory usage: 0.00 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "object    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Categorical columns (top 5 each):\n",
      "\n",
      "fare_id_price_currency_type_payment_method_transfers_transfer_duration:\n",
      "fare_id_price_currency_type_payment_method_transfers_transfer_duration\n",
      "fare_00,0.5,CAD,0,,    1\n",
      "fare_01,0.7,CAD,0,,    1\n",
      "fare_02,0.8,CAD,0,,    1\n",
      "fare_03,0.9,CAD,0,,    1\n",
      "fare_04,1.0,CAD,0,,    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 5 rows):\n",
      "  fare_id_price_currency_type_payment_method_transfers_transfer_duration\n",
      "0                                fare_00,0.5,CAD,0,,                    \n",
      "1                                fare_01,0.7,CAD,0,,                    \n",
      "2                                fare_02,0.8,CAD,0,,                    \n",
      "3                                fare_03,0.9,CAD,0,,                    \n",
      "4                                fare_04,1.0,CAD,0,,                    \n",
      "✓ Saved cleaned data to: cleaned_fare_attributes.csv\n",
      "\n",
      "==================================================\n",
      "Processing: fare_rules.txt\n",
      "==================================================\n",
      "Original TXT data loaded with tab separator: (608, 1)\n",
      "\n",
      "Cleaning data for fare_rules.txt...\n",
      "  ✓ Cleaned data shape: (608, 1) → (608, 1)\n",
      "\n",
      "--- Analysis for fare_rules.txt ---\n",
      "Shape: (608, 1)\n",
      "Memory usage: 0.04 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "object    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Categorical columns (top 5 each):\n",
      "\n",
      "route_id_fare_id_contains_id_destination_id_origin_id:\n",
      "route_id_fare_id_contains_id_destination_id_origin_id\n",
      "002A,fare_06,,,    1\n",
      "002B,fare_06,,,    1\n",
      "003A,fare_07,,,    1\n",
      "003B,fare_07,,,    1\n",
      "004B,fare_25,,,    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 5 rows):\n",
      "  route_id_fare_id_contains_id_destination_id_origin_id\n",
      "0                                    002A,fare_06,,,   \n",
      "1                                    002B,fare_06,,,   \n",
      "2                                    003A,fare_07,,,   \n",
      "3                                    003B,fare_07,,,   \n",
      "4                                    004B,fare_25,,,   \n",
      "✓ Saved cleaned data to: cleaned_fare_rules.csv\n",
      "\n",
      "==================================================\n",
      "Processing: routes.txt\n",
      "==================================================\n",
      "Original TXT data loaded with tab separator: (651, 1)\n",
      "\n",
      "Cleaning data for routes.txt...\n",
      "  ✓ Cleaned data shape: (651, 1) → (651, 1)\n",
      "\n",
      "--- Analysis for routes.txt ---\n",
      "Shape: (651, 1)\n",
      "Memory usage: 0.06 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "object    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Categorical columns (top 5 each):\n",
      "\n",
      "route_id_route_short_name_agency_id_route_long_name_route_type:\n",
      "route_id_route_short_name_agency_id_route_long_name_route_type\n",
      "385B,385B,15,37 Station to New Town Station,3                     1\n",
      "056A,056A,67,Abeka lapaz to Race Course,3                         1\n",
      "277B,277B,51,C to Kaneshie Mamprobi Station,3                     1\n",
      "311A,311A,57,Kotobabi Down Station to Accra New Tema Station,3    1\n",
      "200A,200A,31,Circle Odorna Station to New Melcom,3                1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 5 rows):\n",
      "  route_id_route_short_name_agency_id_route_long_name_route_type\n",
      "0      385B,385B,15,37 Station to New Town Station,3            \n",
      "1          056A,056A,67,Abeka lapaz to Race Course,3            \n",
      "2      277B,277B,51,C to Kaneshie Mamprobi Station,3            \n",
      "3  311A,311A,57,Kotobabi Down Station to Accra Ne...            \n",
      "4  200A,200A,31,Circle Odorna Station to New Melc...            \n",
      "✓ Saved cleaned data to: cleaned_routes.csv\n",
      "\n",
      "==================================================\n",
      "Processing: shapes.txt\n",
      "==================================================\n",
      "Original TXT data loaded with tab separator: (86377, 1)\n",
      "\n",
      "Cleaning data for shapes.txt...\n",
      "  ✓ Cleaned data shape: (86377, 1) → (86377, 1)\n",
      "\n",
      "--- Analysis for shapes.txt ---\n",
      "Shape: (86377, 1)\n",
      "Memory usage: 6.63 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "object    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Categorical columns (top 5 each):\n",
      "\n",
      "shape_id_shape_pt_lat_shape_pt_lon_shape_pt_sequence:\n",
      "shape_id_shape_pt_lat_shape_pt_lon_shape_pt_sequence\n",
      "651001,5.56734747578,-0.216433394554,113    1\n",
      "1001,5.60772482353,-0.246924286485,2        1\n",
      "1001,5.6075157,-0.247544,3                  1\n",
      "1001,5.607274,-0.2481385,4                  1\n",
      "1001,5.6069257,-0.2489209,5                 1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 5 rows):\n",
      "  shape_id_shape_pt_lat_shape_pt_lon_shape_pt_sequence\n",
      "0           651001,5.56734747578,-0.216433394554,113  \n",
      "1               1001,5.60772482353,-0.246924286485,2  \n",
      "2                         1001,5.6075157,-0.247544,3  \n",
      "3                         1001,5.607274,-0.2481385,4  \n",
      "4                        1001,5.6069257,-0.2489209,5  \n",
      "✓ Saved cleaned data to: cleaned_shapes.csv\n",
      "\n",
      "==================================================\n",
      "Processing: stops.txt\n",
      "==================================================\n",
      "Original TXT data loaded with tab separator: (2565, 1)\n",
      "\n",
      "Cleaning data for stops.txt...\n",
      "  ✓ Cleaned data shape: (2565, 1) → (2565, 1)\n",
      "\n",
      "--- Analysis for stops.txt ---\n",
      "Shape: (2565, 1)\n",
      "Memory usage: 0.23 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "object    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Categorical columns (top 5 each):\n",
      "\n",
      "stop_id_stop_name_stop_lat_stop_lon:\n",
      "stop_id_stop_name_stop_lat_stop_lon\n",
      "S4842,Junction 1,5.563419819,-0.152562007                  1\n",
      "S1001,Total 447,5.535621166,-0.22892572                    1\n",
      "S3829,Water Works 406,5.580180168,-0.276890993             1\n",
      "T208,Terminal Accra Zongo Lane,5.544865131,-0.209994003    1\n",
      "T1471,Terminal Ridge Hospital,5.562654972,-0.200297497     1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 5 rows):\n",
      "                 stop_id_stop_name_stop_lat_stop_lon\n",
      "0          S4842,Junction 1,5.563419819,-0.152562007\n",
      "1            S1001,Total 447,5.535621166,-0.22892572\n",
      "2     S3829,Water Works 406,5.580180168,-0.276890993\n",
      "3  T208,Terminal Accra Zongo Lane,5.544865131,-0....\n",
      "4  T1471,Terminal Ridge Hospital,5.562654972,-0.2...\n",
      "✓ Saved cleaned data to: cleaned_stops.csv\n",
      "\n",
      "==================================================\n",
      "Processing: stop_times.txt\n",
      "==================================================\n",
      "Original TXT data loaded with tab separator: (4796, 1)\n",
      "\n",
      "Cleaning data for stop_times.txt...\n",
      "  - Converted 'trip_id_arrival_time_departure_time_stop_sequence_stop_id' to datetime\n",
      "  ✓ Cleaned data shape: (4796, 1) → (4796, 1)\n",
      "\n",
      "--- Analysis for stop_times.txt ---\n",
      "Shape: (4796, 1)\n",
      "Memory usage: 0.04 MB\n",
      "\n",
      "Missing values:\n",
      "trip_id_arrival_time_departure_time_stop_sequence_stop_id    4796\n",
      "dtype: int64\n",
      "\n",
      "Data types:\n",
      "datetime64[ns]    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 5 rows):\n",
      "  trip_id_arrival_time_departure_time_stop_sequence_stop_id\n",
      "0                                                NaT       \n",
      "1                                                NaT       \n",
      "2                                                NaT       \n",
      "3                                                NaT       \n",
      "4                                                NaT       \n",
      "✓ Saved cleaned data to: cleaned_stop_times.csv\n",
      "\n",
      "==================================================\n",
      "Processing: trips.txt\n",
      "==================================================\n",
      "Original TXT data loaded with tab separator: (651, 1)\n",
      "\n",
      "Cleaning data for trips.txt...\n",
      "  ✓ Cleaned data shape: (651, 1) → (651, 1)\n",
      "\n",
      "--- Analysis for trips.txt ---\n",
      "Shape: (651, 1)\n",
      "Memory usage: 0.05 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "object    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Categorical columns (top 5 each):\n",
      "\n",
      "route_id_service_id_trip_id_shape_id:\n",
      "route_id_service_id_trip_id_shape_id\n",
      "018A,service,018A_1,1001    1\n",
      "005A,service,005A_1,2001    1\n",
      "005B,service,005B_1,3001    1\n",
      "016A,service,016A_1,4001    1\n",
      "016B,service,016B_1,5001    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 5 rows):\n",
      "  route_id_service_id_trip_id_shape_id\n",
      "0             018A,service,018A_1,1001\n",
      "1             005A,service,005A_1,2001\n",
      "2             005B,service,005B_1,3001\n",
      "3             016A,service,016A_1,4001\n",
      "4             016B,service,016B_1,5001\n",
      "✓ Saved cleaned data to: cleaned_trips.csv\n",
      "\n",
      "==================================================\n",
      "ATTEMPTING TO COMBINE DATASETS\n",
      "==================================================\n",
      "⚠ No common columns found - cannot combine datasets\n",
      "\n",
      "==================================================\n",
      "DATA CLEANING COMPLETE!\n",
      "Finished at: 2025-08-10 12:11:45\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Enhanced TroTro Dataset Cleaning and Analysis\n",
    "import opendatasets as od\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=== TroTro Dataset Cleaning and Analysis ===\")\n",
    "print(f\"Starting analysis at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# 1. Download dataset with error handling\n",
    "def download_dataset():\n",
    "    \"\"\"Download the TroTro dataset from Kaggle\"\"\"\n",
    "    try:\n",
    "        dataset_url = 'https://www.kaggle.com/datasets/godfredaddaiamoako/trotro'\n",
    "        print(\"Downloading dataset...\")\n",
    "        od.download(dataset_url)\n",
    "        print(\"✓ Dataset downloaded successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠ Error downloading dataset: {e}\")\n",
    "        print(\"Please ensure you have Kaggle credentials configured\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# 2. Enhanced data cleaning function\n",
    "def clean_data(df, filename=\"\"):\n",
    "    \"\"\"Comprehensive data cleaning function\"\"\"\n",
    "    print(f\"\\nCleaning data for {filename}...\")\n",
    "    original_shape = df.shape\n",
    "    \n",
    "    # Create a copy to avoid modifying original\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # 1. Remove completely empty rows and columns\n",
    "    df_clean = df_clean.dropna(how='all').dropna(axis=1, how='all')\n",
    "    \n",
    "    # 2. Clean column names\n",
    "    df_clean.columns = df_clean.columns.str.strip().str.lower()\n",
    "    df_clean.columns = df_clean.columns.str.replace(' ', '_').str.replace(r'[^\\w]', '_', regex=True)\n",
    "    \n",
    "    # 3. Handle duplicates\n",
    "    initial_rows = len(df_clean)\n",
    "    df_clean = df_clean.drop_duplicates()\n",
    "    duplicates_removed = initial_rows - len(df_clean)\n",
    "    if duplicates_removed > 0:\n",
    "        print(f\"  - Removed {duplicates_removed} duplicate rows\")\n",
    "    \n",
    "    # 4. Clean text columns\n",
    "    text_columns = df_clean.select_dtypes(include=['object']).columns\n",
    "    for col in text_columns:\n",
    "        if col in df_clean.columns:\n",
    "            # Strip whitespace and handle common issues\n",
    "            df_clean[col] = df_clean[col].astype(str).str.strip()\n",
    "            df_clean[col] = df_clean[col].replace(['nan', 'NaN', 'None', ''], np.nan)\n",
    "            \n",
    "            # Clean special characters and normalize text\n",
    "            df_clean[col] = df_clean[col].str.replace(r'\\s+', ' ', regex=True)\n",
    "            \n",
    "    # 5. Handle missing values intelligently\n",
    "    missing_threshold = 0.7  # Drop columns with >70% missing data\n",
    "    for col in df_clean.columns:\n",
    "        missing_pct = df_clean[col].isnull().sum() / len(df_clean)\n",
    "        if missing_pct > missing_threshold:\n",
    "            print(f\"  - Dropped column '{col}' (>{missing_threshold*100}% missing)\")\n",
    "            df_clean = df_clean.drop(columns=[col])\n",
    "    \n",
    "    # 6. Fill remaining missing values based on data type\n",
    "    for col in df_clean.columns:\n",
    "        if df_clean[col].isnull().any():\n",
    "            if df_clean[col].dtype in ['int64', 'float64']:\n",
    "                # For numeric columns, use median\n",
    "                df_clean[col] = df_clean[col].fillna(df_clean[col].median())\n",
    "            else:\n",
    "                # For categorical columns, use mode or 'Unknown'\n",
    "                mode_val = df_clean[col].mode()\n",
    "                if len(mode_val) > 0:\n",
    "                    df_clean[col] = df_clean[col].fillna(mode_val[0])\n",
    "                else:\n",
    "                    df_clean[col] = df_clean[col].fillna('Unknown')\n",
    "    \n",
    "    # 7. Detect and handle potential date columns\n",
    "    potential_date_cols = [col for col in df_clean.columns if 'date' in col.lower() or 'time' in col.lower()]\n",
    "    for col in potential_date_cols:\n",
    "        try:\n",
    "            df_clean[col] = pd.to_datetime(df_clean[col], errors='coerce')\n",
    "            print(f\"  - Converted '{col}' to datetime\")\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # 8. Clean numeric columns\n",
    "    numeric_cols = df_clean.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_cols:\n",
    "        # Remove outliers using IQR method\n",
    "        Q1 = df_clean[col].quantile(0.25)\n",
    "        Q3 = df_clean[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        outliers_before = len(df_clean[(df_clean[col] < lower_bound) | (df_clean[col] > upper_bound)])\n",
    "        df_clean[col] = df_clean[col].clip(lower=lower_bound, upper=upper_bound)\n",
    "        if outliers_before > 0:\n",
    "            print(f\"  - Capped {outliers_before} outliers in '{col}'\")\n",
    "    \n",
    "    print(f\"  ✓ Cleaned data shape: {original_shape} → {df_clean.shape}\")\n",
    "    return df_clean\n",
    "\n",
    "# 3. Data analysis function\n",
    "def analyze_data(df, filename=\"\"):\n",
    "    \"\"\"Perform comprehensive data analysis\"\"\"\n",
    "    print(f\"\\n--- Analysis for {filename} ---\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    \n",
    "    # Missing values analysis\n",
    "    missing_data = df.isnull().sum()\n",
    "    if missing_data.sum() > 0:\n",
    "        print(\"\\nMissing values:\")\n",
    "        print(missing_data[missing_data > 0])\n",
    "    else:\n",
    "        print(\"\\n✓ No missing values!\")\n",
    "    \n",
    "    # Data types\n",
    "    print(f\"\\nData types:\")\n",
    "    print(df.dtypes.value_counts())\n",
    "    \n",
    "    # Numeric columns summary\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    if len(numeric_cols) > 0:\n",
    "        print(f\"\\nNumeric columns summary:\")\n",
    "        print(df[numeric_cols].describe())\n",
    "    \n",
    "    # Categorical columns info\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "    if len(categorical_cols) > 0:\n",
    "        print(f\"\\nCategorical columns (top 5 each):\")\n",
    "        for col in categorical_cols[:5]:  # Limit to first 5 to avoid clutter\n",
    "            print(f\"\\n{col}:\")\n",
    "            print(df[col].value_counts().head())\n",
    "    \n",
    "    # Display sample\n",
    "    print(f\"\\nSample data (first 5 rows):\")\n",
    "    print(df.head())\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 4. Main execution\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    \n",
    "    # Download dataset\n",
    "    if not download_dataset():\n",
    "        print(\"Failed to download dataset. Please check your Kaggle credentials.\")\n",
    "        return\n",
    "    \n",
    "    # Find data directory\n",
    "    data_dir = 'trotro/trotrolive-datasets/accra'\n",
    "    if not os.path.exists(data_dir):\n",
    "        # Try alternative directory names\n",
    "        possible_dirs = [d for d in os.listdir('.') if 'trotro' in d.lower()]\n",
    "        if possible_dirs:\n",
    "            data_dir = possible_dirs[0]\n",
    "        else:\n",
    "            print(\"Could not find dataset directory\")\n",
    "            return\n",
    "    \n",
    "    print(f\"Using data directory: {data_dir}\")\n",
    "    \n",
    "    # Find both TXT and CSV files\n",
    "    try:\n",
    "        txt_files = [f for f in os.listdir(data_dir) if f.endswith('.txt')]\n",
    "        csv_files = [f for f in os.listdir(data_dir) if f.endswith('.csv')]\n",
    "        data_files = txt_files + csv_files\n",
    "        \n",
    "        print(f\"Found {len(txt_files)} TXT files: {txt_files}\")\n",
    "        print(f\"Found {len(csv_files)} CSV files: {csv_files}\")\n",
    "        print(f\"Total data files to process: {len(data_files)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error accessing data directory: {e}\")\n",
    "        return\n",
    "    \n",
    "    if not data_files:\n",
    "        print(\"No TXT or CSV files found in the directory\")\n",
    "        return\n",
    "    \n",
    "    # Process each file\n",
    "    cleaned_dataframes = {}\n",
    "    \n",
    "    for file in data_files:\n",
    "        try:\n",
    "            file_path = os.path.join(data_dir, file)\n",
    "            print(f\"\\n{'='*50}\")\n",
    "            print(f\"Processing: {file}\")\n",
    "            print(f\"{'='*50}\")\n",
    "            \n",
    "            # Load data with appropriate method based on file extension\n",
    "            file_extension = os.path.splitext(file)[1].lower()\n",
    "            \n",
    "            if file_extension == '.csv':\n",
    "                # For CSV files, try comma separator first\n",
    "                try:\n",
    "                    df = pd.read_csv(file_path)\n",
    "                    print(f\"Original CSV data loaded: {df.shape}\")\n",
    "                except Exception as csv_error:\n",
    "                    print(f\"⚠ Error loading CSV with default separator: {csv_error}\")\n",
    "                    # Fallback to trying different separators for problematic CSV files\n",
    "                    try:\n",
    "                        df = pd.read_csv(file_path, sep=';')  # Semicolon-separated\n",
    "                        print(f\"Original data loaded with semicolon separator: {df.shape}\")\n",
    "                    except:\n",
    "                        df = pd.read_csv(file_path, sep='\\t')  # Tab-separated CSV\n",
    "                        print(f\"Original data loaded with tab separator: {df.shape}\")\n",
    "                        \n",
    "            elif file_extension == '.txt':\n",
    "                # For TXT files, try different separators\n",
    "                try:\n",
    "                    df = pd.read_csv(file_path, sep='\\t')  # Tab-separated (most common for .txt)\n",
    "                    print(f\"Original TXT data loaded with tab separator: {df.shape}\")\n",
    "                except:\n",
    "                    try:\n",
    "                        df = pd.read_csv(file_path, sep=',')  # Comma-separated\n",
    "                        print(f\"Original data loaded with comma separator: {df.shape}\")\n",
    "                    except:\n",
    "                        try:\n",
    "                            df = pd.read_csv(file_path, sep='|')  # Pipe-separated\n",
    "                            print(f\"Original data loaded with pipe separator: {df.shape}\")\n",
    "                        except:\n",
    "                            try:\n",
    "                                df = pd.read_csv(file_path, sep=';')  # Semicolon-separated\n",
    "                                print(f\"Original data loaded with semicolon separator: {df.shape}\")\n",
    "                            except:\n",
    "                                # Try space-separated with flexible whitespace\n",
    "                                df = pd.read_csv(file_path, sep=r'\\s+', engine='python')\n",
    "                                print(f\"Original data loaded with space separator: {df.shape}\")\n",
    "            \n",
    "            # Check if data loaded properly\n",
    "            if df.empty:\n",
    "                print(f\"⚠ Warning: {file} appears to be empty or couldn't be parsed\")\n",
    "                continue\n",
    "            \n",
    "            # Clean data\n",
    "            df_clean = clean_data(df, file)\n",
    "            \n",
    "            # Analyze cleaned data\n",
    "            df_analyzed = analyze_data(df_clean, file)\n",
    "            \n",
    "            # Save cleaned data (maintain original extension or convert to CSV)\n",
    "            file_base = os.path.splitext(file)[0]\n",
    "            cleaned_filename = f'cleaned_{file_base}.csv'  # Always save as CSV for consistency\n",
    "            df_clean.to_csv(cleaned_filename, index=False)\n",
    "            print(f\"✓ Saved cleaned data to: {cleaned_filename}\")\n",
    "            \n",
    "            # Store for potential combination\n",
    "            cleaned_dataframes[file] = df_clean\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error processing {file}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # 5. Combine data if multiple files exist and have similar structure\n",
    "    if len(cleaned_dataframes) > 1:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(\"ATTEMPTING TO COMBINE DATASETS\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        try:\n",
    "            # Check if files have similar columns\n",
    "            all_columns = [set(df.columns) for df in cleaned_dataframes.values()]\n",
    "            common_columns = set.intersection(*all_columns)\n",
    "            \n",
    "            if len(common_columns) > 0:\n",
    "                print(f\"Found {len(common_columns)} common columns\")\n",
    "                \n",
    "                # Combine datasets using common columns\n",
    "                combined_dfs = []\n",
    "                for filename, df in cleaned_dataframes.items():\n",
    "                    df_subset = df[list(common_columns)].copy()\n",
    "                    df_subset['source_file'] = filename\n",
    "                    combined_dfs.append(df_subset)\n",
    "                \n",
    "                combined_data = pd.concat(combined_dfs, ignore_index=True)\n",
    "                \n",
    "                # Analyze combined data\n",
    "                print(\"\\nCombined dataset analysis:\")\n",
    "                analyze_data(combined_data, \"Combined Dataset\")\n",
    "                \n",
    "                # Save combined data\n",
    "                combined_data.to_csv('combined_trotro_data.csv', index=False)\n",
    "                print(\"✓ Saved combined data to: combined_trotro_data.csv\")\n",
    "                \n",
    "            else:\n",
    "                print(\"⚠ No common columns found - cannot combine datasets\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error combining datasets: {e}\")\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"DATA CLEANING COMPLETE!\")\n",
    "    print(f\"Finished at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"{'='*50}\")\n",
    "\n",
    "# Execute the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
