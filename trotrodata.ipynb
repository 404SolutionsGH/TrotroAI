{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be0a12cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opendatasets in c:\\users\\cbotc\\repo\\trotronotebook\\.venv\\lib\\site-packages (0.1.22)\n",
      "Requirement already satisfied: pandas in c:\\users\\cbotc\\repo\\trotronotebook\\.venv\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\cbotc\\repo\\trotronotebook\\.venv\\lib\\site-packages (2.3.2)\n",
      "Requirement already satisfied: legacy-cgi in c:\\users\\cbotc\\repo\\trotronotebook\\.venv\\lib\\site-packages (2.6.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\cbotc\\repo\\trotronotebook\\.venv\\lib\\site-packages (from opendatasets) (4.67.1)\n",
      "Requirement already satisfied: kaggle in c:\\users\\cbotc\\repo\\trotronotebook\\.venv\\lib\\site-packages (from opendatasets) (1.7.4.5)\n",
      "Requirement already satisfied: click in c:\\users\\cbotc\\repo\\trotronotebook\\.venv\\lib\\site-packages (from opendatasets) (8.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\cbotc\\repo\\trotronotebook\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\cbotc\\repo\\trotronotebook\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\cbotc\\repo\\trotronotebook\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\cbotc\\repo\\trotronotebook\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\cbotc\\repo\\trotronotebook\\.venv\\lib\\site-packages (from click->opendatasets) (0.4.6)\n",
      "Requirement already satisfied: bleach in c:\\users\\cbotc\\repo\\trotronotebook\\.venv\\lib\\site-packages (from kaggle->opendatasets) (6.2.0)\n",
      "Requirement already satisfied: certifi>=14.05.14 in c:\\users\\cbotc\\repo\\trotronotebook\\.venv\\lib\\site-packages (from kaggle->opendatasets) (2025.8.3)\n",
      "Requirement already satisfied: charset-normalizer in c:\\users\\cbotc\\repo\\trotronotebook\\.venv\\lib\\site-packages (from kaggle->opendatasets) (3.4.3)\n",
      "Requirement already satisfied: idna in c:\\users\\cbotc\\repo\\trotronotebook\\.venv\\lib\\site-packages (from kaggle->opendatasets) (3.10)\n",
      "Requirement already satisfied: protobuf in c:\\users\\cbotc\\repo\\trotronotebook\\.venv\\lib\\site-packages (from kaggle->opendatasets) (6.31.1)\n",
      "Requirement already satisfied: python-slugify in c:\\users\\cbotc\\repo\\trotronotebook\\.venv\\lib\\site-packages (from kaggle->opendatasets) (8.0.4)\n",
      "Requirement already satisfied: requests in c:\\users\\cbotc\\repo\\trotronotebook\\.venv\\lib\\site-packages (from kaggle->opendatasets) (2.32.4)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in c:\\users\\cbotc\\repo\\trotronotebook\\.venv\\lib\\site-packages (from kaggle->opendatasets) (80.9.0)\n",
      "Requirement already satisfied: text-unidecode in c:\\users\\cbotc\\repo\\trotronotebook\\.venv\\lib\\site-packages (from kaggle->opendatasets) (1.3)\n",
      "Requirement already satisfied: urllib3>=1.15.1 in c:\\users\\cbotc\\repo\\trotronotebook\\.venv\\lib\\site-packages (from kaggle->opendatasets) (2.5.0)\n",
      "Requirement already satisfied: webencodings in c:\\users\\cbotc\\repo\\trotronotebook\\.venv\\lib\\site-packages (from kaggle->opendatasets) (0.5.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install required packages if not already installed\n",
    "%pip install opendatasets pandas numpy legacy-cgi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7ae6d3-1d64-4cfe-b289-bfba11f00a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TroTro Dataset Cleaning and Analysis ===\n",
      "Starting analysis at: 2025-08-10 12:11:44\n",
      "Downloading dataset...\n",
      "Skipping, found downloaded files in \".\\trotro\" (use force=True to force download)\n",
      "✓ Dataset downloaded successfully\n",
      "Using data directory: trotro/trotrolive-datasets/accra\n",
      "Found 9 TXT files: ['agency.txt', 'calendar.txt', 'fare_attributes.txt', 'fare_rules.txt', 'routes.txt', 'shapes.txt', 'stops.txt', 'stop_times.txt', 'trips.txt']\n",
      "Found 0 CSV files: []\n",
      "Total data files to process: 9\n",
      "\n",
      "==================================================\n",
      "Processing: agency.txt\n",
      "==================================================\n",
      "Original TXT data loaded with tab separator: (91, 1)\n",
      "\n",
      "Cleaning data for agency.txt...\n",
      "  - Converted 'agency_id_agency_name_agency_url_agency_timezone' to datetime\n",
      "  ✓ Cleaned data shape: (91, 1) → (91, 1)\n",
      "\n",
      "--- Analysis for agency.txt ---\n",
      "Shape: (91, 1)\n",
      "Memory usage: 0.00 MB\n",
      "\n",
      "Missing values:\n",
      "agency_id_agency_name_agency_url_agency_timezone    91\n",
      "dtype: int64\n",
      "\n",
      "Data types:\n",
      "datetime64[ns]    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 5 rows):\n",
      "  agency_id_agency_name_agency_url_agency_timezone\n",
      "0                                              NaT\n",
      "1                                              NaT\n",
      "2                                              NaT\n",
      "3                                              NaT\n",
      "4                                              NaT\n",
      "✓ Saved cleaned data to: cleaned_agency.csv\n",
      "\n",
      "==================================================\n",
      "Processing: calendar.txt\n",
      "==================================================\n",
      "Original TXT data loaded with tab separator: (1, 1)\n",
      "\n",
      "Cleaning data for calendar.txt...\n",
      "  - Converted 'service_id_monday_tuesday_wednesday_thursday_friday_saturday_sunday_start_date_end_date' to datetime\n",
      "  ✓ Cleaned data shape: (1, 1) → (1, 1)\n",
      "\n",
      "--- Analysis for calendar.txt ---\n",
      "Shape: (1, 1)\n",
      "Memory usage: 0.00 MB\n",
      "\n",
      "Missing values:\n",
      "service_id_monday_tuesday_wednesday_thursday_friday_saturday_sunday_start_date_end_date    1\n",
      "dtype: int64\n",
      "\n",
      "Data types:\n",
      "datetime64[ns]    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 5 rows):\n",
      "  service_id_monday_tuesday_wednesday_thursday_friday_saturday_sunday_start_date_end_date\n",
      "0                                                NaT                                     \n",
      "✓ Saved cleaned data to: cleaned_calendar.csv\n",
      "\n",
      "==================================================\n",
      "Processing: fare_attributes.txt\n",
      "==================================================\n",
      "Original TXT data loaded with tab separator: (31, 1)\n",
      "\n",
      "Cleaning data for fare_attributes.txt...\n",
      "  ✓ Cleaned data shape: (31, 1) → (31, 1)\n",
      "\n",
      "--- Analysis for fare_attributes.txt ---\n",
      "Shape: (31, 1)\n",
      "Memory usage: 0.00 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "object    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Categorical columns (top 5 each):\n",
      "\n",
      "fare_id_price_currency_type_payment_method_transfers_transfer_duration:\n",
      "fare_id_price_currency_type_payment_method_transfers_transfer_duration\n",
      "fare_00,0.5,CAD,0,,    1\n",
      "fare_01,0.7,CAD,0,,    1\n",
      "fare_02,0.8,CAD,0,,    1\n",
      "fare_03,0.9,CAD,0,,    1\n",
      "fare_04,1.0,CAD,0,,    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 5 rows):\n",
      "  fare_id_price_currency_type_payment_method_transfers_transfer_duration\n",
      "0                                fare_00,0.5,CAD,0,,                    \n",
      "1                                fare_01,0.7,CAD,0,,                    \n",
      "2                                fare_02,0.8,CAD,0,,                    \n",
      "3                                fare_03,0.9,CAD,0,,                    \n",
      "4                                fare_04,1.0,CAD,0,,                    \n",
      "✓ Saved cleaned data to: cleaned_fare_attributes.csv\n",
      "\n",
      "==================================================\n",
      "Processing: fare_rules.txt\n",
      "==================================================\n",
      "Original TXT data loaded with tab separator: (608, 1)\n",
      "\n",
      "Cleaning data for fare_rules.txt...\n",
      "  ✓ Cleaned data shape: (608, 1) → (608, 1)\n",
      "\n",
      "--- Analysis for fare_rules.txt ---\n",
      "Shape: (608, 1)\n",
      "Memory usage: 0.04 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "object    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Categorical columns (top 5 each):\n",
      "\n",
      "route_id_fare_id_contains_id_destination_id_origin_id:\n",
      "route_id_fare_id_contains_id_destination_id_origin_id\n",
      "002A,fare_06,,,    1\n",
      "002B,fare_06,,,    1\n",
      "003A,fare_07,,,    1\n",
      "003B,fare_07,,,    1\n",
      "004B,fare_25,,,    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 5 rows):\n",
      "  route_id_fare_id_contains_id_destination_id_origin_id\n",
      "0                                    002A,fare_06,,,   \n",
      "1                                    002B,fare_06,,,   \n",
      "2                                    003A,fare_07,,,   \n",
      "3                                    003B,fare_07,,,   \n",
      "4                                    004B,fare_25,,,   \n",
      "✓ Saved cleaned data to: cleaned_fare_rules.csv\n",
      "\n",
      "==================================================\n",
      "Processing: routes.txt\n",
      "==================================================\n",
      "Original TXT data loaded with tab separator: (651, 1)\n",
      "\n",
      "Cleaning data for routes.txt...\n",
      "  ✓ Cleaned data shape: (651, 1) → (651, 1)\n",
      "\n",
      "--- Analysis for routes.txt ---\n",
      "Shape: (651, 1)\n",
      "Memory usage: 0.06 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "object    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Categorical columns (top 5 each):\n",
      "\n",
      "route_id_route_short_name_agency_id_route_long_name_route_type:\n",
      "route_id_route_short_name_agency_id_route_long_name_route_type\n",
      "385B,385B,15,37 Station to New Town Station,3                     1\n",
      "056A,056A,67,Abeka lapaz to Race Course,3                         1\n",
      "277B,277B,51,C to Kaneshie Mamprobi Station,3                     1\n",
      "311A,311A,57,Kotobabi Down Station to Accra New Tema Station,3    1\n",
      "200A,200A,31,Circle Odorna Station to New Melcom,3                1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 5 rows):\n",
      "  route_id_route_short_name_agency_id_route_long_name_route_type\n",
      "0      385B,385B,15,37 Station to New Town Station,3            \n",
      "1          056A,056A,67,Abeka lapaz to Race Course,3            \n",
      "2      277B,277B,51,C to Kaneshie Mamprobi Station,3            \n",
      "3  311A,311A,57,Kotobabi Down Station to Accra Ne...            \n",
      "4  200A,200A,31,Circle Odorna Station to New Melc...            \n",
      "✓ Saved cleaned data to: cleaned_routes.csv\n",
      "\n",
      "==================================================\n",
      "Processing: shapes.txt\n",
      "==================================================\n",
      "Original TXT data loaded with tab separator: (86377, 1)\n",
      "\n",
      "Cleaning data for shapes.txt...\n",
      "  ✓ Cleaned data shape: (86377, 1) → (86377, 1)\n",
      "\n",
      "--- Analysis for shapes.txt ---\n",
      "Shape: (86377, 1)\n",
      "Memory usage: 6.63 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "object    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Categorical columns (top 5 each):\n",
      "\n",
      "shape_id_shape_pt_lat_shape_pt_lon_shape_pt_sequence:\n",
      "shape_id_shape_pt_lat_shape_pt_lon_shape_pt_sequence\n",
      "651001,5.56734747578,-0.216433394554,113    1\n",
      "1001,5.60772482353,-0.246924286485,2        1\n",
      "1001,5.6075157,-0.247544,3                  1\n",
      "1001,5.607274,-0.2481385,4                  1\n",
      "1001,5.6069257,-0.2489209,5                 1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 5 rows):\n",
      "  shape_id_shape_pt_lat_shape_pt_lon_shape_pt_sequence\n",
      "0           651001,5.56734747578,-0.216433394554,113  \n",
      "1               1001,5.60772482353,-0.246924286485,2  \n",
      "2                         1001,5.6075157,-0.247544,3  \n",
      "3                         1001,5.607274,-0.2481385,4  \n",
      "4                        1001,5.6069257,-0.2489209,5  \n",
      "✓ Saved cleaned data to: cleaned_shapes.csv\n",
      "\n",
      "==================================================\n",
      "Processing: stops.txt\n",
      "==================================================\n",
      "Original TXT data loaded with tab separator: (2565, 1)\n",
      "\n",
      "Cleaning data for stops.txt...\n",
      "  ✓ Cleaned data shape: (2565, 1) → (2565, 1)\n",
      "\n",
      "--- Analysis for stops.txt ---\n",
      "Shape: (2565, 1)\n",
      "Memory usage: 0.23 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "object    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Categorical columns (top 5 each):\n",
      "\n",
      "stop_id_stop_name_stop_lat_stop_lon:\n",
      "stop_id_stop_name_stop_lat_stop_lon\n",
      "S4842,Junction 1,5.563419819,-0.152562007                  1\n",
      "S1001,Total 447,5.535621166,-0.22892572                    1\n",
      "S3829,Water Works 406,5.580180168,-0.276890993             1\n",
      "T208,Terminal Accra Zongo Lane,5.544865131,-0.209994003    1\n",
      "T1471,Terminal Ridge Hospital,5.562654972,-0.200297497     1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 5 rows):\n",
      "                 stop_id_stop_name_stop_lat_stop_lon\n",
      "0          S4842,Junction 1,5.563419819,-0.152562007\n",
      "1            S1001,Total 447,5.535621166,-0.22892572\n",
      "2     S3829,Water Works 406,5.580180168,-0.276890993\n",
      "3  T208,Terminal Accra Zongo Lane,5.544865131,-0....\n",
      "4  T1471,Terminal Ridge Hospital,5.562654972,-0.2...\n",
      "✓ Saved cleaned data to: cleaned_stops.csv\n",
      "\n",
      "==================================================\n",
      "Processing: stop_times.txt\n",
      "==================================================\n",
      "Original TXT data loaded with tab separator: (4796, 1)\n",
      "\n",
      "Cleaning data for stop_times.txt...\n",
      "  - Converted 'trip_id_arrival_time_departure_time_stop_sequence_stop_id' to datetime\n",
      "  ✓ Cleaned data shape: (4796, 1) → (4796, 1)\n",
      "\n",
      "--- Analysis for stop_times.txt ---\n",
      "Shape: (4796, 1)\n",
      "Memory usage: 0.04 MB\n",
      "\n",
      "Missing values:\n",
      "trip_id_arrival_time_departure_time_stop_sequence_stop_id    4796\n",
      "dtype: int64\n",
      "\n",
      "Data types:\n",
      "datetime64[ns]    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 5 rows):\n",
      "  trip_id_arrival_time_departure_time_stop_sequence_stop_id\n",
      "0                                                NaT       \n",
      "1                                                NaT       \n",
      "2                                                NaT       \n",
      "3                                                NaT       \n",
      "4                                                NaT       \n",
      "✓ Saved cleaned data to: cleaned_stop_times.csv\n",
      "\n",
      "==================================================\n",
      "Processing: trips.txt\n",
      "==================================================\n",
      "Original TXT data loaded with tab separator: (651, 1)\n",
      "\n",
      "Cleaning data for trips.txt...\n",
      "  ✓ Cleaned data shape: (651, 1) → (651, 1)\n",
      "\n",
      "--- Analysis for trips.txt ---\n",
      "Shape: (651, 1)\n",
      "Memory usage: 0.05 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "object    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Categorical columns (top 5 each):\n",
      "\n",
      "route_id_service_id_trip_id_shape_id:\n",
      "route_id_service_id_trip_id_shape_id\n",
      "018A,service,018A_1,1001    1\n",
      "005A,service,005A_1,2001    1\n",
      "005B,service,005B_1,3001    1\n",
      "016A,service,016A_1,4001    1\n",
      "016B,service,016B_1,5001    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 5 rows):\n",
      "  route_id_service_id_trip_id_shape_id\n",
      "0             018A,service,018A_1,1001\n",
      "1             005A,service,005A_1,2001\n",
      "2             005B,service,005B_1,3001\n",
      "3             016A,service,016A_1,4001\n",
      "4             016B,service,016B_1,5001\n",
      "✓ Saved cleaned data to: cleaned_trips.csv\n",
      "\n",
      "==================================================\n",
      "ATTEMPTING TO COMBINE DATASETS\n",
      "==================================================\n",
      "⚠ No common columns found - cannot combine datasets\n",
      "\n",
      "==================================================\n",
      "DATA CLEANING COMPLETE!\n",
      "Finished at: 2025-08-10 12:11:45\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Enhanced TroTro Multi-City Dataset Cleaning and Repackaging\n",
    "import opendatasets as od\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import re\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=== TroTro Multi-City Dataset Cleaning and Analysis ===\")\n",
    "print(f\"Starting analysis at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# 1. Download dataset with error handling\n",
    "def download_dataset():\n",
    "    \"\"\"Download the TroTro dataset from Kaggle\"\"\"\n",
    "    try:\n",
    "        dataset_url = 'https://www.kaggle.com/datasets/godfredaddaiamoako/trotro'\n",
    "        print(\"Downloading dataset...\")\n",
    "        od.download(dataset_url)\n",
    "        print(\"✓ Dataset downloaded successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠ Error downloading dataset: {e}\")\n",
    "        print(\"Please ensure you have Kaggle credentials configured\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# 2. Enhanced data cleaning function\n",
    "def clean_data(df, filename=\"\", city=\"\"):\n",
    "    \"\"\"Comprehensive data cleaning function\"\"\"\n",
    "    print(f\"\\nCleaning data for {city}/{filename}...\")\n",
    "    original_shape = df.shape\n",
    "    \n",
    "    if df.empty:\n",
    "        print(f\"  ⚠ Warning: {filename} is empty, skipping...\")\n",
    "        return None\n",
    "    \n",
    "    # Create a copy to avoid modifying original\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # 1. Remove completely empty rows and columns\n",
    "    df_clean = df_clean.dropna(how='all').dropna(axis=1, how='all')\n",
    "    \n",
    "    # 2. Clean column names\n",
    "    df_clean.columns = df_clean.columns.str.strip().str.lower()\n",
    "    df_clean.columns = df_clean.columns.str.replace(' ', '_').str.replace(r'[^\\w]', '_', regex=True)\n",
    "    \n",
    "    # 3. Handle duplicates\n",
    "    initial_rows = len(df_clean)\n",
    "    df_clean = df_clean.drop_duplicates()\n",
    "    duplicates_removed = initial_rows - len(df_clean)\n",
    "    if duplicates_removed > 0:\n",
    "        print(f\"  - Removed {duplicates_removed} duplicate rows\")\n",
    "    \n",
    "    # 4. Clean text columns\n",
    "    text_columns = df_clean.select_dtypes(include=['object']).columns\n",
    "    for col in text_columns:\n",
    "        if col in df_clean.columns:\n",
    "            # Strip whitespace and handle common issues\n",
    "            df_clean[col] = df_clean[col].astype(str).str.strip()\n",
    "            df_clean[col] = df_clean[col].replace(['nan', 'NaN', 'None', ''], np.nan)\n",
    "            \n",
    "            # Clean special characters and normalize text\n",
    "            df_clean[col] = df_clean[col].str.replace(r'\\s+', ' ', regex=True)\n",
    "            \n",
    "    # 5. Handle missing values intelligently\n",
    "    missing_threshold = 0.7  # Drop columns with >70% missing data\n",
    "    columns_to_drop = []\n",
    "    for col in df_clean.columns:\n",
    "        missing_pct = df_clean[col].isnull().sum() / len(df_clean)\n",
    "        if missing_pct > missing_threshold:\n",
    "            columns_to_drop.append(col)\n",
    "            print(f\"  - Dropped column '{col}' (>{missing_threshold*100}% missing)\")\n",
    "    \n",
    "    if columns_to_drop:\n",
    "        df_clean = df_clean.drop(columns=columns_to_drop)\n",
    "    \n",
    "    # 6. Fill remaining missing values based on data type\n",
    "    for col in df_clean.columns:\n",
    "        if df_clean[col].isnull().any():\n",
    "            if df_clean[col].dtype in ['int64', 'float64']:\n",
    "                # For numeric columns, use median\n",
    "                df_clean[col] = df_clean[col].fillna(df_clean[col].median())\n",
    "            else:\n",
    "                # For categorical columns, use mode or 'Unknown'\n",
    "                mode_val = df_clean[col].mode()\n",
    "                if len(mode_val) > 0:\n",
    "                    df_clean[col] = df_clean[col].fillna(mode_val[0])\n",
    "                else:\n",
    "                    df_clean[col] = df_clean[col].fillna('Unknown')\n",
    "    \n",
    "    # 7. Detect and handle potential date columns\n",
    "    potential_date_cols = [col for col in df_clean.columns if 'date' in col.lower() or 'time' in col.lower()]\n",
    "    for col in potential_date_cols:\n",
    "        try:\n",
    "            df_clean[col] = pd.to_datetime(df_clean[col], errors='coerce')\n",
    "            print(f\"  - Converted '{col}' to datetime\")\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # 8. Clean numeric columns\n",
    "    numeric_cols = df_clean.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_cols:\n",
    "        if len(df_clean[col].dropna()) > 0:  # Only if we have numeric data\n",
    "            # Remove outliers using IQR method\n",
    "            Q1 = df_clean[col].quantile(0.25)\n",
    "            Q3 = df_clean[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            \n",
    "            if IQR > 0:  # Only apply if there's variation in the data\n",
    "                lower_bound = Q1 - 1.5 * IQR\n",
    "                upper_bound = Q3 + 1.5 * IQR\n",
    "                \n",
    "                outliers_before = len(df_clean[(df_clean[col] < lower_bound) | (df_clean[col] > upper_bound)])\n",
    "                df_clean[col] = df_clean[col].clip(lower=lower_bound, upper=upper_bound)\n",
    "                if outliers_before > 0:\n",
    "                    print(f\"  - Capped {outliers_before} outliers in '{col}'\")\n",
    "    \n",
    "    # 9. Add city identifier\n",
    "    df_clean['city'] = city\n",
    "    df_clean['source_file'] = filename\n",
    "    \n",
    "    print(f\"  ✓ Cleaned data shape: {original_shape} → {df_clean.shape}\")\n",
    "    return df_clean\n",
    "\n",
    "# 3. Data analysis function\n",
    "def analyze_data(df, filename=\"\", city=\"\"):\n",
    "    \"\"\"Perform comprehensive data analysis\"\"\"\n",
    "    print(f\"\\n--- Analysis for {city}/{filename} ---\")\n",
    "    if df is None or df.empty:\n",
    "        print(\"No data to analyze\")\n",
    "        return df\n",
    "        \n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    \n",
    "    # Missing values analysis\n",
    "    missing_data = df.isnull().sum()\n",
    "    if missing_data.sum() > 0:\n",
    "        print(\"\\nMissing values:\")\n",
    "        print(missing_data[missing_data > 0])\n",
    "    else:\n",
    "        print(\"\\n✓ No missing values!\")\n",
    "    \n",
    "    # Data types\n",
    "    print(f\"\\nData types:\")\n",
    "    print(df.dtypes.value_counts())\n",
    "    \n",
    "    # Numeric columns summary\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    if len(numeric_cols) > 0:\n",
    "        print(f\"\\nNumeric columns summary:\")\n",
    "        print(df[numeric_cols].describe())\n",
    "    \n",
    "    # Categorical columns info\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "    if len(categorical_cols) > 0:\n",
    "        print(f\"\\nCategorical columns (top 3 each):\")\n",
    "        for col in categorical_cols[:3]:  # Limit to first 3 to avoid clutter\n",
    "            print(f\"\\n{col}:\")\n",
    "            print(df[col].value_counts().head(3))\n",
    "    \n",
    "    # Display sample\n",
    "    print(f\"\\nSample data (first 3 rows):\")\n",
    "    print(df.head(3))\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 4. Load data with multiple separator attempts\n",
    "def load_data_file(file_path):\n",
    "    \"\"\"Try to load a data file with different separators\"\"\"\n",
    "    file_extension = os.path.splitext(file_path)[1].lower()\n",
    "    \n",
    "    separators_to_try = [',', '\\t', ';', '|', r'\\s+']\n",
    "    \n",
    "    for sep in separators_to_try:\n",
    "        try:\n",
    "            if sep == r'\\s+':\n",
    "                df = pd.read_csv(file_path, sep=sep, engine='python')\n",
    "            else:\n",
    "                df = pd.read_csv(file_path, sep=sep)\n",
    "            \n",
    "            # Check if data loaded properly (more than just headers)\n",
    "            if not df.empty and len(df.columns) > 1:\n",
    "                print(f\"  ✓ Loaded with separator '{sep}': {df.shape}\")\n",
    "                return df\n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    print(f\"  ❌ Could not load {file_path} with any separator\")\n",
    "    return None\n",
    "\n",
    "# 5. Create output directories\n",
    "def create_output_structure():\n",
    "    \"\"\"Create clean directory structure\"\"\"\n",
    "    # Create main output directory\n",
    "    output_dir = Path('cleaned_trotro_data')\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Create subdirectories\n",
    "    (output_dir / 'by_city').mkdir(exist_ok=True)\n",
    "    (output_dir / 'combined').mkdir(exist_ok=True)\n",
    "    (output_dir / 'reports').mkdir(exist_ok=True)\n",
    "    \n",
    "    return output_dir\n",
    "\n",
    "# 6. Main execution function\n",
    "def main():\n",
    "    \"\"\"Main execution function for multi-city cleaning\"\"\"\n",
    "    \n",
    "    # Download dataset\n",
    "    if not download_dataset():\n",
    "        print(\"Failed to download dataset. Please check your Kaggle credentials.\")\n",
    "        return\n",
    "    \n",
    "    # Find data directory structure\n",
    "    base_data_dir = Path('trotro')\n",
    "    if not base_data_dir.exists():\n",
    "        # Try alternative directory names\n",
    "        possible_dirs = [d for d in Path('.').iterdir() if d.is_dir() and 'trotro' in d.name.lower()]\n",
    "        if possible_dirs:\n",
    "            base_data_dir = possible_dirs[0]\n",
    "        else:\n",
    "            print(\"Could not find dataset directory\")\n",
    "            return\n",
    "    \n",
    "    # Look for the nested structure\n",
    "    datasets_dir = base_data_dir / 'trotrolive-datasets'\n",
    "    if not datasets_dir.exists():\n",
    "        datasets_dir = base_data_dir\n",
    "    \n",
    "    print(f\"Using data directory: {datasets_dir}\")\n",
    "    \n",
    "    # Create output structure\n",
    "    output_dir = create_output_structure()\n",
    "    \n",
    "    # Find all city directories\n",
    "    city_dirs = [d for d in datasets_dir.iterdir() if d.is_dir()]\n",
    "    print(f\"Found {len(city_dirs)} city directories: {[d.name for d in city_dirs]}\")\n",
    "    \n",
    "    if not city_dirs:\n",
    "        print(\"No city directories found\")\n",
    "        return\n",
    "    \n",
    "    # Storage for all cleaned data\n",
    "    all_city_data = {}\n",
    "    all_files_data = []\n",
    "    \n",
    "    # Process each city\n",
    "    for city_dir in city_dirs:\n",
    "        city_name = city_dir.name\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"PROCESSING CITY: {city_name.upper()}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Find data files in city directory\n",
    "        data_files = []\n",
    "        for ext in ['*.txt', '*.csv']:\n",
    "            data_files.extend(city_dir.glob(ext))\n",
    "        \n",
    "        if not data_files:\n",
    "            print(f\"No data files found in {city_name}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"Found {len(data_files)} data files in {city_name}: {[f.name for f in data_files]}\")\n",
    "        \n",
    "        # Storage for this city's data\n",
    "        city_cleaned_data = {}\n",
    "        \n",
    "        # Process each file in the city\n",
    "        for data_file in data_files:\n",
    "            try:\n",
    "                print(f\"\\n{'-'*40}\")\n",
    "                print(f\"Processing: {city_name}/{data_file.name}\")\n",
    "                print(f\"{'-'*40}\")\n",
    "                \n",
    "                # Load data\n",
    "                df = load_data_file(data_file)\n",
    "                if df is None:\n",
    "                    continue\n",
    "                \n",
    "                # Clean data\n",
    "                df_clean = clean_data(df, data_file.name, city_name)\n",
    "                if df_clean is None:\n",
    "                    continue\n",
    "                \n",
    "                # Analyze cleaned data\n",
    "                df_analyzed = analyze_data(df_clean, data_file.name, city_name)\n",
    "                \n",
    "                # Save individual cleaned file\n",
    "                city_output_dir = output_dir / 'by_city' / city_name\n",
    "                city_output_dir.mkdir(exist_ok=True)\n",
    "                \n",
    "                cleaned_filename = city_output_dir / f'cleaned_{data_file.stem}.csv'\n",
    "                df_clean.to_csv(cleaned_filename, index=False)\n",
    "                print(f\"✓ Saved to: {cleaned_filename}\")\n",
    "                \n",
    "                # Store for city-level combination\n",
    "                city_cleaned_data[data_file.name] = df_clean\n",
    "                \n",
    "                # Store for global combination\n",
    "                all_files_data.append(df_clean)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error processing {city_name}/{data_file.name}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Combine all files for this city\n",
    "        if city_cleaned_data:\n",
    "            print(f\"\\n{'-'*40}\")\n",
    "            print(f\"COMBINING DATA FOR {city_name.upper()}\")\n",
    "            print(f\"{'-'*40}\")\n",
    "            \n",
    "            try:\n",
    "                # Find common columns across all files in this city\n",
    "                all_columns = [set(df.columns) for df in city_cleaned_data.values()]\n",
    "                common_columns = set.intersection(*all_columns) if all_columns else set()\n",
    "                \n",
    "                if len(common_columns) > 2:  # More than just city and source_file\n",
    "                    print(f\"Found {len(common_columns)} common columns for {city_name}\")\n",
    "                    \n",
    "                    # Combine city data\n",
    "                    city_combined = pd.concat([df[list(common_columns)] for df in city_cleaned_data.values()], \n",
    "                                            ignore_index=True)\n",
    "                    \n",
    "                    # Save city combined data\n",
    "                    city_combined_file = output_dir / 'by_city' / f'{city_name}_combined.csv'\n",
    "                    city_combined.to_csv(city_combined_file, index=False)\n",
    "                    print(f\"✓ City combined data saved: {city_combined_file}\")\n",
    "                    \n",
    "                    # Store for global analysis\n",
    "                    all_city_data[city_name] = city_combined\n",
    "                    \n",
    "                else:\n",
    "                    print(f\"⚠ Too few common columns to combine {city_name} data\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error combining {city_name} data: {e}\")\n",
    "    \n",
    "    # Create global combined dataset\n",
    "    if all_files_data:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"CREATING GLOBAL COMBINED DATASET\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        try:\n",
    "            # Find common columns across ALL files from ALL cities\n",
    "            all_columns = [set(df.columns) for df in all_files_data]\n",
    "            global_common_columns = set.intersection(*all_columns) if all_columns else set()\n",
    "            \n",
    "            if len(global_common_columns) > 2:\n",
    "                print(f\"Found {len(global_common_columns)} common columns across all cities\")\n",
    "                \n",
    "                # Create global combined dataset\n",
    "                global_combined = pd.concat([df[list(global_common_columns)] for df in all_files_data], \n",
    "                                          ignore_index=True)\n",
    "                \n",
    "                # Save global combined data\n",
    "                global_combined_file = output_dir / 'combined' / 'all_cities_combined.csv'\n",
    "                global_combined.to_csv(global_combined_file, index=False)\n",
    "                print(f\"✓ Global combined data saved: {global_combined_file}\")\n",
    "                \n",
    "                # Generate summary report\n",
    "                summary_report = f\"\"\"\n",
    "TroTro Dataset Cleaning Summary Report\n",
    "Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "Cities Processed: {len(all_city_data)}\n",
    "Total Files Processed: {len(all_files_data)}\n",
    "Global Combined Dataset Shape: {global_combined.shape}\n",
    "\n",
    "City Breakdown:\n",
    "\"\"\"\n",
    "                for city, data in all_city_data.items():\n",
    "                    summary_report += f\"- {city}: {data.shape[0]} records, {data.shape[1]} columns\\n\"\n",
    "                \n",
    "                summary_report += f\"\\nCommon Columns Across All Data:\\n\"\n",
    "                for col in sorted(global_common_columns):\n",
    "                    summary_report += f\"- {col}\\n\"\n",
    "                \n",
    "                # Save summary report\n",
    "                report_file = output_dir / 'reports' / 'cleaning_summary.txt'\n",
    "                with open(report_file, 'w') as f:\n",
    "                    f.write(summary_report)\n",
    "                \n",
    "                print(f\"✓ Summary report saved: {report_file}\")\n",
    "                \n",
    "            else:\n",
    "                print(\"⚠ Too few common columns to create global combined dataset\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error creating global combined dataset: {e}\")\n",
    "    \n",
    "    # Final summary\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"CLEANING COMPLETE!\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Output directory: {output_dir}\")\n",
    "    print(f\"- Individual city files: {output_dir / 'by_city'}\")\n",
    "    print(f\"- Combined datasets: {output_dir / 'combined'}\")\n",
    "    print(f\"- Reports: {output_dir / 'reports'}\")\n",
    "    print(f\"Finished at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# Execute the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
