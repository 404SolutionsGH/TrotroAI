{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be0a12cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: opendatasets in c:\\users\\cbotc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (0.1.22)\n",
      "Requirement already satisfied: pandas in c:\\users\\cbotc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (2.3.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\cbotc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (2.3.2)\n",
      "Requirement already satisfied: legacy-cgi in c:\\users\\cbotc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (2.6.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\cbotc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from opendatasets) (4.67.1)\n",
      "Requirement already satisfied: kaggle in c:\\users\\cbotc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from opendatasets) (1.7.4.5)\n",
      "Requirement already satisfied: click in c:\\users\\cbotc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from opendatasets) (8.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\cbotc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\cbotc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\cbotc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\cbotc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\cbotc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from click->opendatasets) (0.4.6)\n",
      "Requirement already satisfied: bleach in c:\\users\\cbotc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from kaggle->opendatasets) (6.2.0)\n",
      "Requirement already satisfied: certifi>=14.05.14 in c:\\users\\cbotc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from kaggle->opendatasets) (2025.8.3)\n",
      "Requirement already satisfied: charset-normalizer in c:\\users\\cbotc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from kaggle->opendatasets) (3.4.3)\n",
      "Requirement already satisfied: idna in c:\\users\\cbotc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from kaggle->opendatasets) (3.10)\n",
      "Requirement already satisfied: protobuf in c:\\users\\cbotc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from kaggle->opendatasets) (6.31.1)\n",
      "Requirement already satisfied: python-slugify in c:\\users\\cbotc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from kaggle->opendatasets) (8.0.4)\n",
      "Requirement already satisfied: requests in c:\\users\\cbotc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from kaggle->opendatasets) (2.32.4)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in c:\\users\\cbotc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from kaggle->opendatasets) (80.9.0)\n",
      "Requirement already satisfied: text-unidecode in c:\\users\\cbotc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from kaggle->opendatasets) (1.3)\n",
      "Requirement already satisfied: urllib3>=1.15.1 in c:\\users\\cbotc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from kaggle->opendatasets) (2.5.0)\n",
      "Requirement already satisfied: webencodings in c:\\users\\cbotc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from kaggle->opendatasets) (0.5.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
      "[notice] To update, run: C:\\Users\\cbotc\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install required packages if not already installed\n",
    "%pip install opendatasets pandas numpy legacy-cgi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb7ae6d3-1d64-4cfe-b289-bfba11f00a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TroTro Multi-City Dataset Cleaning and Analysis ===\n",
      "Starting analysis at: 2025-08-11 08:44:35\n",
      "Downloading dataset...\n",
      "Skipping, found downloaded files in \".\\trotro\" (use force=True to force download)\n",
      "✓ Dataset downloaded successfully\n",
      "Using data directory: trotro\\trotrolive-datasets\n",
      "Found 11 city directories: ['abidjan', 'accra', 'addisababa', 'addisababa-minibus', 'alexendria', 'bamako', 'freetown', 'kampala', 'kumasi', 'lagos', 'nairobi']\n",
      "\n",
      "============================================================\n",
      "PROCESSING CITY: ABIDJAN\n",
      "============================================================\n",
      "Found 15 data files in abidjan: ['agency.txt', 'calendar.txt', 'calendar_dates.txt', 'fare_attributes.txt', 'fare_rules.txt', 'feed_info.txt', 'frequencies.txt', 'levels.txt', 'pathways.txt', 'routes.txt', 'shapes.txt', 'stops.txt', 'stop_times.txt', 'transfers.txt', 'trips.txt']\n",
      "\n",
      "----------------------------------------\n",
      "Processing: abidjan/agency.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (25, 8)\n",
      "\n",
      "Cleaning data for abidjan/agency.txt...\n",
      "  - Converted 'agency_timezone' to datetime\n",
      "  ✓ Cleaned data shape: (25, 8) → (25, 6)\n",
      "\n",
      "--- Analysis for abidjan/agency.txt ---\n",
      "Shape: (25, 6)\n",
      "Memory usage: 0.01 MB\n",
      "\n",
      "Missing values:\n",
      "agency_timezone    25\n",
      "dtype: int64\n",
      "\n",
      "Data types:\n",
      "object            5\n",
      "datetime64[ns]    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "agency_id:\n",
      "agency_id\n",
      "Gbaka d'Abobo        1\n",
      "Gbaka d'Adjamé       1\n",
      "Gbaka d'Attécoubé    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "agency_name:\n",
      "agency_name\n",
      "Woro-woro d'Attécoubé    3\n",
      "Gbaka d'Adjamé           1\n",
      "Gbaka d'Abobo            1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "agency_url:\n",
      "agency_url\n",
      "https://data-transport.org    25\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "           agency_id        agency_name agency_timezone  \\\n",
      "0      Gbaka d'Abobo      Gbaka d'Abobo             NaT   \n",
      "1     Gbaka d'Adjamé     Gbaka d'Adjamé             NaT   \n",
      "2  Gbaka d'Attécoubé  Gbaka d'Attécoubé             NaT   \n",
      "\n",
      "                   agency_url     city source_file  \n",
      "0  https://data-transport.org  abidjan  agency.txt  \n",
      "1  https://data-transport.org  abidjan  agency.txt  \n",
      "2  https://data-transport.org  abidjan  agency.txt  \n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\abidjan\\cleaned_agency.csv\n",
      "\n",
      "----------------------------------------\n",
      "Processing: abidjan/calendar.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (1, 10)\n",
      "\n",
      "Cleaning data for abidjan/calendar.txt...\n",
      "  - Converted 'start_date' to datetime\n",
      "  - Converted 'end_date' to datetime\n",
      "  ✓ Cleaned data shape: (1, 10) → (1, 12)\n",
      "\n",
      "--- Analysis for abidjan/calendar.txt ---\n",
      "Shape: (1, 12)\n",
      "Memory usage: 0.00 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "int64             7\n",
      "object            3\n",
      "datetime64[ns]    2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Numeric columns summary:\n",
      "       monday  tuesday  wednesday  thursday  friday  saturday  sunday\n",
      "count     1.0      1.0        1.0       1.0     1.0       1.0     1.0\n",
      "mean      1.0      1.0        1.0       1.0     1.0       1.0     1.0\n",
      "std       NaN      NaN        NaN       NaN     NaN       NaN     NaN\n",
      "min       1.0      1.0        1.0       1.0     1.0       1.0     1.0\n",
      "25%       1.0      1.0        1.0       1.0     1.0       1.0     1.0\n",
      "50%       1.0      1.0        1.0       1.0     1.0       1.0     1.0\n",
      "75%       1.0      1.0        1.0       1.0     1.0       1.0     1.0\n",
      "max       1.0      1.0        1.0       1.0     1.0       1.0     1.0\n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "service_id:\n",
      "service_id\n",
      "Mo-Su    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "city:\n",
      "city\n",
      "abidjan    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "source_file:\n",
      "source_file\n",
      "calendar.txt    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "  service_id  monday  tuesday  wednesday  thursday  friday  saturday  sunday  \\\n",
      "0      Mo-Su       1        1          1         1       1         1       1   \n",
      "\n",
      "                     start_date                      end_date     city  \\\n",
      "0 1970-01-01 00:00:00.020230101 1970-01-01 00:00:00.020231231  abidjan   \n",
      "\n",
      "    source_file  \n",
      "0  calendar.txt  \n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\abidjan\\cleaned_calendar.csv\n",
      "\n",
      "----------------------------------------\n",
      "Processing: abidjan/calendar_dates.txt\n",
      "----------------------------------------\n",
      "  ❌ Could not load trotro\\trotrolive-datasets\\abidjan\\calendar_dates.txt with any separator\n",
      "\n",
      "----------------------------------------\n",
      "Processing: abidjan/fare_attributes.txt\n",
      "----------------------------------------\n",
      "  ❌ Could not load trotro\\trotrolive-datasets\\abidjan\\fare_attributes.txt with any separator\n",
      "\n",
      "----------------------------------------\n",
      "Processing: abidjan/fare_rules.txt\n",
      "----------------------------------------\n",
      "  ❌ Could not load trotro\\trotrolive-datasets\\abidjan\\fare_rules.txt with any separator\n",
      "\n",
      "----------------------------------------\n",
      "Processing: abidjan/feed_info.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (1, 9)\n",
      "\n",
      "Cleaning data for abidjan/feed_info.txt...\n",
      "  - Converted 'feed_start_date' to datetime\n",
      "  - Converted 'feed_end_date' to datetime\n",
      "  ✓ Cleaned data shape: (1, 9) → (1, 10)\n",
      "\n",
      "--- Analysis for abidjan/feed_info.txt ---\n",
      "Shape: (1, 10)\n",
      "Memory usage: 0.00 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "object            7\n",
      "datetime64[ns]    2\n",
      "float64           1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Numeric columns summary:\n",
      "       feed_version\n",
      "count           1.0\n",
      "mean            0.1\n",
      "std             NaN\n",
      "min             0.1\n",
      "25%             0.1\n",
      "50%             0.1\n",
      "75%             0.1\n",
      "max             0.1\n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "feed_publisher_name:\n",
      "feed_publisher_name\n",
      "Data Transport    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "feed_publisher_url:\n",
      "feed_publisher_url\n",
      "http://data-transport.org    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "feed_lang:\n",
      "feed_lang\n",
      "fr    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "  feed_publisher_name         feed_publisher_url feed_lang  \\\n",
      "0      Data Transport  http://data-transport.org        fr   \n",
      "\n",
      "                feed_start_date                 feed_end_date  feed_version  \\\n",
      "0 1970-01-01 00:00:00.020230101 1970-01-01 00:00:00.020231231           0.1   \n",
      "\n",
      "        feed_contact_email           feed_contact_url     city    source_file  \n",
      "0  labs@data-transport.org  http://data-transport.org  abidjan  feed_info.txt  \n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\abidjan\\cleaned_feed_info.csv\n",
      "\n",
      "----------------------------------------\n",
      "Processing: abidjan/frequencies.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (1076, 5)\n",
      "\n",
      "Cleaning data for abidjan/frequencies.txt...\n",
      "  - Converted 'start_time' to datetime\n",
      "  - Converted 'end_time' to datetime\n",
      "  - Converted 'exact_times' to datetime\n",
      "  - Capped 18 outliers in 'headway_secs'\n",
      "  ✓ Cleaned data shape: (1076, 5) → (1076, 7)\n",
      "\n",
      "--- Analysis for abidjan/frequencies.txt ---\n",
      "Shape: (1076, 7)\n",
      "Memory usage: 0.16 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "datetime64[ns]    3\n",
      "int64             2\n",
      "object            2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Numeric columns summary:\n",
      "          trip_id  headway_secs\n",
      "count  1076.00000   1076.000000\n",
      "mean    458.89777   1251.500000\n",
      "std     290.82871    982.664719\n",
      "min       0.00000    120.000000\n",
      "25%     200.75000    600.000000\n",
      "50%     441.50000    900.000000\n",
      "75%     710.25000   1800.000000\n",
      "max     979.00000   3600.000000\n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "city:\n",
      "city\n",
      "abidjan    1076\n",
      "Name: count, dtype: int64\n",
      "\n",
      "source_file:\n",
      "source_file\n",
      "frequencies.txt    1076\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "   trip_id          start_time            end_time  headway_secs exact_times  \\\n",
      "0        0 2025-08-11 05:00:00 2025-08-11 22:00:00           900  1970-01-01   \n",
      "1        1 2025-08-11 05:00:00 2025-08-11 22:00:00           900  1970-01-01   \n",
      "2       10 2025-08-11 05:00:00 2025-08-11 22:00:00           600  1970-01-01   \n",
      "\n",
      "      city      source_file  \n",
      "0  abidjan  frequencies.txt  \n",
      "1  abidjan  frequencies.txt  \n",
      "2  abidjan  frequencies.txt  \n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\abidjan\\cleaned_frequencies.csv\n",
      "\n",
      "----------------------------------------\n",
      "Processing: abidjan/levels.txt\n",
      "----------------------------------------\n",
      "  ❌ Could not load trotro\\trotrolive-datasets\\abidjan\\levels.txt with any separator\n",
      "\n",
      "----------------------------------------\n",
      "Processing: abidjan/pathways.txt\n",
      "----------------------------------------\n",
      "  ❌ Could not load trotro\\trotrolive-datasets\\abidjan\\pathways.txt with any separator\n",
      "\n",
      "----------------------------------------\n",
      "Processing: abidjan/routes.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (490, 12)\n",
      "\n",
      "Cleaning data for abidjan/routes.txt...\n",
      "  ✓ Cleaned data shape: (490, 12) → (490, 8)\n",
      "\n",
      "--- Analysis for abidjan/routes.txt ---\n",
      "Shape: (490, 8)\n",
      "Memory usage: 0.22 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "object    6\n",
      "int64     2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Numeric columns summary:\n",
      "           route_id  route_type\n",
      "count  4.900000e+02       490.0\n",
      "mean   1.334756e+07         3.0\n",
      "std    2.032172e+06         0.0\n",
      "min    1.017901e+07         3.0\n",
      "25%    1.158108e+07         3.0\n",
      "50%    1.349794e+07         3.0\n",
      "75%    1.534721e+07         3.0\n",
      "max    1.582049e+07         3.0\n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "agency_id:\n",
      "agency_id\n",
      "Woro-woro de Cocody    87\n",
      "Woro-woro Banalisé     85\n",
      "Gbaka d'Adjamé         65\n",
      "Name: count, dtype: int64\n",
      "\n",
      "route_long_name:\n",
      "route_long_name\n",
      "gbaka : Adjamé Liberté ↔ Treichville Gare de Bassam    1\n",
      "gbaka : Adjamé ↔ Abobo Gare Mairie                     1\n",
      "gbaka : Adjamé Liberté ↔ Abobo Gare Mairie             1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "route_color:\n",
      "route_color\n",
      "1779C2    490\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "        agency_id  route_id  route_type  \\\n",
      "0  Gbaka d'Adjamé  10179006           3   \n",
      "1   Gbaka d'Abobo  10179435           3   \n",
      "2   Gbaka d'Abobo  10184139           3   \n",
      "\n",
      "                                     route_long_name route_color  \\\n",
      "0  gbaka : Adjamé Liberté ↔ Treichville Gare de B...      1779C2   \n",
      "1                 gbaka : Adjamé ↔ Abobo Gare Mairie      1779C2   \n",
      "2         gbaka : Adjamé Liberté ↔ Abobo Gare Mairie      1779C2   \n",
      "\n",
      "  route_text_color     city source_file  \n",
      "0           FFFFFF  abidjan  routes.txt  \n",
      "1           FFFFFF  abidjan  routes.txt  \n",
      "2           FFFFFF  abidjan  routes.txt  \n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\abidjan\\cleaned_routes.csv\n",
      "\n",
      "----------------------------------------\n",
      "Processing: abidjan/shapes.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (74504, 4)\n",
      "\n",
      "Cleaning data for abidjan/shapes.txt...\n",
      "  - Capped 9063 outliers in 'shape_pt_lat'\n",
      "  - Capped 3391 outliers in 'shape_pt_lon'\n",
      "  - Capped 1440 outliers in 'shape_pt_sequence'\n",
      "  ✓ Cleaned data shape: (74504, 4) → (74504, 6)\n",
      "\n",
      "--- Analysis for abidjan/shapes.txt ---\n",
      "Shape: (74504, 6)\n",
      "Memory usage: 10.44 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "int64      2\n",
      "float64    2\n",
      "object     2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Numeric columns summary:\n",
      "           shape_id  shape_pt_lat  shape_pt_lon  shape_pt_sequence\n",
      "count  7.450400e+04  74504.000000  74504.000000       74504.000000\n",
      "mean   1.361995e+07      5.351968     -4.011938          59.320775\n",
      "std    2.144032e+06      0.032583      0.046324          48.859261\n",
      "min    1.017867e+07      5.283110     -4.131610           0.000000\n",
      "25%    1.156933e+07      5.332025     -4.042980          19.000000\n",
      "50%    1.508844e+07      5.353904     -4.014915          46.000000\n",
      "75%    1.541301e+07      5.364635     -3.983893          89.000000\n",
      "max    1.582049e+07      5.413550     -3.895263         194.000000\n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "city:\n",
      "city\n",
      "abidjan    74504\n",
      "Name: count, dtype: int64\n",
      "\n",
      "source_file:\n",
      "source_file\n",
      "shapes.txt    74504\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "   shape_id  shape_pt_lat  shape_pt_lon  shape_pt_sequence     city  \\\n",
      "0  10178669      5.353258     -4.014545                  0  abidjan   \n",
      "1  10178669      5.353220     -4.014481                  1  abidjan   \n",
      "2  10178669      5.353076     -4.014356                  2  abidjan   \n",
      "\n",
      "  source_file  \n",
      "0  shapes.txt  \n",
      "1  shapes.txt  \n",
      "2  shapes.txt  \n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\abidjan\\cleaned_shapes.csv\n",
      "\n",
      "----------------------------------------\n",
      "Processing: abidjan/stops.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (4834, 14)\n",
      "\n",
      "Cleaning data for abidjan/stops.txt...\n",
      "  - Capped 163 outliers in 'stop_lat'\n",
      "  - Capped 122 outliers in 'stop_lon'\n",
      "  ✓ Cleaned data shape: (4834, 14) → (4834, 6)\n",
      "\n",
      "--- Analysis for abidjan/stops.txt ---\n",
      "Shape: (4834, 6)\n",
      "Memory usage: 1.21 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "object     4\n",
      "float64    2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Numeric columns summary:\n",
      "          stop_lat     stop_lon\n",
      "count  4834.000000  4834.000000\n",
      "mean      5.358113    -3.999570\n",
      "std       0.050646     0.067331\n",
      "min       5.238013    -4.180979\n",
      "25%       5.327884    -4.050139\n",
      "50%       5.355141    -4.004590\n",
      "75%       5.387798    -3.962913\n",
      "max       5.477670    -3.832073\n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "stop_id:\n",
      "stop_id\n",
      "node/10218963489    1\n",
      "node/10218963490    1\n",
      "node/10218963491    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "stop_name:\n",
      "stop_name\n",
      "Carrefour Marché     40\n",
      "Carrefour Mosquée    29\n",
      "Boulangerie          25\n",
      "Name: count, dtype: int64\n",
      "\n",
      "city:\n",
      "city\n",
      "abidjan    4834\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "            stop_id          stop_name  stop_lat  stop_lon     city  \\\n",
      "0  node/10218963489         Chez Rifat  5.349222 -3.975893  abidjan   \n",
      "1  node/10218963490   Carrefour garage  5.347851 -3.975529  abidjan   \n",
      "2  node/10218963491  Gorille carrefour  5.346528 -3.975153  abidjan   \n",
      "\n",
      "  source_file  \n",
      "0   stops.txt  \n",
      "1   stops.txt  \n",
      "2   stops.txt  \n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\abidjan\\cleaned_stops.csv\n",
      "\n",
      "----------------------------------------\n",
      "Processing: abidjan/stop_times.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (15683, 11)\n",
      "\n",
      "Cleaning data for abidjan/stop_times.txt...\n",
      "  - Converted 'arrival_time' to datetime\n",
      "  - Converted 'departure_time' to datetime\n",
      "  - Converted 'timepoint' to datetime\n",
      "  - Capped 501 outliers in 'stop_sequence'\n",
      "  ✓ Cleaned data shape: (15683, 11) → (15683, 8)\n",
      "\n",
      "--- Analysis for abidjan/stop_times.txt ---\n",
      "Shape: (15683, 8)\n",
      "Memory usage: 3.34 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "object            3\n",
      "datetime64[ns]    3\n",
      "int64             2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Numeric columns summary:\n",
      "            trip_id  stop_sequence\n",
      "count  15683.000000   15683.000000\n",
      "mean     513.637059      12.968820\n",
      "std      297.657314      10.524426\n",
      "min        0.000000       0.000000\n",
      "25%      256.000000       5.000000\n",
      "50%      560.000000      10.000000\n",
      "75%      769.000000      19.000000\n",
      "max      979.000000      40.000000\n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "stop_id:\n",
      "stop_id\n",
      "node/7052366501    27\n",
      "node/7052366502    24\n",
      "node/7054787060    24\n",
      "Name: count, dtype: int64\n",
      "\n",
      "city:\n",
      "city\n",
      "abidjan    15683\n",
      "Name: count, dtype: int64\n",
      "\n",
      "source_file:\n",
      "source_file\n",
      "stop_times.txt    15683\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "   trip_id           stop_id  stop_sequence        arrival_time  \\\n",
      "0        0  node/10591673049              7 2025-08-11 07:49:56   \n",
      "1        0  node/10591673050              6 2025-08-11 07:45:08   \n",
      "2        0  node/10591673051              4 2025-08-11 07:38:38   \n",
      "\n",
      "       departure_time  timepoint     city     source_file  \n",
      "0 2025-08-11 07:49:56 1970-01-01  abidjan  stop_times.txt  \n",
      "1 2025-08-11 07:45:08 1970-01-01  abidjan  stop_times.txt  \n",
      "2 2025-08-11 07:38:38 1970-01-01  abidjan  stop_times.txt  \n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\abidjan\\cleaned_stop_times.csv\n",
      "\n",
      "----------------------------------------\n",
      "Processing: abidjan/transfers.txt\n",
      "----------------------------------------\n",
      "  ❌ Could not load trotro\\trotrolive-datasets\\abidjan\\transfers.txt with any separator\n",
      "\n",
      "----------------------------------------\n",
      "Processing: abidjan/trips.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (980, 10)\n",
      "\n",
      "Cleaning data for abidjan/trips.txt...\n",
      "  ✓ Cleaned data shape: (980, 10) → (980, 8)\n",
      "\n",
      "--- Analysis for abidjan/trips.txt ---\n",
      "Shape: (980, 8)\n",
      "Memory usage: 0.25 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "int64     4\n",
      "object    4\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Numeric columns summary:\n",
      "           route_id      shape_id     trip_id  direction_id\n",
      "count  9.800000e+02  9.800000e+02  980.000000    980.000000\n",
      "mean   1.334756e+07  1.334893e+07  489.500000      0.500000\n",
      "std    2.031134e+06  2.027481e+06  283.045933      0.500255\n",
      "min    1.017901e+07  1.017867e+07    0.000000      0.000000\n",
      "25%    1.158107e+07  1.158111e+07  244.750000      0.000000\n",
      "50%    1.349794e+07  1.349807e+07  489.500000      0.500000\n",
      "75%    1.534722e+07  1.534717e+07  734.250000      1.000000\n",
      "max    1.582049e+07  1.582049e+07  979.000000      1.000000\n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "service_id:\n",
      "service_id\n",
      "Mo-Su    980\n",
      "Name: count, dtype: int64\n",
      "\n",
      "trip_headsign:\n",
      "trip_headsign\n",
      "Adjamé Liberté    12\n",
      "Palmeraie         12\n",
      "9 kilo            11\n",
      "Name: count, dtype: int64\n",
      "\n",
      "city:\n",
      "city\n",
      "abidjan    980\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "   route_id service_id  shape_id  trip_id               trip_headsign  \\\n",
      "0  10179006      Mo-Su  10178669        0  Treichville Gare de Bassam   \n",
      "1  10179006      Mo-Su  10178996        1              Adjamé Liberté   \n",
      "2  10185142      Mo-Su  10185123       10                 Mosquée Ado   \n",
      "\n",
      "   direction_id     city source_file  \n",
      "0             0  abidjan   trips.txt  \n",
      "1             1  abidjan   trips.txt  \n",
      "2             0  abidjan   trips.txt  \n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\abidjan\\cleaned_trips.csv\n",
      "\n",
      "----------------------------------------\n",
      "COMBINING DATA FOR ABIDJAN\n",
      "----------------------------------------\n",
      "⚠ Too few common columns to combine abidjan data\n",
      "\n",
      "============================================================\n",
      "PROCESSING CITY: ACCRA\n",
      "============================================================\n",
      "Found 9 data files in accra: ['agency.txt', 'calendar.txt', 'fare_attributes.txt', 'fare_rules.txt', 'routes.txt', 'shapes.txt', 'stops.txt', 'stop_times.txt', 'trips.txt']\n",
      "\n",
      "----------------------------------------\n",
      "Processing: accra/agency.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (91, 4)\n",
      "\n",
      "Cleaning data for accra/agency.txt...\n",
      "  - Converted 'agency_timezone' to datetime\n",
      "  ✓ Cleaned data shape: (91, 4) → (91, 6)\n",
      "\n",
      "--- Analysis for accra/agency.txt ---\n",
      "Shape: (91, 6)\n",
      "Memory usage: 0.03 MB\n",
      "\n",
      "Missing values:\n",
      "agency_timezone    91\n",
      "dtype: int64\n",
      "\n",
      "Data types:\n",
      "object            4\n",
      "int64             1\n",
      "datetime64[ns]    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Numeric columns summary:\n",
      "        agency_id\n",
      "count   91.000000\n",
      "mean    51.725275\n",
      "std     30.269554\n",
      "min      0.000000\n",
      "25%     25.500000\n",
      "50%     54.000000\n",
      "75%     78.500000\n",
      "max    101.000000\n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "agency_name:\n",
      "agency_name\n",
      "East Legon-La Bawaleshie-M-A-S-A GPRTU                  3\n",
      "Lapaz Branch of Tiger Transport Services Association    2\n",
      "Abeka Lapaz Highway (GPRTU)                             1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "agency_url:\n",
      "agency_url\n",
      "https://en.wikipedia.org/wiki/Ghana_Private_Road_Transport_Union    91\n",
      "Name: count, dtype: int64\n",
      "\n",
      "city:\n",
      "city\n",
      "accra    91\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "   agency_id                                        agency_name  \\\n",
      "0          0             East Legon-La Bawaleshie-M-A-S-A GPRTU   \n",
      "1          1  Abeka Lapaz Circle Accra Local of Abeka Branch...   \n",
      "2          2                        Abeka Lapaz Highway (GPRTU)   \n",
      "\n",
      "                                          agency_url agency_timezone   city  \\\n",
      "0  https://en.wikipedia.org/wiki/Ghana_Private_Ro...             NaT  accra   \n",
      "1  https://en.wikipedia.org/wiki/Ghana_Private_Ro...             NaT  accra   \n",
      "2  https://en.wikipedia.org/wiki/Ghana_Private_Ro...             NaT  accra   \n",
      "\n",
      "  source_file  \n",
      "0  agency.txt  \n",
      "1  agency.txt  \n",
      "2  agency.txt  \n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\accra\\cleaned_agency.csv\n",
      "\n",
      "----------------------------------------\n",
      "Processing: accra/calendar.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (1, 10)\n",
      "\n",
      "Cleaning data for accra/calendar.txt...\n",
      "  - Converted 'start_date' to datetime\n",
      "  - Converted 'end_date' to datetime\n",
      "  ✓ Cleaned data shape: (1, 10) → (1, 12)\n",
      "\n",
      "--- Analysis for accra/calendar.txt ---\n",
      "Shape: (1, 12)\n",
      "Memory usage: 0.00 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "int64             7\n",
      "object            3\n",
      "datetime64[ns]    2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Numeric columns summary:\n",
      "       monday  tuesday  wednesday  thursday  friday  saturday  sunday\n",
      "count     1.0      1.0        1.0       1.0     1.0       1.0     1.0\n",
      "mean      1.0      1.0        1.0       1.0     1.0       1.0     1.0\n",
      "std       NaN      NaN        NaN       NaN     NaN       NaN     NaN\n",
      "min       1.0      1.0        1.0       1.0     1.0       1.0     1.0\n",
      "25%       1.0      1.0        1.0       1.0     1.0       1.0     1.0\n",
      "50%       1.0      1.0        1.0       1.0     1.0       1.0     1.0\n",
      "75%       1.0      1.0        1.0       1.0     1.0       1.0     1.0\n",
      "max       1.0      1.0        1.0       1.0     1.0       1.0     1.0\n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "service_id:\n",
      "service_id\n",
      "service    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "city:\n",
      "city\n",
      "accra    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "source_file:\n",
      "source_file\n",
      "calendar.txt    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "  service_id  monday  tuesday  wednesday  thursday  friday  saturday  sunday  \\\n",
      "0    service       1        1          1         1       1         1       1   \n",
      "\n",
      "                     start_date                      end_date   city  \\\n",
      "0 1970-01-01 00:00:00.020150520 1970-01-01 00:00:00.020170531  accra   \n",
      "\n",
      "    source_file  \n",
      "0  calendar.txt  \n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\accra\\cleaned_calendar.csv\n",
      "\n",
      "----------------------------------------\n",
      "Processing: accra/fare_attributes.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (31, 6)\n",
      "\n",
      "Cleaning data for accra/fare_attributes.txt...\n",
      "  - Capped 2 outliers in 'price'\n",
      "  ✓ Cleaned data shape: (31, 6) → (31, 6)\n",
      "\n",
      "--- Analysis for accra/fare_attributes.txt ---\n",
      "Shape: (31, 6)\n",
      "Memory usage: 0.01 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "object     4\n",
      "float64    1\n",
      "int64      1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Numeric columns summary:\n",
      "           price  payment_method\n",
      "count  31.000000            31.0\n",
      "mean    2.275806             0.0\n",
      "std     1.228998             0.0\n",
      "min     0.500000             0.0\n",
      "25%     1.350000             0.0\n",
      "50%     2.100000             0.0\n",
      "75%     2.900000             0.0\n",
      "max     5.225000             0.0\n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "fare_id:\n",
      "fare_id\n",
      "fare_00    1\n",
      "fare_01    1\n",
      "fare_02    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "currency_type:\n",
      "currency_type\n",
      "CAD    31\n",
      "Name: count, dtype: int64\n",
      "\n",
      "city:\n",
      "city\n",
      "accra    31\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "   fare_id  price currency_type  payment_method   city          source_file\n",
      "0  fare_00    0.5           CAD               0  accra  fare_attributes.txt\n",
      "1  fare_01    0.7           CAD               0  accra  fare_attributes.txt\n",
      "2  fare_02    0.8           CAD               0  accra  fare_attributes.txt\n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\accra\\cleaned_fare_attributes.csv\n",
      "\n",
      "----------------------------------------\n",
      "Processing: accra/fare_rules.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (608, 5)\n",
      "\n",
      "Cleaning data for accra/fare_rules.txt...\n",
      "  ✓ Cleaned data shape: (608, 5) → (608, 4)\n",
      "\n",
      "--- Analysis for accra/fare_rules.txt ---\n",
      "Shape: (608, 4)\n",
      "Memory usage: 0.13 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "object    4\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "route_id:\n",
      "route_id\n",
      "002A    1\n",
      "002B    1\n",
      "003A    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "fare_id:\n",
      "fare_id\n",
      "fare_09    72\n",
      "fare_07    69\n",
      "fare_14    68\n",
      "Name: count, dtype: int64\n",
      "\n",
      "city:\n",
      "city\n",
      "accra    608\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "  route_id  fare_id   city     source_file\n",
      "0     002A  fare_06  accra  fare_rules.txt\n",
      "1     002B  fare_06  accra  fare_rules.txt\n",
      "2     003A  fare_07  accra  fare_rules.txt\n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\accra\\cleaned_fare_rules.csv\n",
      "\n",
      "----------------------------------------\n",
      "Processing: accra/routes.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (651, 5)\n",
      "\n",
      "Cleaning data for accra/routes.txt...\n",
      "  ✓ Cleaned data shape: (651, 5) → (651, 7)\n",
      "\n",
      "--- Analysis for accra/routes.txt ---\n",
      "Shape: (651, 7)\n",
      "Memory usage: 0.20 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "object    5\n",
      "int64     2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Numeric columns summary:\n",
      "        agency_id  route_type\n",
      "count  651.000000       651.0\n",
      "mean    46.145929         3.0\n",
      "std     30.539486         0.0\n",
      "min      0.000000         3.0\n",
      "25%     21.000000         3.0\n",
      "50%     46.000000         3.0\n",
      "75%     70.000000         3.0\n",
      "max    101.000000         3.0\n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "route_id:\n",
      "route_id\n",
      "385B    1\n",
      "056A    1\n",
      "277B    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "route_short_name:\n",
      "route_short_name\n",
      "385B    1\n",
      "056A    1\n",
      "277B    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "route_long_name:\n",
      "route_long_name\n",
      "Abeka lapaz to Circle                          37\n",
      "Circle Odorna Station to Orgle Road Station     3\n",
      "Abeka lapaz to Korle Bu Station                 3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "  route_id route_short_name  agency_id                 route_long_name  \\\n",
      "0     385B             385B         15  37 Station to New Town Station   \n",
      "1     056A             056A         67      Abeka lapaz to Race Course   \n",
      "2     277B             277B         51  C to Kaneshie Mamprobi Station   \n",
      "\n",
      "   route_type   city source_file  \n",
      "0           3  accra  routes.txt  \n",
      "1           3  accra  routes.txt  \n",
      "2           3  accra  routes.txt  \n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\accra\\cleaned_routes.csv\n",
      "\n",
      "----------------------------------------\n",
      "Processing: accra/shapes.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (86377, 4)\n",
      "\n",
      "Cleaning data for accra/shapes.txt...\n",
      "  - Capped 625 outliers in 'shape_pt_lat'\n",
      "  - Capped 8385 outliers in 'shape_pt_lon'\n",
      "  - Capped 2515 outliers in 'shape_pt_sequence'\n",
      "  ✓ Cleaned data shape: (86377, 4) → (86377, 6)\n",
      "\n",
      "--- Analysis for accra/shapes.txt ---\n",
      "Shape: (86377, 6)\n",
      "Memory usage: 11.94 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "int64      2\n",
      "float64    2\n",
      "object     2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Numeric columns summary:\n",
      "            shape_id  shape_pt_lat  shape_pt_lon  shape_pt_sequence\n",
      "count   86377.000000  86377.000000  86377.000000       86377.000000\n",
      "mean   354400.840235      5.594348     -0.218710          88.011774\n",
      "std    195443.109972      0.041122      0.043346          67.710437\n",
      "min      1001.000000      5.521124     -0.316253           1.000000\n",
      "25%    188001.000000      5.562194     -0.244842          34.000000\n",
      "50%    377001.000000      5.588663     -0.220332          72.000000\n",
      "75%    539001.000000      5.620869     -0.197235         126.000000\n",
      "max    651001.000000      5.708882     -0.125824         264.000000\n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "city:\n",
      "city\n",
      "accra    86377\n",
      "Name: count, dtype: int64\n",
      "\n",
      "source_file:\n",
      "source_file\n",
      "shapes.txt    86377\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "   shape_id  shape_pt_lat  shape_pt_lon  shape_pt_sequence   city source_file\n",
      "0    651001      5.567347     -0.216433                113  accra  shapes.txt\n",
      "1      1001      5.607725     -0.246924                  2  accra  shapes.txt\n",
      "2      1001      5.607516     -0.247544                  3  accra  shapes.txt\n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\accra\\cleaned_shapes.csv\n",
      "\n",
      "----------------------------------------\n",
      "Processing: accra/stops.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (2565, 4)\n",
      "\n",
      "Cleaning data for accra/stops.txt...\n",
      "  - Capped 42 outliers in 'stop_lat'\n",
      "  - Capped 219 outliers in 'stop_lon'\n",
      "  ✓ Cleaned data shape: (2565, 4) → (2565, 6)\n",
      "\n",
      "--- Analysis for accra/stops.txt ---\n",
      "Shape: (2565, 6)\n",
      "Memory usage: 0.60 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "object     4\n",
      "float64    2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Numeric columns summary:\n",
      "          stop_lat     stop_lon\n",
      "count  2565.000000  2565.000000\n",
      "mean      5.595498    -0.228509\n",
      "std       0.046857     0.045382\n",
      "min       5.521120    -0.328380\n",
      "25%       5.559720    -0.254170\n",
      "50%       5.586580    -0.232579\n",
      "75%       5.625255    -0.204697\n",
      "max       5.723558    -0.130487\n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "stop_id:\n",
      "stop_id\n",
      "S4842    1\n",
      "S1001    1\n",
      "S3829    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "stop_name:\n",
      "stop_name\n",
      "Abelemkpe    8\n",
      "Abrantie     6\n",
      "Caprice      6\n",
      "Name: count, dtype: int64\n",
      "\n",
      "city:\n",
      "city\n",
      "accra    2565\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "  stop_id        stop_name  stop_lat  stop_lon   city source_file\n",
      "0   S4842       Junction 1  5.563420 -0.152562  accra   stops.txt\n",
      "1   S1001        Total 447  5.535621 -0.228926  accra   stops.txt\n",
      "2   S3829  Water Works 406  5.580180 -0.276891  accra   stops.txt\n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\accra\\cleaned_stops.csv\n",
      "\n",
      "----------------------------------------\n",
      "Processing: accra/stop_times.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (4796, 5)\n",
      "\n",
      "Cleaning data for accra/stop_times.txt...\n",
      "  - Converted 'arrival_time' to datetime\n",
      "  - Converted 'departure_time' to datetime\n",
      "  - Capped 106 outliers in 'stop_sequence'\n",
      "  ✓ Cleaned data shape: (4796, 5) → (4796, 7)\n",
      "\n",
      "--- Analysis for accra/stop_times.txt ---\n",
      "Shape: (4796, 7)\n",
      "Memory usage: 1.14 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "object            4\n",
      "datetime64[ns]    2\n",
      "float64           1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Numeric columns summary:\n",
      "       stop_sequence\n",
      "count    4796.000000\n",
      "mean        5.158465\n",
      "std         3.461604\n",
      "min         1.000000\n",
      "25%         2.000000\n",
      "50%         4.000000\n",
      "75%         7.000000\n",
      "max        14.500000\n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "trip_id:\n",
      "trip_id\n",
      "406A_1    26\n",
      "256A_1    23\n",
      "120B_1    22\n",
      "Name: count, dtype: int64\n",
      "\n",
      "stop_id:\n",
      "stop_id\n",
      "T0      112\n",
      "T75      66\n",
      "T136     47\n",
      "Name: count, dtype: int64\n",
      "\n",
      "city:\n",
      "city\n",
      "accra    4796\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "  trip_id        arrival_time      departure_time  stop_sequence stop_id  \\\n",
      "0  002A_1 2025-08-11 12:51:38 2025-08-11 12:51:38            1.0   T5622   \n",
      "1  002A_1 2025-08-11 13:01:28 2025-08-11 13:01:28            2.0   S5624   \n",
      "2  002A_1 2025-08-11 13:02:48 2025-08-11 13:02:48            3.0   S3975   \n",
      "\n",
      "    city     source_file  \n",
      "0  accra  stop_times.txt  \n",
      "1  accra  stop_times.txt  \n",
      "2  accra  stop_times.txt  \n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\accra\\cleaned_stop_times.csv\n",
      "\n",
      "----------------------------------------\n",
      "Processing: accra/trips.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (651, 4)\n",
      "\n",
      "Cleaning data for accra/trips.txt...\n",
      "  ✓ Cleaned data shape: (651, 4) → (651, 6)\n",
      "\n",
      "--- Analysis for accra/trips.txt ---\n",
      "Shape: (651, 6)\n",
      "Memory usage: 0.18 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "object    5\n",
      "int64     1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Numeric columns summary:\n",
      "            shape_id\n",
      "count     651.000000\n",
      "mean   326001.000000\n",
      "std    188071.794802\n",
      "min      1001.000000\n",
      "25%    163501.000000\n",
      "50%    326001.000000\n",
      "75%    488501.000000\n",
      "max    651001.000000\n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "route_id:\n",
      "route_id\n",
      "018A    1\n",
      "005A    1\n",
      "005B    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "service_id:\n",
      "service_id\n",
      "service    651\n",
      "Name: count, dtype: int64\n",
      "\n",
      "trip_id:\n",
      "trip_id\n",
      "018A_1    1\n",
      "005A_1    1\n",
      "005B_1    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "  route_id service_id trip_id  shape_id   city source_file\n",
      "0     018A    service  018A_1      1001  accra   trips.txt\n",
      "1     005A    service  005A_1      2001  accra   trips.txt\n",
      "2     005B    service  005B_1      3001  accra   trips.txt\n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\accra\\cleaned_trips.csv\n",
      "\n",
      "----------------------------------------\n",
      "COMBINING DATA FOR ACCRA\n",
      "----------------------------------------\n",
      "⚠ Too few common columns to combine accra data\n",
      "\n",
      "============================================================\n",
      "PROCESSING CITY: ADDISABABA\n",
      "============================================================\n",
      "Found 9 data files in addisababa: ['agency.txt', 'calendar.txt', 'feed_info.txt', 'frequencies.txt', 'routes.txt', 'shapes.txt', 'stops.txt', 'stop_times.txt', 'trips.txt']\n",
      "\n",
      "----------------------------------------\n",
      "Processing: addisababa/agency.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (1, 5)\n",
      "\n",
      "Cleaning data for addisababa/agency.txt...\n",
      "  - Converted 'agency_timezone' to datetime\n",
      "  ✓ Cleaned data shape: (1, 5) → (1, 7)\n",
      "\n",
      "--- Analysis for addisababa/agency.txt ---\n",
      "Shape: (1, 7)\n",
      "Memory usage: 0.00 MB\n",
      "\n",
      "Missing values:\n",
      "agency_timezone    1\n",
      "dtype: int64\n",
      "\n",
      "Data types:\n",
      "object            6\n",
      "datetime64[ns]    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "agency_url:\n",
      "agency_url\n",
      "https://addismaptransit.com/    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "agency_lang:\n",
      "agency_lang\n",
      "en    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "agency_name:\n",
      "agency_name\n",
      "Addis Ababa Transport (all)    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "                     agency_url agency_lang                  agency_name  \\\n",
      "0  https://addismaptransit.com/          en  Addis Ababa Transport (all)   \n",
      "\n",
      "  agency_id agency_timezone        city source_file  \n",
      "0        AA             NaT  addisababa  agency.txt  \n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\addisababa\\cleaned_agency.csv\n",
      "\n",
      "----------------------------------------\n",
      "Processing: addisababa/calendar.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (1, 10)\n",
      "\n",
      "Cleaning data for addisababa/calendar.txt...\n",
      "  - Converted 'start_date' to datetime\n",
      "  - Converted 'end_date' to datetime\n",
      "  ✓ Cleaned data shape: (1, 10) → (1, 12)\n",
      "\n",
      "--- Analysis for addisababa/calendar.txt ---\n",
      "Shape: (1, 12)\n",
      "Memory usage: 0.00 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "int64             8\n",
      "datetime64[ns]    2\n",
      "object            2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Numeric columns summary:\n",
      "       service_id  monday  tuesday  wednesday  thursday  friday  saturday  \\\n",
      "count         1.0     1.0      1.0        1.0       1.0     1.0       1.0   \n",
      "mean          0.0     1.0      1.0        1.0       1.0     1.0       1.0   \n",
      "std           NaN     NaN      NaN        NaN       NaN     NaN       NaN   \n",
      "min           0.0     1.0      1.0        1.0       1.0     1.0       1.0   \n",
      "25%           0.0     1.0      1.0        1.0       1.0     1.0       1.0   \n",
      "50%           0.0     1.0      1.0        1.0       1.0     1.0       1.0   \n",
      "75%           0.0     1.0      1.0        1.0       1.0     1.0       1.0   \n",
      "max           0.0     1.0      1.0        1.0       1.0     1.0       1.0   \n",
      "\n",
      "       sunday  \n",
      "count     1.0  \n",
      "mean      1.0  \n",
      "std       NaN  \n",
      "min       1.0  \n",
      "25%       1.0  \n",
      "50%       1.0  \n",
      "75%       1.0  \n",
      "max       1.0  \n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "city:\n",
      "city\n",
      "addisababa    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "source_file:\n",
      "source_file\n",
      "calendar.txt    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "   service_id                    start_date                      end_date  \\\n",
      "0           0 1970-01-01 00:00:00.020191201 1970-01-01 00:00:00.020991231   \n",
      "\n",
      "   monday  tuesday  wednesday  thursday  friday  saturday  sunday        city  \\\n",
      "0       1        1          1         1       1         1       1  addisababa   \n",
      "\n",
      "    source_file  \n",
      "0  calendar.txt  \n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\addisababa\\cleaned_calendar.csv\n",
      "\n",
      "----------------------------------------\n",
      "Processing: addisababa/feed_info.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (1, 8)\n",
      "\n",
      "Cleaning data for addisababa/feed_info.txt...\n",
      "  - Converted 'feed_end_date' to datetime\n",
      "  - Converted 'feed_start_date' to datetime\n",
      "  ✓ Cleaned data shape: (1, 8) → (1, 10)\n",
      "\n",
      "--- Analysis for addisababa/feed_info.txt ---\n",
      "Shape: (1, 10)\n",
      "Memory usage: 0.00 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "object            7\n",
      "datetime64[ns]    2\n",
      "float64           1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Numeric columns summary:\n",
      "       feed_version\n",
      "count           1.0\n",
      "mean            1.0\n",
      "std             NaN\n",
      "min             1.0\n",
      "25%             1.0\n",
      "50%             1.0\n",
      "75%             1.0\n",
      "max             1.0\n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "feed_publisher_name:\n",
      "feed_publisher_name\n",
      "AddisMap + DT4A    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "feed_publisher_url:\n",
      "feed_publisher_url\n",
      "https://digitaltransport4africa.org/    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "feed_lang:\n",
      "feed_lang\n",
      "en    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "   feed_version feed_publisher_name                    feed_publisher_url  \\\n",
      "0           1.0     AddisMap + DT4A  https://digitaltransport4africa.org/   \n",
      "\n",
      "  feed_lang feed_contact_email                 feed_end_date  \\\n",
      "0        en  info@addismap.com 1970-01-01 00:00:00.020991231   \n",
      "\n",
      "                feed_start_date                      feed_contact_url  \\\n",
      "0 1970-01-01 00:00:00.020191201  https://addismaptransit.com/support/   \n",
      "\n",
      "         city    source_file  \n",
      "0  addisababa  feed_info.txt  \n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\addisababa\\cleaned_feed_info.csv\n",
      "\n",
      "----------------------------------------\n",
      "Processing: addisababa/frequencies.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (881, 5)\n",
      "\n",
      "Cleaning data for addisababa/frequencies.txt...\n",
      "  - Converted 'start_time' to datetime\n",
      "  - Converted 'end_time' to datetime\n",
      "  - Converted 'exact_times' to datetime\n",
      "  ✓ Cleaned data shape: (881, 5) → (881, 7)\n",
      "\n",
      "--- Analysis for addisababa/frequencies.txt ---\n",
      "Shape: (881, 7)\n",
      "Memory usage: 0.14 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "datetime64[ns]    3\n",
      "int64             2\n",
      "object            2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Numeric columns summary:\n",
      "         trip_id  headway_secs\n",
      "count  881.00000    881.000000\n",
      "mean   440.00000   2833.416572\n",
      "std    254.46709   4053.239588\n",
      "min      0.00000    120.000000\n",
      "25%    220.00000   3000.000000\n",
      "50%    440.00000   3000.000000\n",
      "75%    660.00000   3000.000000\n",
      "max    880.00000  86400.000000\n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "city:\n",
      "city\n",
      "addisababa    881\n",
      "Name: count, dtype: int64\n",
      "\n",
      "source_file:\n",
      "source_file\n",
      "frequencies.txt    881\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "   trip_id          start_time            end_time  headway_secs exact_times  \\\n",
      "0        0 2025-08-11 05:00:00 2025-08-11 22:00:00          3000  1970-01-01   \n",
      "1        1 2025-08-11 05:00:00 2025-08-11 22:00:00          3000  1970-01-01   \n",
      "2        2 2025-08-11 05:00:00 2025-08-11 22:00:00          3000  1970-01-01   \n",
      "\n",
      "         city      source_file  \n",
      "0  addisababa  frequencies.txt  \n",
      "1  addisababa  frequencies.txt  \n",
      "2  addisababa  frequencies.txt  \n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\addisababa\\cleaned_frequencies.csv\n",
      "\n",
      "----------------------------------------\n",
      "Processing: addisababa/routes.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (442, 8)\n",
      "\n",
      "Cleaning data for addisababa/routes.txt...\n",
      "  - Capped 144 outliers in 'route_id'\n",
      "  ✓ Cleaned data shape: (442, 8) → (442, 9)\n",
      "\n",
      "--- Analysis for addisababa/routes.txt ---\n",
      "Shape: (442, 9)\n",
      "Memory usage: 0.20 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "object     7\n",
      "float64    1\n",
      "int64      1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Numeric columns summary:\n",
      "           route_id  route_type\n",
      "count  4.420000e+02  442.000000\n",
      "mean   1.600921e+07    2.986425\n",
      "std    2.780725e+05    0.201573\n",
      "min    1.553149e+07    0.000000\n",
      "25%    1.587665e+07    3.000000\n",
      "50%    1.596085e+07    3.000000\n",
      "75%    1.610676e+07    3.000000\n",
      "max    1.645192e+07    3.000000\n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "route_long_name:\n",
      "route_long_name\n",
      "Megenagna Terminal ↔ Aba Kiros Roundabout    3\n",
      "Gurara ↔ 4 Kilo                              2\n",
      "Megenagna ↔ Addisu Gebeya                    2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "route_short_name:\n",
      "route_short_name\n",
      "AB097    1\n",
      "AB101    1\n",
      "AB049    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "agency_id:\n",
      "agency_id\n",
      "AA    442\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "                               route_long_name route_short_name agency_id  \\\n",
      "0        Megenegna Terminal ↔ Legedadi Mission            AB097        AA   \n",
      "1    Megenagna Terminal ↔ Aba Kiros Roundabout            AB101        AA   \n",
      "2  Megenegna Terminal ↔ Ayat Chefe Condominium            AB049        AA   \n",
      "\n",
      "     route_id  route_type route_color route_text_color        city source_file  \n",
      "0  15531489.5           3      1779c2           ffffff  addisababa  routes.txt  \n",
      "1  15531489.5           3      1779c2           ffffff  addisababa  routes.txt  \n",
      "2  15531489.5           3      1779c2           ffffff  addisababa  routes.txt  \n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\addisababa\\cleaned_routes.csv\n",
      "\n",
      "----------------------------------------\n",
      "Processing: addisababa/shapes.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (278945, 5)\n",
      "\n",
      "Cleaning data for addisababa/shapes.txt...\n",
      "  - Capped 33775 outliers in 'shape_id'\n",
      "  - Capped 13790 outliers in 'shape_pt_lat'\n",
      "  - Capped 21869 outliers in 'shape_pt_lon'\n",
      "  - Capped 10186 outliers in 'shape_pt_sequence'\n",
      "  ✓ Cleaned data shape: (278945, 5) → (278945, 6)\n",
      "\n",
      "--- Analysis for addisababa/shapes.txt ---\n",
      "Shape: (278945, 6)\n",
      "Memory usage: 39.90 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "float64    4\n",
      "object     2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Numeric columns summary:\n",
      "           shape_id   shape_pt_lat   shape_pt_lon  shape_pt_sequence\n",
      "count  2.789450e+05  278945.000000  278945.000000      278945.000000\n",
      "mean   1.602016e+07       9.006533      38.754195         196.138239\n",
      "std    7.061667e+05       0.040940       0.043790         147.210936\n",
      "min    1.452149e+07       8.910051      38.653545           1.000000\n",
      "25%    1.586742e+07       8.984446      38.726165          80.000000\n",
      "50%    1.596310e+07       9.012617      38.750209         165.000000\n",
      "75%    1.676471e+07       9.034042      38.774578         277.000000\n",
      "max    1.708233e+07       9.108437      38.847198         572.500000\n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "city:\n",
      "city\n",
      "addisababa    278945\n",
      "Name: count, dtype: int64\n",
      "\n",
      "source_file:\n",
      "source_file\n",
      "shapes.txt    278945\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "     shape_id  shape_pt_lat  shape_pt_lon  shape_pt_sequence        city  \\\n",
      "0  14521488.5      9.081773     38.847198                1.0  addisababa   \n",
      "1  14521488.5      9.081302     38.847198                2.0  addisababa   \n",
      "2  14521488.5      9.080956     38.847198                3.0  addisababa   \n",
      "\n",
      "  source_file  \n",
      "0  shapes.txt  \n",
      "1  shapes.txt  \n",
      "2  shapes.txt  \n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\addisababa\\cleaned_shapes.csv\n",
      "\n",
      "----------------------------------------\n",
      "Processing: addisababa/stops.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (2259, 6)\n",
      "\n",
      "Cleaning data for addisababa/stops.txt...\n",
      "  - Capped 196 outliers in 'stop_lat'\n",
      "  - Capped 218 outliers in 'stop_lon'\n",
      "  ✓ Cleaned data shape: (2259, 6) → (2259, 7)\n",
      "\n",
      "--- Analysis for addisababa/stops.txt ---\n",
      "Shape: (2259, 7)\n",
      "Memory usage: 0.57 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "object     4\n",
      "float64    2\n",
      "int64      1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Numeric columns summary:\n",
      "       location_type     stop_lat     stop_lon\n",
      "count         2259.0  2259.000000  2259.000000\n",
      "mean             0.0     9.004545    38.761756\n",
      "std              0.0     0.047789     0.054354\n",
      "min              0.0     8.901948    38.647217\n",
      "25%              0.0     8.981719    38.732173\n",
      "50%              0.0     9.012176    38.756828\n",
      "75%              0.0     9.034900    38.788811\n",
      "max              0.0     9.114671    38.873767\n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "stop_name:\n",
      "stop_name\n",
      "Mexico          39\n",
      "Megenagna       36\n",
      "Piassa Arada    26\n",
      "Name: count, dtype: int64\n",
      "\n",
      "stop_id:\n",
      "stop_id\n",
      "way/604046026    1\n",
      "way/604046027    1\n",
      "way/604046028    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "city:\n",
      "city\n",
      "addisababa    2259\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "   location_type  stop_lat stop_name        stop_id   stop_lon        city  \\\n",
      "0              0  9.021026     C.M.C  way/604046026  38.850456  addisababa   \n",
      "1              0  9.020937     C.M.C  way/604046027  38.850449  addisababa   \n",
      "2              0  9.020631      Meri  way/604046028  38.860390  addisababa   \n",
      "\n",
      "  source_file  \n",
      "0   stops.txt  \n",
      "1   stops.txt  \n",
      "2   stops.txt  \n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\addisababa\\cleaned_stops.csv\n",
      "\n",
      "----------------------------------------\n",
      "Processing: addisababa/stop_times.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (8925, 12)\n",
      "\n",
      "Cleaning data for addisababa/stop_times.txt...\n",
      "  - Converted 'arrival_time' to datetime\n",
      "  - Converted 'departure_time' to datetime\n",
      "  - Converted 'timepoint' to datetime\n",
      "  - Capped 327 outliers in 'stop_sequence'\n",
      "  ✓ Cleaned data shape: (8925, 12) → (8925, 10)\n",
      "\n",
      "--- Analysis for addisababa/stop_times.txt ---\n",
      "Shape: (8925, 10)\n",
      "Memory usage: 2.06 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "datetime64[ns]    3\n",
      "object            3\n",
      "int64             2\n",
      "float64           2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Numeric columns summary:\n",
      "           trip_id  stop_sequence  continuous_pickup  continuous_drop_off\n",
      "count  8925.000000    8925.000000             8925.0               8925.0\n",
      "mean    475.656583       6.679552                0.0                  0.0\n",
      "std     269.109326       4.585772                0.0                  0.0\n",
      "min       0.000000       1.000000                0.0                  0.0\n",
      "25%     231.000000       3.000000                0.0                  0.0\n",
      "50%     520.000000       6.000000                0.0                  0.0\n",
      "75%     709.000000       9.000000                0.0                  0.0\n",
      "max     880.000000      18.000000                0.0                  0.0\n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "stop_id:\n",
      "stop_id\n",
      "node/10913863177    26\n",
      "node/10842129026    24\n",
      "node/7041071395     24\n",
      "Name: count, dtype: int64\n",
      "\n",
      "city:\n",
      "city\n",
      "addisababa    8925\n",
      "Name: count, dtype: int64\n",
      "\n",
      "source_file:\n",
      "source_file\n",
      "stop_times.txt    8925\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "   trip_id        arrival_time      departure_time          stop_id  \\\n",
      "0        0 2025-08-11 06:00:00 2025-08-11 06:00:00  node/7037183574   \n",
      "1        0 2025-08-11 06:15:55 2025-08-11 06:15:55  node/7105158908   \n",
      "2        0 2025-08-11 06:17:24 2025-08-11 06:17:24  node/7105158907   \n",
      "\n",
      "   stop_sequence                     timepoint  continuous_pickup  \\\n",
      "0              1 1970-01-01 00:00:00.000000001                0.0   \n",
      "1              2 1970-01-01 00:00:00.000000000                0.0   \n",
      "2              3 1970-01-01 00:00:00.000000000                0.0   \n",
      "\n",
      "   continuous_drop_off        city     source_file  \n",
      "0                  0.0  addisababa  stop_times.txt  \n",
      "1                  0.0  addisababa  stop_times.txt  \n",
      "2                  0.0  addisababa  stop_times.txt  \n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\addisababa\\cleaned_stop_times.csv\n",
      "\n",
      "----------------------------------------\n",
      "Processing: addisababa/trips.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (881, 6)\n",
      "\n",
      "Cleaning data for addisababa/trips.txt...\n",
      "  - Capped 287 outliers in 'route_id'\n",
      "  - Capped 317 outliers in 'shape_id'\n",
      "  ✓ Cleaned data shape: (881, 6) → (881, 8)\n",
      "\n",
      "--- Analysis for addisababa/trips.txt ---\n",
      "Shape: (881, 8)\n",
      "Memory usage: 0.18 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "int64     5\n",
      "object    3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Numeric columns summary:\n",
      "         trip_id  service_id      route_id      shape_id  direction_id\n",
      "count  881.00000       881.0  8.810000e+02  8.810000e+02    881.000000\n",
      "mean   440.00000         0.0  1.601135e+07  1.597542e+07      0.498297\n",
      "std    254.46709         0.0  2.847510e+05  2.600590e+05      0.500281\n",
      "min      0.00000         0.0  1.552261e+07  1.555962e+07      0.000000\n",
      "25%    220.00000         0.0  1.587672e+07  1.586715e+07      0.000000\n",
      "50%    440.00000         0.0  1.596088e+07  1.591807e+07      0.000000\n",
      "75%    660.00000         0.0  1.611280e+07  1.607217e+07      1.000000\n",
      "max    880.00000         0.0  1.646692e+07  1.637970e+07      1.000000\n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "trip_headsign:\n",
      "trip_headsign\n",
      "Megenagna       58\n",
      "Piassa Arada    52\n",
      "Mexico          46\n",
      "Name: count, dtype: int64\n",
      "\n",
      "city:\n",
      "city\n",
      "addisababa    881\n",
      "Name: count, dtype: int64\n",
      "\n",
      "source_file:\n",
      "source_file\n",
      "trips.txt    881\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "   trip_id  service_id  route_id  shape_id  direction_id       trip_headsign  \\\n",
      "0        0           0  15522608  15559623             0  Megenagna Terminal   \n",
      "1        1           0  15522608  15559623             1    Legedadi Mission   \n",
      "2        2           0  15522608  15559623             0  Megenagna Terminal   \n",
      "\n",
      "         city source_file  \n",
      "0  addisababa   trips.txt  \n",
      "1  addisababa   trips.txt  \n",
      "2  addisababa   trips.txt  \n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\addisababa\\cleaned_trips.csv\n",
      "\n",
      "----------------------------------------\n",
      "COMBINING DATA FOR ADDISABABA\n",
      "----------------------------------------\n",
      "⚠ Too few common columns to combine addisababa data\n",
      "\n",
      "============================================================\n",
      "PROCESSING CITY: ADDISABABA-MINIBUS\n",
      "============================================================\n",
      "Found 9 data files in addisababa-minibus: ['agency.txt', 'calendar.txt', 'feed_info.txt', 'frequencies.txt', 'routes.txt', 'shapes.txt', 'stops.txt', 'stop_times.txt', 'trips.txt']\n",
      "\n",
      "----------------------------------------\n",
      "Processing: addisababa-minibus/agency.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (1, 5)\n",
      "\n",
      "Cleaning data for addisababa-minibus/agency.txt...\n",
      "  - Converted 'agency_timezone' to datetime\n",
      "  ✓ Cleaned data shape: (1, 5) → (1, 7)\n",
      "\n",
      "--- Analysis for addisababa-minibus/agency.txt ---\n",
      "Shape: (1, 7)\n",
      "Memory usage: 0.00 MB\n",
      "\n",
      "Missing values:\n",
      "agency_timezone    1\n",
      "dtype: int64\n",
      "\n",
      "Data types:\n",
      "object            6\n",
      "datetime64[ns]    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "agency_url:\n",
      "agency_url\n",
      "https://addismaptransit.com/    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "agency_id:\n",
      "agency_id\n",
      "AA    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "agency_lang:\n",
      "agency_lang\n",
      "en    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "                     agency_url agency_id agency_timezone agency_lang  \\\n",
      "0  https://addismaptransit.com/        AA             NaT          en   \n",
      "\n",
      "                       agency_name                city source_file  \n",
      "0  Addis Ababa Transport (Minibus)  addisababa-minibus  agency.txt  \n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\addisababa-minibus\\cleaned_agency.csv\n",
      "\n",
      "----------------------------------------\n",
      "Processing: addisababa-minibus/calendar.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (1, 10)\n",
      "\n",
      "Cleaning data for addisababa-minibus/calendar.txt...\n",
      "  - Converted 'start_date' to datetime\n",
      "  - Converted 'end_date' to datetime\n",
      "  ✓ Cleaned data shape: (1, 10) → (1, 12)\n",
      "\n",
      "--- Analysis for addisababa-minibus/calendar.txt ---\n",
      "Shape: (1, 12)\n",
      "Memory usage: 0.00 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "int64             8\n",
      "datetime64[ns]    2\n",
      "object            2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Numeric columns summary:\n",
      "       service_id  monday  tuesday  wednesday  thursday  friday  saturday  \\\n",
      "count         1.0     1.0      1.0        1.0       1.0     1.0       1.0   \n",
      "mean          0.0     1.0      1.0        1.0       1.0     1.0       1.0   \n",
      "std           NaN     NaN      NaN        NaN       NaN     NaN       NaN   \n",
      "min           0.0     1.0      1.0        1.0       1.0     1.0       1.0   \n",
      "25%           0.0     1.0      1.0        1.0       1.0     1.0       1.0   \n",
      "50%           0.0     1.0      1.0        1.0       1.0     1.0       1.0   \n",
      "75%           0.0     1.0      1.0        1.0       1.0     1.0       1.0   \n",
      "max           0.0     1.0      1.0        1.0       1.0     1.0       1.0   \n",
      "\n",
      "       sunday  \n",
      "count     1.0  \n",
      "mean      1.0  \n",
      "std       NaN  \n",
      "min       1.0  \n",
      "25%       1.0  \n",
      "50%       1.0  \n",
      "75%       1.0  \n",
      "max       1.0  \n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "city:\n",
      "city\n",
      "addisababa-minibus    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "source_file:\n",
      "source_file\n",
      "calendar.txt    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "   service_id                    start_date                      end_date  \\\n",
      "0           0 1970-01-01 00:00:00.020191201 1970-01-01 00:00:00.020991231   \n",
      "\n",
      "   monday  tuesday  wednesday  thursday  friday  saturday  sunday  \\\n",
      "0       1        1          1         1       1         1       1   \n",
      "\n",
      "                 city   source_file  \n",
      "0  addisababa-minibus  calendar.txt  \n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\addisababa-minibus\\cleaned_calendar.csv\n",
      "\n",
      "----------------------------------------\n",
      "Processing: addisababa-minibus/feed_info.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (1, 8)\n",
      "\n",
      "Cleaning data for addisababa-minibus/feed_info.txt...\n",
      "  - Converted 'feed_end_date' to datetime\n",
      "  - Converted 'feed_start_date' to datetime\n",
      "  ✓ Cleaned data shape: (1, 8) → (1, 10)\n",
      "\n",
      "--- Analysis for addisababa-minibus/feed_info.txt ---\n",
      "Shape: (1, 10)\n",
      "Memory usage: 0.00 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "object            7\n",
      "datetime64[ns]    2\n",
      "float64           1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Numeric columns summary:\n",
      "       feed_version\n",
      "count           1.0\n",
      "mean            1.0\n",
      "std             NaN\n",
      "min             1.0\n",
      "25%             1.0\n",
      "50%             1.0\n",
      "75%             1.0\n",
      "max             1.0\n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "feed_contact_url:\n",
      "feed_contact_url\n",
      "https://addismaptransit.com/support/    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "feed_publisher_url:\n",
      "feed_publisher_url\n",
      "https://digitaltransport4africa.org/    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "feed_publisher_name:\n",
      "feed_publisher_name\n",
      "AddisMap + DT4A    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "                       feed_contact_url                    feed_publisher_url  \\\n",
      "0  https://addismaptransit.com/support/  https://digitaltransport4africa.org/   \n",
      "\n",
      "   feed_version feed_publisher_name                 feed_end_date feed_lang  \\\n",
      "0           1.0     AddisMap + DT4A 1970-01-01 00:00:00.020991231        en   \n",
      "\n",
      "                feed_start_date feed_contact_email                city  \\\n",
      "0 1970-01-01 00:00:00.020191201  info@addismap.com  addisababa-minibus   \n",
      "\n",
      "     source_file  \n",
      "0  feed_info.txt  \n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\addisababa-minibus\\cleaned_feed_info.csv\n",
      "\n",
      "----------------------------------------\n",
      "Processing: addisababa-minibus/frequencies.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (492, 5)\n",
      "\n",
      "Cleaning data for addisababa-minibus/frequencies.txt...\n",
      "  - Converted 'start_time' to datetime\n",
      "  - Converted 'end_time' to datetime\n",
      "  - Converted 'exact_times' to datetime\n",
      "  ✓ Cleaned data shape: (492, 5) → (492, 7)\n",
      "\n",
      "--- Analysis for addisababa-minibus/frequencies.txt ---\n",
      "Shape: (492, 7)\n",
      "Memory usage: 0.08 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "datetime64[ns]    3\n",
      "int64             2\n",
      "object            2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Numeric columns summary:\n",
      "          trip_id  headway_secs\n",
      "count  492.000000    492.000000\n",
      "mean   245.500000   2996.341463\n",
      "std    142.172431     81.150267\n",
      "min      0.000000   1200.000000\n",
      "25%    122.750000   3000.000000\n",
      "50%    245.500000   3000.000000\n",
      "75%    368.250000   3000.000000\n",
      "max    491.000000   3000.000000\n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "city:\n",
      "city\n",
      "addisababa-minibus    492\n",
      "Name: count, dtype: int64\n",
      "\n",
      "source_file:\n",
      "source_file\n",
      "frequencies.txt    492\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "   trip_id          start_time            end_time  headway_secs exact_times  \\\n",
      "0        0 2025-08-11 05:00:00 2025-08-11 22:00:00          3000  1970-01-01   \n",
      "1        1 2025-08-11 05:00:00 2025-08-11 22:00:00          3000  1970-01-01   \n",
      "2        2 2025-08-11 05:00:00 2025-08-11 22:00:00          3000  1970-01-01   \n",
      "\n",
      "                 city      source_file  \n",
      "0  addisababa-minibus  frequencies.txt  \n",
      "1  addisababa-minibus  frequencies.txt  \n",
      "2  addisababa-minibus  frequencies.txt  \n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\addisababa-minibus\\cleaned_frequencies.csv\n",
      "\n",
      "----------------------------------------\n",
      "Processing: addisababa-minibus/routes.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (247, 8)\n",
      "\n",
      "Cleaning data for addisababa-minibus/routes.txt...\n",
      "  - Capped 8 outliers in 'route_id'\n",
      "  ✓ Cleaned data shape: (247, 8) → (247, 9)\n",
      "\n",
      "--- Analysis for addisababa-minibus/routes.txt ---\n",
      "Shape: (247, 9)\n",
      "Memory usage: 0.12 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "object     7\n",
      "int64      1\n",
      "float64    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Numeric columns summary:\n",
      "       route_type      route_id\n",
      "count       247.0  2.470000e+02\n",
      "mean          3.0  1.591469e+07\n",
      "std           0.0  6.197220e+04\n",
      "min           3.0  1.574856e+07\n",
      "25%           3.0  1.587668e+07\n",
      "50%           3.0  1.591034e+07\n",
      "75%           3.0  1.596208e+07\n",
      "max           3.0  1.602889e+07\n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "agency_id:\n",
      "agency_id\n",
      "AA    247\n",
      "Name: count, dtype: int64\n",
      "\n",
      "route_long_name:\n",
      "route_long_name\n",
      "Wello Sefer ↔ Kera (Minibus)             2\n",
      "Kality Menaheria ↔ 4 Kilo (Minibus)      2\n",
      "Kality Total ↔ Autobus Tera (Minibus)    2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "route_short_name:\n",
      "route_short_name\n",
      "TX341    1\n",
      "TX002    1\n",
      "TX200    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "   route_type agency_id                         route_long_name  \\\n",
      "0           3        AA        Olympia ↔ Agona Cinema (Minibus)   \n",
      "1           3        AA  Total 3 kuter Mazoria ↔ Kera (Minibus)   \n",
      "2           3        AA            Wello Sefer ↔ Kera (Minibus)   \n",
      "\n",
      "  route_short_name    route_id route_color route_text_color  \\\n",
      "0            TX341  15748561.5      1779c2           ffffff   \n",
      "1            TX002  15748561.5      1779c2           ffffff   \n",
      "2            TX200  15748561.5      1779c2           ffffff   \n",
      "\n",
      "                 city source_file  \n",
      "0  addisababa-minibus  routes.txt  \n",
      "1  addisababa-minibus  routes.txt  \n",
      "2  addisababa-minibus  routes.txt  \n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\addisababa-minibus\\cleaned_routes.csv\n",
      "\n",
      "----------------------------------------\n",
      "Processing: addisababa-minibus/shapes.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (121486, 5)\n",
      "\n",
      "Cleaning data for addisababa-minibus/shapes.txt...\n",
      "  - Capped 3720 outliers in 'shape_id'\n",
      "  - Capped 4587 outliers in 'shape_pt_lat'\n",
      "  - Capped 6976 outliers in 'shape_pt_lon'\n",
      "  - Capped 3184 outliers in 'shape_pt_sequence'\n",
      "  ✓ Cleaned data shape: (121486, 5) → (121486, 6)\n",
      "\n",
      "--- Analysis for addisababa-minibus/shapes.txt ---\n",
      "Shape: (121486, 6)\n",
      "Memory usage: 18.31 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "float64    4\n",
      "object     2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Numeric columns summary:\n",
      "           shape_id   shape_pt_lat   shape_pt_lon  shape_pt_sequence\n",
      "count  1.214860e+05  121486.000000  121486.000000      121486.000000\n",
      "mean   1.591629e+07       9.000803      38.751003         146.734546\n",
      "std    6.138246e+04       0.038153       0.035701         105.897885\n",
      "min    1.574872e+07       8.909213      38.662900           1.000000\n",
      "25%    1.587675e+07       8.980714      38.725983          62.000000\n",
      "50%    1.591308e+07       9.010690      38.747729         127.000000\n",
      "75%    1.596210e+07       9.028382      38.768038         209.000000\n",
      "max    1.600831e+07       9.069924      38.831120         429.500000\n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "city:\n",
      "city\n",
      "addisababa-minibus    121486\n",
      "Name: count, dtype: int64\n",
      "\n",
      "source_file:\n",
      "source_file\n",
      "shapes.txt    121486\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "     shape_id  shape_pt_lat  shape_pt_lon  shape_pt_sequence  \\\n",
      "0  15748723.5      8.987489     38.760896                1.0   \n",
      "1  15748723.5      8.987486     38.761126                2.0   \n",
      "2  15748723.5      8.987481     38.761715                3.0   \n",
      "\n",
      "                 city source_file  \n",
      "0  addisababa-minibus  shapes.txt  \n",
      "1  addisababa-minibus  shapes.txt  \n",
      "2  addisababa-minibus  shapes.txt  \n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\addisababa-minibus\\cleaned_shapes.csv\n",
      "\n",
      "----------------------------------------\n",
      "Processing: addisababa-minibus/stops.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (905, 6)\n",
      "\n",
      "Cleaning data for addisababa-minibus/stops.txt...\n",
      "  - Capped 36 outliers in 'stop_lon'\n",
      "  - Capped 50 outliers in 'stop_lat'\n",
      "  ✓ Cleaned data shape: (905, 6) → (905, 7)\n",
      "\n",
      "--- Analysis for addisababa-minibus/stops.txt ---\n",
      "Shape: (905, 7)\n",
      "Memory usage: 0.24 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "object     4\n",
      "float64    2\n",
      "int64      1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Numeric columns summary:\n",
      "         stop_lon    stop_lat  location_type\n",
      "count  905.000000  905.000000          905.0\n",
      "mean    38.760473    9.000823            0.0\n",
      "std      0.043347    0.038663            0.0\n",
      "min     38.663952    8.911398            0.0\n",
      "25%     38.732906    8.982268            0.0\n",
      "50%     38.753349    9.010012            0.0\n",
      "75%     38.784342    9.029514            0.0\n",
      "max     38.861497    9.069169            0.0\n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "stop_name:\n",
      "stop_name\n",
      "Megenagna    19\n",
      "Mexico       18\n",
      "Kera         13\n",
      "Name: count, dtype: int64\n",
      "\n",
      "stop_id:\n",
      "stop_id\n",
      "node/6728583386    1\n",
      "node/6967281242    1\n",
      "node/7041052978    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "city:\n",
      "city\n",
      "addisababa-minibus    905\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "    stop_lon  stop_lat         stop_name          stop_id  location_type  \\\n",
      "0  38.751676  9.033148             Arada  node/6728583386              0   \n",
      "1  38.752624  9.033012  Piassa Arada (2)  node/6967281242              0   \n",
      "2  38.762999  8.965927  kadisco Building  node/7041052978              0   \n",
      "\n",
      "                 city source_file  \n",
      "0  addisababa-minibus   stops.txt  \n",
      "1  addisababa-minibus   stops.txt  \n",
      "2  addisababa-minibus   stops.txt  \n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\addisababa-minibus\\cleaned_stops.csv\n",
      "\n",
      "----------------------------------------\n",
      "Processing: addisababa-minibus/stop_times.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (3894, 12)\n",
      "\n",
      "Cleaning data for addisababa-minibus/stop_times.txt...\n",
      "  - Converted 'arrival_time' to datetime\n",
      "  - Converted 'departure_time' to datetime\n",
      "  - Converted 'timepoint' to datetime\n",
      "  - Capped 109 outliers in 'stop_sequence'\n",
      "  ✓ Cleaned data shape: (3894, 12) → (3894, 10)\n",
      "\n",
      "--- Analysis for addisababa-minibus/stop_times.txt ---\n",
      "Shape: (3894, 10)\n",
      "Memory usage: 0.93 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "int64             3\n",
      "datetime64[ns]    3\n",
      "object            3\n",
      "float64           1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Numeric columns summary:\n",
      "           trip_id  stop_sequence  continuous_pickup  continuous_drop_off\n",
      "count  3894.000000    3894.000000             3894.0               3894.0\n",
      "mean    246.466102       5.157550                0.0                  0.0\n",
      "std     145.069130       3.388327                0.0                  0.0\n",
      "min       0.000000       1.000000                0.0                  0.0\n",
      "25%     118.000000       2.000000                0.0                  0.0\n",
      "50%     252.000000       4.000000                0.0                  0.0\n",
      "75%     368.000000       7.000000                0.0                  0.0\n",
      "max     491.000000      14.500000                0.0                  0.0\n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "stop_id:\n",
      "stop_id\n",
      "node/10913863177    26\n",
      "node/10842129026    24\n",
      "node/10842129098    23\n",
      "Name: count, dtype: int64\n",
      "\n",
      "city:\n",
      "city\n",
      "addisababa-minibus    3894\n",
      "Name: count, dtype: int64\n",
      "\n",
      "source_file:\n",
      "source_file\n",
      "stop_times.txt    3894\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "   trip_id        arrival_time      departure_time          stop_id  \\\n",
      "0        0 2025-08-11 06:00:00 2025-08-11 06:00:00  node/7123265066   \n",
      "1        0 2025-08-11 06:02:30 2025-08-11 06:02:30  node/7123265065   \n",
      "2        0 2025-08-11 06:05:03 2025-08-11 06:05:03  node/7123265064   \n",
      "\n",
      "   stop_sequence                     timepoint  continuous_pickup  \\\n",
      "0            1.0 1970-01-01 00:00:00.000000001                  0   \n",
      "1            2.0 1970-01-01 00:00:00.000000000                  0   \n",
      "2            3.0 1970-01-01 00:00:00.000000000                  0   \n",
      "\n",
      "   continuous_drop_off                city     source_file  \n",
      "0                    0  addisababa-minibus  stop_times.txt  \n",
      "1                    0  addisababa-minibus  stop_times.txt  \n",
      "2                    0  addisababa-minibus  stop_times.txt  \n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\addisababa-minibus\\cleaned_stop_times.csv\n",
      "\n",
      "----------------------------------------\n",
      "Processing: addisababa-minibus/trips.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (492, 6)\n",
      "\n",
      "Cleaning data for addisababa-minibus/trips.txt...\n",
      "  - Capped 16 outliers in 'route_id'\n",
      "  - Capped 16 outliers in 'shape_id'\n",
      "  ✓ Cleaned data shape: (492, 6) → (492, 8)\n",
      "\n",
      "--- Analysis for addisababa-minibus/trips.txt ---\n",
      "Shape: (492, 8)\n",
      "Memory usage: 0.11 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "int64     5\n",
      "object    3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Numeric columns summary:\n",
      "          trip_id      route_id  service_id      shape_id  direction_id\n",
      "count  492.000000  4.920000e+02       492.0  4.920000e+02    492.000000\n",
      "mean   245.500000  1.591495e+07         0.0  1.591318e+07      0.497967\n",
      "std    142.172431  6.189762e+04         0.0  6.235302e+04      0.500505\n",
      "min      0.000000  1.574866e+07         0.0  1.573603e+07      0.000000\n",
      "25%    122.750000  1.587672e+07         0.0  1.587162e+07      0.000000\n",
      "50%    245.500000  1.591038e+07         0.0  1.591025e+07      0.000000\n",
      "75%    368.250000  1.596210e+07         0.0  1.596202e+07      1.000000\n",
      "max    491.000000  1.602889e+07         0.0  1.600831e+07      1.000000\n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "trip_headsign:\n",
      "trip_headsign\n",
      "Megenagna       30\n",
      "Piassa Arada    23\n",
      "Mexico          20\n",
      "Name: count, dtype: int64\n",
      "\n",
      "city:\n",
      "city\n",
      "addisababa-minibus    492\n",
      "Name: count, dtype: int64\n",
      "\n",
      "source_file:\n",
      "source_file\n",
      "trips.txt    492\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "   trip_id  route_id  service_id  shape_id  direction_id  \\\n",
      "0        0  15748658           0  15736031             0   \n",
      "1        1  15748658           0  15736031             1   \n",
      "2        2  15748658           0  15736031             0   \n",
      "\n",
      "           trip_headsign                city source_file  \n",
      "0                Olympia  addisababa-minibus   trips.txt  \n",
      "1           Agona Cinema  addisababa-minibus   trips.txt  \n",
      "2  Total 3 Kuter Mazoria  addisababa-minibus   trips.txt  \n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\addisababa-minibus\\cleaned_trips.csv\n",
      "\n",
      "----------------------------------------\n",
      "COMBINING DATA FOR ADDISABABA-MINIBUS\n",
      "----------------------------------------\n",
      "⚠ Too few common columns to combine addisababa-minibus data\n",
      "\n",
      "============================================================\n",
      "PROCESSING CITY: ALEXENDRIA\n",
      "============================================================\n",
      "Found 9 data files in alexendria: ['agency.txt', 'calendar.txt', 'feed_info.txt', 'frequencies.txt', 'routes.txt', 'shapes.txt', 'stops.txt', 'stop_times.txt', 'trips.txt']\n",
      "\n",
      "----------------------------------------\n",
      "Processing: alexendria/agency.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (5, 4)\n",
      "\n",
      "Cleaning data for alexendria/agency.txt...\n",
      "  - Converted 'agency_timezone' to datetime\n",
      "  ✓ Cleaned data shape: (5, 4) → (5, 6)\n",
      "\n",
      "--- Analysis for alexendria/agency.txt ---\n",
      "Shape: (5, 6)\n",
      "Memory usage: 0.00 MB\n",
      "\n",
      "Missing values:\n",
      "agency_timezone    5\n",
      "dtype: int64\n",
      "\n",
      "Data types:\n",
      "object            5\n",
      "datetime64[ns]    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "agency_id:\n",
      "agency_id\n",
      "P_B_8     1\n",
      "P_O_14    1\n",
      "COOP      1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "agency_name:\n",
      "agency_name\n",
      "Paratransit 8 Seater Suzuki Chevrolet or DMF (Blue Licenseplates)    1\n",
      "Paratransit 14 Seater Microbus (Orange Licenseplates)                1\n",
      "Cooperative paratransit 29 Seater (Grey Licenseplates)               1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "agency_url:\n",
      "agency_url\n",
      "https://digitaltransport4africa.org/    5\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "  agency_id                                        agency_name  \\\n",
      "0     P_B_8  Paratransit 8 Seater Suzuki Chevrolet or DMF (...   \n",
      "1    P_O_14  Paratransit 14 Seater Microbus (Orange License...   \n",
      "2      COOP  Cooperative paratransit 29 Seater (Grey Licens...   \n",
      "\n",
      "                             agency_url agency_timezone        city  \\\n",
      "0  https://digitaltransport4africa.org/             NaT  alexendria   \n",
      "1  https://digitaltransport4africa.org/             NaT  alexendria   \n",
      "2  https://digitaltransport4africa.org/             NaT  alexendria   \n",
      "\n",
      "  source_file  \n",
      "0  agency.txt  \n",
      "1  agency.txt  \n",
      "2  agency.txt  \n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\alexendria\\cleaned_agency.csv\n",
      "\n",
      "----------------------------------------\n",
      "Processing: alexendria/calendar.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (1, 10)\n",
      "\n",
      "Cleaning data for alexendria/calendar.txt...\n",
      "  - Converted 'start_date' to datetime\n",
      "  - Converted 'end_date' to datetime\n",
      "  ✓ Cleaned data shape: (1, 10) → (1, 12)\n",
      "\n",
      "--- Analysis for alexendria/calendar.txt ---\n",
      "Shape: (1, 12)\n",
      "Memory usage: 0.00 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "int64             7\n",
      "object            3\n",
      "datetime64[ns]    2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Numeric columns summary:\n",
      "       monday  tuesday  wednesday  thursday  friday  saturday  sunday\n",
      "count     1.0      1.0        1.0       1.0     1.0       1.0     1.0\n",
      "mean      1.0      1.0        1.0       1.0     1.0       1.0     1.0\n",
      "std       NaN      NaN        NaN       NaN     NaN       NaN     NaN\n",
      "min       1.0      1.0        1.0       1.0     1.0       1.0     1.0\n",
      "25%       1.0      1.0        1.0       1.0     1.0       1.0     1.0\n",
      "50%       1.0      1.0        1.0       1.0     1.0       1.0     1.0\n",
      "75%       1.0      1.0        1.0       1.0     1.0       1.0     1.0\n",
      "max       1.0      1.0        1.0       1.0     1.0       1.0     1.0\n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "service_id:\n",
      "service_id\n",
      "Ground_Daily    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "city:\n",
      "city\n",
      "alexendria    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "source_file:\n",
      "source_file\n",
      "calendar.txt    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "   monday  tuesday  wednesday  thursday  friday  saturday  sunday  \\\n",
      "0       1        1          1         1       1         1       1   \n",
      "\n",
      "                     start_date                      end_date    service_id  \\\n",
      "0 1970-01-01 00:00:00.020230101 1970-01-01 00:00:00.020231230  Ground_Daily   \n",
      "\n",
      "         city   source_file  \n",
      "0  alexendria  calendar.txt  \n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\alexendria\\cleaned_calendar.csv\n",
      "\n",
      "----------------------------------------\n",
      "Processing: alexendria/feed_info.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (1, 7)\n",
      "\n",
      "Cleaning data for alexendria/feed_info.txt...\n",
      "  - Converted 'feed_start_date' to datetime\n",
      "  - Converted 'feed_end_date' to datetime\n",
      "  ✓ Cleaned data shape: (1, 7) → (1, 9)\n",
      "\n",
      "--- Analysis for alexendria/feed_info.txt ---\n",
      "Shape: (1, 9)\n",
      "Memory usage: 0.00 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "object            7\n",
      "datetime64[ns]    2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "feed_publisher_name:\n",
      "feed_publisher_name\n",
      "DigitalTransport4Africa    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "feed_publisher_url:\n",
      "feed_publisher_url\n",
      "https://digitaltransport4africa.org/    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "feed_contact_url:\n",
      "feed_contact_url\n",
      "https://digitaltransport4africa.org/    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "       feed_publisher_name                    feed_publisher_url  \\\n",
      "0  DigitalTransport4Africa  https://digitaltransport4africa.org/   \n",
      "\n",
      "                       feed_contact_url               feed_start_date  \\\n",
      "0  https://digitaltransport4africa.org/ 1970-01-01 00:00:00.020230101   \n",
      "\n",
      "                  feed_end_date feed_version feed_lang        city  \\\n",
      "0 1970-01-01 00:00:00.020240101    alex_dt4a        en  alexendria   \n",
      "\n",
      "     source_file  \n",
      "0  feed_info.txt  \n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\alexendria\\cleaned_feed_info.csv\n",
      "\n",
      "----------------------------------------\n",
      "Processing: alexendria/frequencies.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (192, 4)\n",
      "\n",
      "Cleaning data for alexendria/frequencies.txt...\n",
      "  - Converted 'start_time' to datetime\n",
      "  - Converted 'end_time' to datetime\n",
      "  - Capped 28 outliers in 'headway_secs'\n",
      "  ✓ Cleaned data shape: (192, 4) → (192, 6)\n",
      "\n",
      "--- Analysis for alexendria/frequencies.txt ---\n",
      "Shape: (192, 6)\n",
      "Memory usage: 0.04 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "object            3\n",
      "datetime64[ns]    2\n",
      "int64             1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Numeric columns summary:\n",
      "       headway_secs\n",
      "count    192.000000\n",
      "mean     342.875000\n",
      "std      235.063019\n",
      "min       60.000000\n",
      "25%      180.000000\n",
      "50%      300.000000\n",
      "75%      420.000000\n",
      "max      780.000000\n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "trip_id:\n",
      "trip_id\n",
      "OvxARhItfijPV-_xuvfnZ-07:00:00    1\n",
      "7OIDYqQb_VCEwYHFGH_e5-07:00:00    1\n",
      "zivMUtHDofqSlzEUoK8F0-07:00:00    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "city:\n",
      "city\n",
      "alexendria    192\n",
      "Name: count, dtype: int64\n",
      "\n",
      "source_file:\n",
      "source_file\n",
      "frequencies.txt    192\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "                          trip_id          start_time            end_time  \\\n",
      "0  OvxARhItfijPV-_xuvfnZ-07:00:00 2025-08-11 07:00:00 2025-08-11 22:00:00   \n",
      "1  7OIDYqQb_VCEwYHFGH_e5-07:00:00 2025-08-11 07:00:00 2025-08-11 22:00:00   \n",
      "2  zivMUtHDofqSlzEUoK8F0-07:00:00 2025-08-11 07:00:00 2025-08-11 22:00:00   \n",
      "\n",
      "   headway_secs        city      source_file  \n",
      "0            60  alexendria  frequencies.txt  \n",
      "1            60  alexendria  frequencies.txt  \n",
      "2           360  alexendria  frequencies.txt  \n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\alexendria\\cleaned_frequencies.csv\n",
      "\n",
      "----------------------------------------\n",
      "Processing: alexendria/routes.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (104, 7)\n",
      "\n",
      "Cleaning data for alexendria/routes.txt...\n",
      "  ✓ Cleaned data shape: (104, 7) → (104, 9)\n",
      "\n",
      "--- Analysis for alexendria/routes.txt ---\n",
      "Shape: (104, 9)\n",
      "Memory usage: 0.04 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "object    6\n",
      "int64     3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Numeric columns summary:\n",
      "       route_type  continuous_pickup  continuous_drop_off\n",
      "count       104.0              104.0                104.0\n",
      "mean          3.0                1.0                  1.0\n",
      "std           0.0                0.0                  0.0\n",
      "min           3.0                1.0                  1.0\n",
      "25%           3.0                1.0                  1.0\n",
      "50%           3.0                1.0                  1.0\n",
      "75%           3.0                1.0                  1.0\n",
      "max           3.0                1.0                  1.0\n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "route_id:\n",
      "route_id\n",
      "I46ZQc9g0OMvTpnnq0RXs    1\n",
      "tv4mLSYvBC5Q4aSnL5h83    1\n",
      "d591FuxnMulU7vr6uNxrB    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "agency_id:\n",
      "agency_id\n",
      "P_O_14    74\n",
      "P_B_8     13\n",
      "Bus       12\n",
      "Name: count, dtype: int64\n",
      "\n",
      "route_long_name:\n",
      "route_long_name\n",
      "Asafra - Train Station (El-Shohada Square)    3\n",
      "El-Awayed - El-Sa'ah (Clock Square)           2\n",
      "El-Awayed - Sidi Gabir                        2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "                route_id agency_id              route_long_name  \\\n",
      "0  I46ZQc9g0OMvTpnnq0RXs    P_O_14          Asafra - Sidi Bishr   \n",
      "1  tv4mLSYvBC5Q4aSnL5h83    P_O_14  Asafra - El-Mawqaf El-Geded   \n",
      "2  d591FuxnMulU7vr6uNxrB    P_O_14               Asafra - Hadra   \n",
      "\n",
      "  route_short_name  route_type  continuous_pickup  continuous_drop_off  \\\n",
      "0         Microbus           3                  1                    1   \n",
      "1         Microbus           3                  1                    1   \n",
      "2         Microbus           3                  1                    1   \n",
      "\n",
      "         city source_file  \n",
      "0  alexendria  routes.txt  \n",
      "1  alexendria  routes.txt  \n",
      "2  alexendria  routes.txt  \n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\alexendria\\cleaned_routes.csv\n",
      "\n",
      "----------------------------------------\n",
      "Processing: alexendria/shapes.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (29358, 4)\n",
      "\n",
      "Cleaning data for alexendria/shapes.txt...\n",
      "  - Capped 282 outliers in 'shape_pt_sequence'\n",
      "  - Capped 2024 outliers in 'shape_pt_lat'\n",
      "  - Capped 1221 outliers in 'shape_pt_lon'\n",
      "  ✓ Cleaned data shape: (29358, 4) → (29358, 6)\n",
      "\n",
      "--- Analysis for alexendria/shapes.txt ---\n",
      "Shape: (29358, 6)\n",
      "Memory usage: 6.10 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "object     3\n",
      "float64    2\n",
      "int64      1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Numeric columns summary:\n",
      "       shape_pt_sequence  shape_pt_lat  shape_pt_lon\n",
      "count       29358.000000  29358.000000  29358.000000\n",
      "mean          111.860685     31.200490     29.930145\n",
      "std            85.575006      0.052025      0.065542\n",
      "min             1.000000     31.087677     29.779712\n",
      "25%            41.000000     31.179520     29.900735\n",
      "50%            94.000000     31.204644     29.935609\n",
      "75%           165.000000     31.240749     29.981417\n",
      "max           351.000000     31.321844     30.072326\n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "shape_id:\n",
      "shape_id\n",
      "RmCIDxJQKwF7GOCKKX3Oh_Shape    428\n",
      "MrihzTWrnXBOK-ItJhoR8_Shape    419\n",
      "LYcY_J8e-Dijvjm6uZhgZ_Shape    419\n",
      "Name: count, dtype: int64\n",
      "\n",
      "city:\n",
      "city\n",
      "alexendria    29358\n",
      "Name: count, dtype: int64\n",
      "\n",
      "source_file:\n",
      "source_file\n",
      "shapes.txt    29358\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "                      shape_id  shape_pt_sequence  shape_pt_lat  shape_pt_lon  \\\n",
      "0  OvxARhItfijPV-_xuvfnZ_Shape                  1     31.256244     29.993791   \n",
      "1  OvxARhItfijPV-_xuvfnZ_Shape                  2     31.255878     29.994113   \n",
      "2  OvxARhItfijPV-_xuvfnZ_Shape                  3     31.256315     29.994751   \n",
      "\n",
      "         city source_file  \n",
      "0  alexendria  shapes.txt  \n",
      "1  alexendria  shapes.txt  \n",
      "2  alexendria  shapes.txt  \n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\alexendria\\cleaned_shapes.csv\n",
      "\n",
      "----------------------------------------\n",
      "Processing: alexendria/stops.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (441, 4)\n",
      "\n",
      "Cleaning data for alexendria/stops.txt...\n",
      "  - Capped 25 outliers in 'stop_lat'\n",
      "  - Capped 6 outliers in 'stop_lon'\n",
      "  ✓ Cleaned data shape: (441, 4) → (441, 6)\n",
      "\n",
      "--- Analysis for alexendria/stops.txt ---\n",
      "Shape: (441, 6)\n",
      "Memory usage: 0.09 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "object     3\n",
      "float64    2\n",
      "int64      1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Numeric columns summary:\n",
      "          stop_id    stop_lat    stop_lon\n",
      "count  441.000000  441.000000  441.000000\n",
      "mean   223.859410   31.203577   29.931327\n",
      "std    129.187841    0.065617    0.087965\n",
      "min      1.000000   31.065325   29.717785\n",
      "25%    112.000000   31.178913   29.885535\n",
      "50%    223.000000   31.210804   29.948260\n",
      "75%    336.000000   31.254638   29.997369\n",
      "max    447.000000   31.320357   30.071091\n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "stop_name:\n",
      "stop_name\n",
      "El Awayed         7\n",
      "El Saah Square    6\n",
      "Kilo 21           5\n",
      "Name: count, dtype: int64\n",
      "\n",
      "city:\n",
      "city\n",
      "alexendria    441\n",
      "Name: count, dtype: int64\n",
      "\n",
      "source_file:\n",
      "source_file\n",
      "stops.txt    441\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "   stop_id                  stop_name   stop_lat   stop_lon        city  \\\n",
      "0        1  Borg Al-Arab Old Terminal  31.065325  29.717785  alexendria   \n",
      "1        2              Baheeg Square  31.065325  29.717785  alexendria   \n",
      "2        3            Al-Marwa Mosque  31.065325  29.717785  alexendria   \n",
      "\n",
      "  source_file  \n",
      "0   stops.txt  \n",
      "1   stops.txt  \n",
      "2   stops.txt  \n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\alexendria\\cleaned_stops.csv\n",
      "\n",
      "----------------------------------------\n",
      "Processing: alexendria/stop_times.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (2547, 6)\n",
      "\n",
      "Cleaning data for alexendria/stop_times.txt...\n",
      "  - Converted 'arrival_time' to datetime\n",
      "  - Converted 'departure_time' to datetime\n",
      "  - Converted 'timepoint' to datetime\n",
      "  - Capped 108 outliers in 'stop_sequence'\n",
      "  ✓ Cleaned data shape: (2547, 6) → (2547, 8)\n",
      "\n",
      "--- Analysis for alexendria/stop_times.txt ---\n",
      "Shape: (2547, 8)\n",
      "Memory usage: 0.59 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "object            3\n",
      "datetime64[ns]    3\n",
      "int64             2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Numeric columns summary:\n",
      "           stop_id  stop_sequence\n",
      "count  2547.000000    2547.000000\n",
      "mean    237.499018      11.323518\n",
      "std     115.329346       9.282285\n",
      "min       1.000000       1.000000\n",
      "25%     164.500000       4.000000\n",
      "50%     229.000000       9.000000\n",
      "75%     329.000000      16.000000\n",
      "max     447.000000      34.000000\n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "trip_id:\n",
      "trip_id\n",
      "RmCIDxJQKwF7GOCKKX3Oh-07:00:00    54\n",
      "LYcY_J8e-Dijvjm6uZhgZ-07:00:00    52\n",
      "Z-51SkRkO5YyfPeYNTS-n-07:00:00    50\n",
      "Name: count, dtype: int64\n",
      "\n",
      "city:\n",
      "city\n",
      "alexendria    2547\n",
      "Name: count, dtype: int64\n",
      "\n",
      "source_file:\n",
      "source_file\n",
      "stop_times.txt    2547\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "                          trip_id  stop_id  stop_sequence        arrival_time  \\\n",
      "0  _npHlyCCY7o0R20RyqvT8-07:00:00      189              1 2025-08-11 07:00:00   \n",
      "1  _npHlyCCY7o0R20RyqvT8-07:00:00      192              2 2025-08-11 07:04:58   \n",
      "2  _npHlyCCY7o0R20RyqvT8-07:00:00      197              3 2025-08-11 07:07:07   \n",
      "\n",
      "       departure_time  timepoint        city     source_file  \n",
      "0 2025-08-11 07:00:15 1970-01-01  alexendria  stop_times.txt  \n",
      "1 2025-08-11 07:05:13 1970-01-01  alexendria  stop_times.txt  \n",
      "2 2025-08-11 07:07:22 1970-01-01  alexendria  stop_times.txt  \n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\alexendria\\cleaned_stop_times.csv\n",
      "\n",
      "----------------------------------------\n",
      "Processing: alexendria/trips.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (192, 6)\n",
      "\n",
      "Cleaning data for alexendria/trips.txt...\n",
      "  ✓ Cleaned data shape: (192, 6) → (192, 8)\n",
      "\n",
      "--- Analysis for alexendria/trips.txt ---\n",
      "Shape: (192, 8)\n",
      "Memory usage: 0.09 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "object    7\n",
      "int64     1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Numeric columns summary:\n",
      "       direction_id\n",
      "count    192.000000\n",
      "mean       0.494792\n",
      "std        0.501280\n",
      "min        0.000000\n",
      "25%        0.000000\n",
      "50%        0.000000\n",
      "75%        1.000000\n",
      "max        1.000000\n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "route_id:\n",
      "route_id\n",
      "I46ZQc9g0OMvTpnnq0RXs    2\n",
      "tv4mLSYvBC5Q4aSnL5h83    2\n",
      "d591FuxnMulU7vr6uNxrB    2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "service_id:\n",
      "service_id\n",
      "Ground_Daily    192\n",
      "Name: count, dtype: int64\n",
      "\n",
      "trip_headsign:\n",
      "trip_headsign\n",
      "Train Station (El-Shohada Square)    22\n",
      "El-Awayed                            19\n",
      "El-Mansheya                          15\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "                route_id    service_id trip_headsign  direction_id  \\\n",
      "0  I46ZQc9g0OMvTpnnq0RXs  Ground_Daily        Asafra             1   \n",
      "1  tv4mLSYvBC5Q4aSnL5h83  Ground_Daily        Asafra             1   \n",
      "2  d591FuxnMulU7vr6uNxrB  Ground_Daily        Asafra             0   \n",
      "\n",
      "                      shape_id                         trip_id        city  \\\n",
      "0  OvxARhItfijPV-_xuvfnZ_Shape  OvxARhItfijPV-_xuvfnZ-07:00:00  alexendria   \n",
      "1  zivMUtHDofqSlzEUoK8F0_Shape  zivMUtHDofqSlzEUoK8F0-07:00:00  alexendria   \n",
      "2  Wq0wtD2-ddsT-JtVfCc3g_Shape  Wq0wtD2-ddsT-JtVfCc3g-07:00:00  alexendria   \n",
      "\n",
      "  source_file  \n",
      "0   trips.txt  \n",
      "1   trips.txt  \n",
      "2   trips.txt  \n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\alexendria\\cleaned_trips.csv\n",
      "\n",
      "----------------------------------------\n",
      "COMBINING DATA FOR ALEXENDRIA\n",
      "----------------------------------------\n",
      "⚠ Too few common columns to combine alexendria data\n",
      "\n",
      "============================================================\n",
      "PROCESSING CITY: BAMAKO\n",
      "============================================================\n",
      "Found 10 data files in bamako: ['agency.txt', 'calendar.txt', 'calendar_dates.txt', 'feed_info.txt', 'frequencies.txt', 'routes.txt', 'shapes.txt', 'stops.txt', 'stop_times.txt', 'trips.txt']\n",
      "\n",
      "----------------------------------------\n",
      "Processing: bamako/agency.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (1, 8)\n",
      "\n",
      "Cleaning data for bamako/agency.txt...\n",
      "  - Converted 'agency_timezone' to datetime\n",
      "  ✓ Cleaned data shape: (1, 8) → (1, 9)\n",
      "\n",
      "--- Analysis for bamako/agency.txt ---\n",
      "Shape: (1, 9)\n",
      "Memory usage: 0.00 MB\n",
      "\n",
      "Missing values:\n",
      "agency_timezone    1\n",
      "dtype: int64\n",
      "\n",
      "Data types:\n",
      "object            7\n",
      "datetime64[ns]    1\n",
      "int64             1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Numeric columns summary:\n",
      "       agency_phone\n",
      "count  1.000000e+00\n",
      "mean   2.237368e+10\n",
      "std             NaN\n",
      "min    2.237368e+10\n",
      "25%    2.237368e+10\n",
      "50%    2.237368e+10\n",
      "75%    2.237368e+10\n",
      "max    2.237368e+10\n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "agency_id:\n",
      "agency_id\n",
      "sotrama_bko    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "agency_name:\n",
      "agency_name\n",
      "Sotrama Bamako    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "agency_url:\n",
      "agency_url\n",
      "https://billetexpress.ml    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "     agency_id     agency_name                agency_url agency_timezone  \\\n",
      "0  sotrama_bko  Sotrama Bamako  https://billetexpress.ml             NaT   \n",
      "\n",
      "  agency_lang  agency_phone           agency_email    city source_file  \n",
      "0          fr   22373678423  info@billetexpress.ml  bamako  agency.txt  \n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\bamako\\cleaned_agency.csv\n",
      "\n",
      "----------------------------------------\n",
      "Processing: bamako/calendar.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (1, 10)\n",
      "\n",
      "Cleaning data for bamako/calendar.txt...\n",
      "  - Converted 'start_date' to datetime\n",
      "  - Converted 'end_date' to datetime\n",
      "  ✓ Cleaned data shape: (1, 10) → (1, 12)\n",
      "\n",
      "--- Analysis for bamako/calendar.txt ---\n",
      "Shape: (1, 12)\n",
      "Memory usage: 0.00 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "int64             7\n",
      "object            3\n",
      "datetime64[ns]    2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Numeric columns summary:\n",
      "       monday  tuesday  wednesday  thursday  friday  saturday  sunday\n",
      "count     1.0      1.0        1.0       1.0     1.0       1.0     1.0\n",
      "mean      1.0      1.0        1.0       1.0     1.0       1.0     1.0\n",
      "std       NaN      NaN        NaN       NaN     NaN       NaN     NaN\n",
      "min       1.0      1.0        1.0       1.0     1.0       1.0     1.0\n",
      "25%       1.0      1.0        1.0       1.0     1.0       1.0     1.0\n",
      "50%       1.0      1.0        1.0       1.0     1.0       1.0     1.0\n",
      "75%       1.0      1.0        1.0       1.0     1.0       1.0     1.0\n",
      "max       1.0      1.0        1.0       1.0     1.0       1.0     1.0\n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "service_id:\n",
      "service_id\n",
      "MON-SUN    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "city:\n",
      "city\n",
      "bamako    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "source_file:\n",
      "source_file\n",
      "calendar.txt    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "  service_id  monday  tuesday  wednesday  thursday  friday  saturday  sunday  \\\n",
      "0    MON-SUN       1        1          1         1       1         1       1   \n",
      "\n",
      "                     start_date                      end_date    city  \\\n",
      "0 1970-01-01 00:00:00.020191201 1970-01-01 00:00:00.020220131  bamako   \n",
      "\n",
      "    source_file  \n",
      "0  calendar.txt  \n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\bamako\\cleaned_calendar.csv\n",
      "\n",
      "----------------------------------------\n",
      "Processing: bamako/calendar_dates.txt\n",
      "----------------------------------------\n",
      "  ❌ Could not load trotro\\trotrolive-datasets\\bamako\\calendar_dates.txt with any separator\n",
      "\n",
      "----------------------------------------\n",
      "Processing: bamako/feed_info.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (1, 6)\n",
      "\n",
      "Cleaning data for bamako/feed_info.txt...\n",
      "  - Converted 'feed_start_date' to datetime\n",
      "  - Converted 'feed_end_date' to datetime\n",
      "  ✓ Cleaned data shape: (1, 6) → (1, 8)\n",
      "\n",
      "--- Analysis for bamako/feed_info.txt ---\n",
      "Shape: (1, 8)\n",
      "Memory usage: 0.00 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "object            5\n",
      "datetime64[ns]    2\n",
      "float64           1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Numeric columns summary:\n",
      "       feed_version\n",
      "count           1.0\n",
      "mean            2.0\n",
      "std             NaN\n",
      "min             2.0\n",
      "25%             2.0\n",
      "50%             2.0\n",
      "75%             2.0\n",
      "max             2.0\n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "feed_publisher_name:\n",
      "feed_publisher_name\n",
      "Data Transport    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "feed_lang:\n",
      "feed_lang\n",
      "fr    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "feed_publisher_url:\n",
      "feed_publisher_url\n",
      "https://www.data-transport.org    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "  feed_publisher_name               feed_start_date  feed_version  \\\n",
      "0      Data Transport 1970-01-01 00:00:00.020191201           2.0   \n",
      "\n",
      "                  feed_end_date feed_lang              feed_publisher_url  \\\n",
      "0 1970-01-01 00:00:00.020220131        fr  https://www.data-transport.org   \n",
      "\n",
      "     city    source_file  \n",
      "0  bamako  feed_info.txt  \n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\bamako\\cleaned_feed_info.csv\n",
      "\n",
      "----------------------------------------\n",
      "Processing: bamako/frequencies.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (210, 5)\n",
      "\n",
      "Cleaning data for bamako/frequencies.txt...\n",
      "  - Converted 'start_time' to datetime\n",
      "  - Converted 'end_time' to datetime\n",
      "  ✓ Cleaned data shape: (210, 5) → (210, 6)\n",
      "\n",
      "--- Analysis for bamako/frequencies.txt ---\n",
      "Shape: (210, 6)\n",
      "Memory usage: 0.04 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "object            3\n",
      "datetime64[ns]    2\n",
      "int64             1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Numeric columns summary:\n",
      "       headway_secs\n",
      "count         210.0\n",
      "mean          600.0\n",
      "std             0.0\n",
      "min           600.0\n",
      "25%           600.0\n",
      "50%           600.0\n",
      "75%           600.0\n",
      "max           600.0\n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "trip_id:\n",
      "trip_id\n",
      "TI1576360557    1\n",
      "TI1576361613    1\n",
      "TI1576362266    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "city:\n",
      "city\n",
      "bamako    210\n",
      "Name: count, dtype: int64\n",
      "\n",
      "source_file:\n",
      "source_file\n",
      "frequencies.txt    210\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "        trip_id          start_time            end_time  headway_secs    city  \\\n",
      "0  TI1576360557 2025-08-11 05:00:00 2025-08-11 23:00:00           600  bamako   \n",
      "1  TI1576361613 2025-08-11 05:00:00 2025-08-11 23:00:00           600  bamako   \n",
      "2  TI1576362266 2025-08-11 05:00:00 2025-08-11 23:00:00           600  bamako   \n",
      "\n",
      "       source_file  \n",
      "0  frequencies.txt  \n",
      "1  frequencies.txt  \n",
      "2  frequencies.txt  \n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\bamako\\cleaned_frequencies.csv\n",
      "\n",
      "----------------------------------------\n",
      "Processing: bamako/routes.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (105, 10)\n",
      "\n",
      "Cleaning data for bamako/routes.txt...\n",
      "  ✓ Cleaned data shape: (105, 10) → (105, 7)\n",
      "\n",
      "--- Analysis for bamako/routes.txt ---\n",
      "Shape: (105, 7)\n",
      "Memory usage: 0.04 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "object    5\n",
      "int64     2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Numeric columns summary:\n",
      "       route_short_name  route_type\n",
      "count        105.000000       105.0\n",
      "mean          53.000000         3.0\n",
      "std           30.454885         0.0\n",
      "min            1.000000         3.0\n",
      "25%           27.000000         3.0\n",
      "50%           53.000000         3.0\n",
      "75%           79.000000         3.0\n",
      "max          105.000000         3.0\n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "route_id:\n",
      "route_id\n",
      "Sotrama.L1    1\n",
      "Sotrama.L2    1\n",
      "Sotrama.L6    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "agency_id:\n",
      "agency_id\n",
      "sotrama_bko    105\n",
      "Name: count, dtype: int64\n",
      "\n",
      "route_long_name:\n",
      "route_long_name\n",
      "Sotrama 1 : Railda ⇄ Sangarébougou         1\n",
      "Sotrama 2 : Railda ⇄ Marseille             1\n",
      "Sotrama 6 : Doumanzana_Nafadji ⇄ Railda    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "     route_id    agency_id  route_short_name  \\\n",
      "0  Sotrama.L1  sotrama_bko                 1   \n",
      "1  Sotrama.L2  sotrama_bko                 2   \n",
      "2  Sotrama.L6  sotrama_bko                 6   \n",
      "\n",
      "                           route_long_name  route_type    city source_file  \n",
      "0       Sotrama 1 : Railda ⇄ Sangarébougou           3  bamako  routes.txt  \n",
      "1           Sotrama 2 : Railda ⇄ Marseille           3  bamako  routes.txt  \n",
      "2  Sotrama 6 : Doumanzana_Nafadji ⇄ Railda           3  bamako  routes.txt  \n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\bamako\\cleaned_routes.csv\n",
      "\n",
      "----------------------------------------\n",
      "Processing: bamako/shapes.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (50712, 5)\n",
      "\n",
      "Cleaning data for bamako/shapes.txt...\n",
      "  - Capped 945 outliers in 'shape_pt_lat'\n",
      "  - Capped 2618 outliers in 'shape_pt_lon'\n",
      "  - Capped 735 outliers in 'shape_pt_sequence'\n",
      "  ✓ Cleaned data shape: (50712, 5) → (50712, 6)\n",
      "\n",
      "--- Analysis for bamako/shapes.txt ---\n",
      "Shape: (50712, 6)\n",
      "Memory usage: 9.67 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "object     3\n",
      "float64    3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Numeric columns summary:\n",
      "       shape_pt_lat  shape_pt_lon  shape_pt_sequence\n",
      "count  50712.000000  50712.000000       50712.000000\n",
      "mean      12.619227     -7.989494         137.916933\n",
      "std        0.046910      0.044482          94.933619\n",
      "min       12.494358     -8.085871           1.000000\n",
      "25%       12.587670     -8.012592          61.000000\n",
      "50%       12.624820     -7.996010         123.000000\n",
      "75%       12.649877     -7.963739         198.000000\n",
      "max       12.743189     -7.890459         403.500000\n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "shape_id:\n",
      "shape_id\n",
      "SHI1577467646    563\n",
      "SHI1577182075    506\n",
      "SHI1577468575    486\n",
      "Name: count, dtype: int64\n",
      "\n",
      "city:\n",
      "city\n",
      "bamako    50712\n",
      "Name: count, dtype: int64\n",
      "\n",
      "source_file:\n",
      "source_file\n",
      "shapes.txt    50712\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "        shape_id  shape_pt_lat  shape_pt_lon  shape_pt_sequence    city  \\\n",
      "0  SHI1576361537     12.650056     -7.993281                1.0  bamako   \n",
      "1  SHI1576361537     12.650167     -7.992815                2.0  bamako   \n",
      "2  SHI1576361537     12.650318     -7.992191                3.0  bamako   \n",
      "\n",
      "  source_file  \n",
      "0  shapes.txt  \n",
      "1  shapes.txt  \n",
      "2  shapes.txt  \n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\bamako\\cleaned_shapes.csv\n",
      "\n",
      "----------------------------------------\n",
      "Processing: bamako/stops.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (3457, 14)\n",
      "\n",
      "Cleaning data for bamako/stops.txt...\n",
      "  - Capped 144 outliers in 'stop_id'\n",
      "  - Capped 18 outliers in 'stop_lat'\n",
      "  - Capped 49 outliers in 'stop_lon'\n",
      "  ✓ Cleaned data shape: (3457, 14) → (3457, 7)\n",
      "\n",
      "--- Analysis for bamako/stops.txt ---\n",
      "Shape: (3457, 7)\n",
      "Memory usage: 0.92 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "object     4\n",
      "float64    3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Numeric columns summary:\n",
      "            stop_id     stop_lat     stop_lon\n",
      "count  3.457000e+03  3457.000000  3457.000000\n",
      "mean   6.976894e+09    12.621817    -7.986941\n",
      "std    5.157485e+07     0.054557     0.052037\n",
      "min    6.845042e+09    12.455685    -8.120782\n",
      "25%    6.951594e+09    12.581540    -8.020236\n",
      "50%    6.960222e+09    12.627281    -7.990448\n",
      "75%    7.022629e+09    12.665443    -7.953204\n",
      "max    7.083578e+09    12.766977    -7.852658\n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "stop_name:\n",
      "stop_name\n",
      "Arrêt            283\n",
      "Arrêt Sotrama     87\n",
      "Arret             83\n",
      "Name: count, dtype: int64\n",
      "\n",
      "stop_desc:\n",
      "stop_desc\n",
      "Arrêt BKO            283\n",
      "Arrêt Sotrama BKO     88\n",
      "Arret BKO             83\n",
      "Name: count, dtype: int64\n",
      "\n",
      "city:\n",
      "city\n",
      "bamako    3457\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "        stop_id      stop_name          stop_desc   stop_lat  stop_lon  \\\n",
      "0  6.954470e+09  Hirondelle Da  Hirondelle Da BKO  12.654686 -7.976516   \n",
      "1  6.936887e+09         Railda         Railda BKO  12.650048 -7.993274   \n",
      "2  6.936887e+09   Troisième Da   Troisième Da BKO  12.653119 -7.981954   \n",
      "\n",
      "     city source_file  \n",
      "0  bamako   stops.txt  \n",
      "1  bamako   stops.txt  \n",
      "2  bamako   stops.txt  \n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\bamako\\cleaned_stops.csv\n",
      "\n",
      "----------------------------------------\n",
      "Processing: bamako/stop_times.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (8826, 10)\n",
      "\n",
      "Cleaning data for bamako/stop_times.txt...\n",
      "  - Converted 'arrival_time' to datetime\n",
      "  - Converted 'departure_time' to datetime\n",
      "  - Capped 2495 outliers in 'stop_id'\n",
      "  - Capped 275 outliers in 'stop_sequence'\n",
      "  ✓ Cleaned data shape: (8826, 10) → (8826, 7)\n",
      "\n",
      "--- Analysis for bamako/stop_times.txt ---\n",
      "Shape: (8826, 7)\n",
      "Memory usage: 1.78 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "object            3\n",
      "datetime64[ns]    2\n",
      "float64           1\n",
      "int64             1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Numeric columns summary:\n",
      "            stop_id  stop_sequence\n",
      "count  8.826000e+03    8826.000000\n",
      "mean   6.958674e+09      30.107863\n",
      "std    1.989276e+07      23.762477\n",
      "min    6.922906e+09       1.000000\n",
      "25%    6.948970e+09      11.000000\n",
      "50%    6.955521e+09      24.000000\n",
      "75%    6.966346e+09      43.000000\n",
      "max    6.992411e+09      91.000000\n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "trip_id:\n",
      "trip_id\n",
      "TI1577354850    138\n",
      "TI1577202076    125\n",
      "TI1577107909    122\n",
      "Name: count, dtype: int64\n",
      "\n",
      "city:\n",
      "city\n",
      "bamako    8826\n",
      "Name: count, dtype: int64\n",
      "\n",
      "source_file:\n",
      "source_file\n",
      "stop_times.txt    8826\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "        trip_id        arrival_time      departure_time       stop_id  \\\n",
      "0  TI1577055855 2025-08-11 05:00:00 2025-08-11 05:00:00  6.958399e+09   \n",
      "1  TI1577055855 2025-08-11 05:26:28 2025-08-11 05:26:28  6.952268e+09   \n",
      "2  TI1577108055 2025-08-11 05:00:00 2025-08-11 05:00:00  6.922906e+09   \n",
      "\n",
      "   stop_sequence    city     source_file  \n",
      "0              1  bamako  stop_times.txt  \n",
      "1              2  bamako  stop_times.txt  \n",
      "2              1  bamako  stop_times.txt  \n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\bamako\\cleaned_stop_times.csv\n",
      "\n",
      "----------------------------------------\n",
      "Processing: bamako/trips.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (210, 10)\n",
      "\n",
      "Cleaning data for bamako/trips.txt...\n",
      "  ✓ Cleaned data shape: (210, 10) → (210, 8)\n",
      "\n",
      "--- Analysis for bamako/trips.txt ---\n",
      "Shape: (210, 8)\n",
      "Memory usage: 0.08 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "object    7\n",
      "int64     1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Numeric columns summary:\n",
      "       direction_id\n",
      "count    210.000000\n",
      "mean       0.500000\n",
      "std        0.501195\n",
      "min        0.000000\n",
      "25%        0.000000\n",
      "50%        0.500000\n",
      "75%        1.000000\n",
      "max        1.000000\n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "route_id:\n",
      "route_id\n",
      "Sotrama.L1    2\n",
      "Sotrama.L2    2\n",
      "Sotrama.L6    2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "service_id:\n",
      "service_id\n",
      "MON-SUN    210\n",
      "Name: count, dtype: int64\n",
      "\n",
      "trip_id:\n",
      "trip_id\n",
      "TI1576360557    1\n",
      "TI1576361613    1\n",
      "TI1576362266    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "     route_id service_id       trip_id       trip_headsign  direction_id  \\\n",
      "0  Sotrama.L1    MON-SUN  TI1576360557       Sangarébougou             0   \n",
      "1  Sotrama.L2    MON-SUN  TI1576361613           Marseille             0   \n",
      "2  Sotrama.L6    MON-SUN  TI1576362266  Doumanzana_Nafadji             0   \n",
      "\n",
      "        shape_id    city source_file  \n",
      "0  SHI1576361537  bamako   trips.txt  \n",
      "1  SHI1576362584  bamako   trips.txt  \n",
      "2  SHI1576363245  bamako   trips.txt  \n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\bamako\\cleaned_trips.csv\n",
      "\n",
      "----------------------------------------\n",
      "COMBINING DATA FOR BAMAKO\n",
      "----------------------------------------\n",
      "⚠ Too few common columns to combine bamako data\n",
      "\n",
      "============================================================\n",
      "PROCESSING CITY: FREETOWN\n",
      "============================================================\n",
      "Found 11 data files in freetown: ['agency.txt', 'calendar.txt', 'fare_attributes.txt', 'fare_rules.txt', 'feed_info.txt', 'frequencies.txt', 'routes.txt', 'shapes.txt', 'stops.txt', 'stop_times.txt', 'trips.txt']\n",
      "\n",
      "----------------------------------------\n",
      "Processing: freetown/agency.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (4, 8)\n",
      "\n",
      "Cleaning data for freetown/agency.txt...\n",
      "  - Converted 'agency_timezone' to datetime\n",
      "  ✓ Cleaned data shape: (4, 8) → (4, 7)\n",
      "\n",
      "--- Analysis for freetown/agency.txt ---\n",
      "Shape: (4, 7)\n",
      "Memory usage: 0.00 MB\n",
      "\n",
      "Missing values:\n",
      "agency_timezone    4\n",
      "dtype: int64\n",
      "\n",
      "Data types:\n",
      "object            6\n",
      "datetime64[ns]    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "agency_id:\n",
      "agency_id\n",
      "Freetown_poda_poda_01       1\n",
      "Freetown_SLRTC_03           1\n",
      "Freetown_Tagrin_Ferry_01    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "agency_name:\n",
      "agency_name\n",
      "Freetown Poda-Poda                         1\n",
      "Sierra Leone Road Transport Corporation    1\n",
      "Tagrin Ferry                               1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "agency_url:\n",
      "agency_url\n",
      "http://www.whereismytransport.com    4\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "                  agency_id                              agency_name  \\\n",
      "0     Freetown_poda_poda_01                       Freetown Poda-Poda   \n",
      "1         Freetown_SLRTC_03  Sierra Leone Road Transport Corporation   \n",
      "2  Freetown_Tagrin_Ferry_01                             Tagrin Ferry   \n",
      "\n",
      "                          agency_url agency_timezone agency_lang      city  \\\n",
      "0  http://www.whereismytransport.com             NaT          en  freetown   \n",
      "1  http://www.whereismytransport.com             NaT          en  freetown   \n",
      "2  http://www.whereismytransport.com             NaT          en  freetown   \n",
      "\n",
      "  source_file  \n",
      "0  agency.txt  \n",
      "1  agency.txt  \n",
      "2  agency.txt  \n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\freetown\\cleaned_agency.csv\n",
      "\n",
      "----------------------------------------\n",
      "Processing: freetown/calendar.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (3, 10)\n",
      "\n",
      "Cleaning data for freetown/calendar.txt...\n",
      "  - Converted 'start_date' to datetime\n",
      "  - Converted 'end_date' to datetime\n",
      "  ✓ Cleaned data shape: (3, 10) → (3, 12)\n",
      "\n",
      "--- Analysis for freetown/calendar.txt ---\n",
      "Shape: (3, 12)\n",
      "Memory usage: 0.00 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "int64             7\n",
      "object            3\n",
      "datetime64[ns]    2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Numeric columns summary:\n",
      "         monday   tuesday  wednesday  thursday    friday  saturday    sunday\n",
      "count  3.000000  3.000000   3.000000  3.000000  3.000000  3.000000  3.000000\n",
      "mean   0.666667  0.666667   0.666667  0.666667  0.666667  0.666667  0.666667\n",
      "std    0.577350  0.577350   0.577350  0.577350  0.577350  0.577350  0.577350\n",
      "min    0.000000  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000\n",
      "25%    0.500000  0.500000   0.500000  0.500000  0.500000  0.500000  0.500000\n",
      "50%    1.000000  1.000000   1.000000  1.000000  1.000000  1.000000  1.000000\n",
      "75%    1.000000  1.000000   1.000000  1.000000  1.000000  1.000000  1.000000\n",
      "max    1.000000  1.000000   1.000000  1.000000  1.000000  1.000000  1.000000\n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "service_id:\n",
      "service_id\n",
      "service_0001    1\n",
      "service_0002    1\n",
      "service_0003    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "city:\n",
      "city\n",
      "freetown    3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "source_file:\n",
      "source_file\n",
      "calendar.txt    3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "     service_id  monday  tuesday  wednesday  thursday  friday  saturday  \\\n",
      "0  service_0001       0        0          0         0       0         1   \n",
      "1  service_0002       1        1          1         1       1         0   \n",
      "2  service_0003       1        1          1         1       1         1   \n",
      "\n",
      "   sunday                    start_date                      end_date  \\\n",
      "0       1 1970-01-01 00:00:00.020180929 1970-01-01 00:00:00.020191029   \n",
      "1       0 1970-01-01 00:00:00.020180929 1970-01-01 00:00:00.020191029   \n",
      "2       1 1970-01-01 00:00:00.020181108 1970-01-01 00:00:00.020191108   \n",
      "\n",
      "       city   source_file  \n",
      "0  freetown  calendar.txt  \n",
      "1  freetown  calendar.txt  \n",
      "2  freetown  calendar.txt  \n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\freetown\\cleaned_calendar.csv\n",
      "\n",
      "----------------------------------------\n",
      "Processing: freetown/fare_attributes.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (103, 7)\n",
      "\n",
      "Cleaning data for freetown/fare_attributes.txt...\n",
      "  - Capped 2 outliers in 'price'\n",
      "  ✓ Cleaned data shape: (103, 7) → (103, 7)\n",
      "\n",
      "--- Analysis for freetown/fare_attributes.txt ---\n",
      "Shape: (103, 7)\n",
      "Memory usage: 0.03 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "object    5\n",
      "int64     2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Numeric columns summary:\n",
      "             price  payment_method\n",
      "count   103.000000      103.000000\n",
      "mean   2121.359223        0.019417\n",
      "std     988.204996        0.138662\n",
      "min    1500.000000        0.000000\n",
      "25%    1500.000000        0.000000\n",
      "50%    1500.000000        0.000000\n",
      "75%    3000.000000        0.000000\n",
      "max    5250.000000        1.000000\n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "fare_id:\n",
      "fare_id\n",
      "fare_0001    1\n",
      "fare_0002    1\n",
      "fare_0003    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "currency_type:\n",
      "currency_type\n",
      "SLL    103\n",
      "Name: count, dtype: int64\n",
      "\n",
      "agency_id:\n",
      "agency_id\n",
      "Freetown_poda_poda_01    69\n",
      "Freetown_Taxi_Cab_04     22\n",
      "Freetown_SLRTC_03        10\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "     fare_id  price currency_type  payment_method              agency_id  \\\n",
      "0  fare_0001   1500           SLL               0  Freetown_poda_poda_01   \n",
      "1  fare_0002   1500           SLL               0  Freetown_poda_poda_01   \n",
      "2  fare_0003   1500           SLL               0      Freetown_SLRTC_03   \n",
      "\n",
      "       city          source_file  \n",
      "0  freetown  fare_attributes.txt  \n",
      "1  freetown  fare_attributes.txt  \n",
      "2  freetown  fare_attributes.txt  \n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\freetown\\cleaned_fare_attributes.csv\n",
      "\n",
      "----------------------------------------\n",
      "Processing: freetown/fare_rules.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (103, 5)\n",
      "\n",
      "Cleaning data for freetown/fare_rules.txt...\n",
      "  ✓ Cleaned data shape: (103, 5) → (103, 4)\n",
      "\n",
      "--- Analysis for freetown/fare_rules.txt ---\n",
      "Shape: (103, 4)\n",
      "Memory usage: 0.02 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "object    4\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "fare_id:\n",
      "fare_id\n",
      "fare_0001    1\n",
      "fare_0002    1\n",
      "fare_0003    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "route_id:\n",
      "route_id\n",
      "route_0001    1\n",
      "route_0002    1\n",
      "route_0003    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "city:\n",
      "city\n",
      "freetown    103\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "     fare_id    route_id      city     source_file\n",
      "0  fare_0001  route_0001  freetown  fare_rules.txt\n",
      "1  fare_0002  route_0002  freetown  fare_rules.txt\n",
      "2  fare_0003  route_0003  freetown  fare_rules.txt\n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\freetown\\cleaned_fare_rules.csv\n",
      "\n",
      "----------------------------------------\n",
      "Processing: freetown/feed_info.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (1, 6)\n",
      "\n",
      "Cleaning data for freetown/feed_info.txt...\n",
      "  - Converted 'feed_start_date' to datetime\n",
      "  - Converted 'feed_end_date' to datetime\n",
      "  ✓ Cleaned data shape: (1, 6) → (1, 8)\n",
      "\n",
      "--- Analysis for freetown/feed_info.txt ---\n",
      "Shape: (1, 8)\n",
      "Memory usage: 0.00 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "object            5\n",
      "datetime64[ns]    2\n",
      "int64             1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Numeric columns summary:\n",
      "       feed_version\n",
      "count           1.0\n",
      "mean            1.0\n",
      "std             NaN\n",
      "min             1.0\n",
      "25%             1.0\n",
      "50%             1.0\n",
      "75%             1.0\n",
      "max             1.0\n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "feed_publisher_name:\n",
      "feed_publisher_name\n",
      "WhereIsMyTransport    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "feed_publisher_url:\n",
      "feed_publisher_url\n",
      "https://www.whereismytransport.com    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "feed_lang:\n",
      "feed_lang\n",
      "en    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "  feed_publisher_name                  feed_publisher_url feed_lang  \\\n",
      "0  WhereIsMyTransport  https://www.whereismytransport.com        en   \n",
      "\n",
      "                feed_start_date                 feed_end_date  feed_version  \\\n",
      "0 1970-01-01 00:00:00.020170920 1970-01-01 00:00:00.020201231             1   \n",
      "\n",
      "       city    source_file  \n",
      "0  freetown  feed_info.txt  \n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\freetown\\cleaned_feed_info.csv\n",
      "\n",
      "----------------------------------------\n",
      "Processing: freetown/frequencies.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (860, 5)\n",
      "\n",
      "Cleaning data for freetown/frequencies.txt...\n",
      "  - Converted 'start_time' to datetime\n",
      "  - Converted 'end_time' to datetime\n",
      "  - Converted 'exact_times' to datetime\n",
      "  - Capped 44 outliers in 'headway_secs'\n",
      "  ✓ Cleaned data shape: (860, 5) → (860, 7)\n",
      "\n",
      "--- Analysis for freetown/frequencies.txt ---\n",
      "Shape: (860, 7)\n",
      "Memory usage: 0.17 MB\n",
      "\n",
      "Missing values:\n",
      "end_time    2\n",
      "dtype: int64\n",
      "\n",
      "Data types:\n",
      "object            3\n",
      "datetime64[ns]    3\n",
      "int64             1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Numeric columns summary:\n",
      "       headway_secs\n",
      "count    860.000000\n",
      "mean     812.302326\n",
      "std      643.499094\n",
      "min       60.000000\n",
      "25%      300.000000\n",
      "50%      600.000000\n",
      "75%     1200.000000\n",
      "max     2550.000000\n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "trip_id:\n",
      "trip_id\n",
      "trip_0402    5\n",
      "trip_0396    5\n",
      "trip_0342    5\n",
      "Name: count, dtype: int64\n",
      "\n",
      "city:\n",
      "city\n",
      "freetown    860\n",
      "Name: count, dtype: int64\n",
      "\n",
      "source_file:\n",
      "source_file\n",
      "frequencies.txt    860\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "     trip_id          start_time            end_time  headway_secs  \\\n",
      "0  trip_0001 2025-08-11 07:00:00 2025-08-11 10:00:00           300   \n",
      "1  trip_0002 2025-08-11 05:00:00 2025-08-11 07:00:00           480   \n",
      "2  trip_0002 2025-08-11 10:00:00 2025-08-11 16:00:00           480   \n",
      "\n",
      "  exact_times      city      source_file  \n",
      "0  1970-01-01  freetown  frequencies.txt  \n",
      "1  1970-01-01  freetown  frequencies.txt  \n",
      "2  1970-01-01  freetown  frequencies.txt  \n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\freetown\\cleaned_frequencies.csv\n",
      "\n",
      "----------------------------------------\n",
      "Processing: freetown/routes.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (103, 9)\n",
      "\n",
      "Cleaning data for freetown/routes.txt...\n",
      "  ✓ Cleaned data shape: (103, 9) → (103, 8)\n",
      "\n",
      "--- Analysis for freetown/routes.txt ---\n",
      "Shape: (103, 8)\n",
      "Memory usage: 0.04 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "object    7\n",
      "int64     1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Numeric columns summary:\n",
      "        route_type\n",
      "count   103.000000\n",
      "mean    709.708738\n",
      "std      69.330924\n",
      "min     700.000000\n",
      "25%     700.000000\n",
      "50%     700.000000\n",
      "75%     700.000000\n",
      "max    1200.000000\n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "route_id:\n",
      "route_id\n",
      "route_0001    1\n",
      "route_0002    1\n",
      "route_0003    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "agency_id:\n",
      "agency_id\n",
      "Freetown_poda_poda_01    69\n",
      "Freetown_Taxi_Cab_04     22\n",
      "Freetown_SLRTC_03        10\n",
      "Name: count, dtype: int64\n",
      "\n",
      "route_long_name:\n",
      "route_long_name\n",
      "Sackville Street to Wellington    2\n",
      "Lumley to Regent Road             1\n",
      "Calaba Town Eastern Police        1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "     route_id              agency_id                route_long_name  \\\n",
      "0  route_0001  Freetown_poda_poda_01          Lumley to Regent Road   \n",
      "1  route_0002  Freetown_poda_poda_01  Congo Town to Goderich Street   \n",
      "2  route_0003      Freetown_SLRTC_03     Calaba Town Eastern Police   \n",
      "\n",
      "   route_type route_color route_text_color      city source_file  \n",
      "0         700      0EC904           000000  freetown  routes.txt  \n",
      "1         700      4CC904           000000  freetown  routes.txt  \n",
      "2         700      048AC9           000000  freetown  routes.txt  \n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\freetown\\cleaned_routes.csv\n",
      "\n",
      "----------------------------------------\n",
      "Processing: freetown/shapes.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (29641, 5)\n",
      "\n",
      "Cleaning data for freetown/shapes.txt...\n",
      "  - Capped 2363 outliers in 'shape_pt_lat'\n",
      "  - Capped 643 outliers in 'shape_pt_lon'\n",
      "  - Capped 1694 outliers in 'shape_pt_sequence'\n",
      "  - Capped 6114 outliers in 'shape_dist_traveled'\n",
      "  ✓ Cleaned data shape: (29641, 5) → (29641, 7)\n",
      "\n",
      "--- Analysis for freetown/shapes.txt ---\n",
      "Shape: (29641, 7)\n",
      "Memory usage: 5.86 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "object     3\n",
      "float64    3\n",
      "int64      1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Numeric columns summary:\n",
      "       shape_pt_lat  shape_pt_lon  shape_pt_sequence  shape_dist_traveled\n",
      "count  29641.000000  29641.000000       29641.000000         29641.000000\n",
      "mean       8.464273    -13.215767         139.783779         13033.035120\n",
      "std        0.024746      0.037406         108.785479         14828.199404\n",
      "min        8.409420    -13.293041           0.000000             0.000000\n",
      "25%        8.452980    -13.244200          55.000000          2622.000000\n",
      "50%        8.474470    -13.219160         115.000000          5747.000000\n",
      "75%        8.482020    -13.192810         193.000000         17737.000000\n",
      "max        8.516630    -13.115725         400.000000         40409.500000\n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "shape_id:\n",
      "shape_id\n",
      "shape_0047    623\n",
      "shape_0058    619\n",
      "shape_0076    603\n",
      "Name: count, dtype: int64\n",
      "\n",
      "city:\n",
      "city\n",
      "freetown    29641\n",
      "Name: count, dtype: int64\n",
      "\n",
      "source_file:\n",
      "source_file\n",
      "shapes.txt    29641\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "       shape_id  shape_pt_lat  shape_pt_lon  shape_pt_sequence  \\\n",
      "0  route_0032_1       8.44450     -13.16181                  0   \n",
      "1  route_0032_1       8.44467     -13.16194                  1   \n",
      "2  route_0032_1       8.44507     -13.16227                  2   \n",
      "\n",
      "   shape_dist_traveled      city source_file  \n",
      "0                  0.0  freetown  shapes.txt  \n",
      "1                 23.0  freetown  shapes.txt  \n",
      "2                 80.0  freetown  shapes.txt  \n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\freetown\\cleaned_shapes.csv\n",
      "\n",
      "----------------------------------------\n",
      "Processing: freetown/stops.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (843, 12)\n",
      "\n",
      "Cleaning data for freetown/stops.txt...\n",
      "  - Dropped column 'parent_station' (>70.0% missing)\n",
      "  - Capped 50 outliers in 'stop_lat'\n",
      "  - Capped 28 outliers in 'stop_lon'\n",
      "  ✓ Cleaned data shape: (843, 12) → (843, 7)\n",
      "\n",
      "--- Analysis for freetown/stops.txt ---\n",
      "Shape: (843, 7)\n",
      "Memory usage: 0.21 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "object     4\n",
      "float64    2\n",
      "int64      1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Numeric columns summary:\n",
      "         stop_lat    stop_lon  location_type\n",
      "count  843.000000  843.000000     843.000000\n",
      "mean     8.455266  -13.216062       0.018980\n",
      "std      0.036340    0.044986       0.136535\n",
      "min      8.372450  -13.288040       0.000000\n",
      "25%      8.438210  -13.249710       0.000000\n",
      "50%      8.470490  -13.224660       0.000000\n",
      "75%      8.482050  -13.188680       0.000000\n",
      "max      8.516620  -13.097135       1.000000\n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "stop_id:\n",
      "stop_id\n",
      "hub_0001    1\n",
      "hub_0002    1\n",
      "hub_0003    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "stop_name:\n",
      "stop_name\n",
      "PWD            9\n",
      "Wilberforce    8\n",
      "Shell          8\n",
      "Name: count, dtype: int64\n",
      "\n",
      "city:\n",
      "city\n",
      "freetown    843\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "    stop_id    stop_name  stop_lat   stop_lon  location_type      city  \\\n",
      "0  hub_0001  Wilberforce  8.468191 -13.261957              1  freetown   \n",
      "1  hub_0002       Lumley  8.456025 -13.272294              1  freetown   \n",
      "2  hub_0003     Aberdeen  8.493606 -13.286955              1  freetown   \n",
      "\n",
      "  source_file  \n",
      "0   stops.txt  \n",
      "1   stops.txt  \n",
      "2   stops.txt  \n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\freetown\\cleaned_stops.csv\n",
      "\n",
      "----------------------------------------\n",
      "Processing: freetown/stop_times.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (6185, 9)\n",
      "\n",
      "Cleaning data for freetown/stop_times.txt...\n",
      "  - Dropped column 'shape_dist_traveled' (>70.0% missing)\n",
      "  - Converted 'arrival_time' to datetime\n",
      "  - Converted 'departure_time' to datetime\n",
      "  - Capped 78 outliers in 'stop_sequence'\n",
      "  ✓ Cleaned data shape: (6185, 9) → (6185, 7)\n",
      "\n",
      "--- Analysis for freetown/stop_times.txt ---\n",
      "Shape: (6185, 7)\n",
      "Memory usage: 1.53 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "object            4\n",
      "datetime64[ns]    2\n",
      "int64             1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Numeric columns summary:\n",
      "       stop_sequence\n",
      "count    6185.000000\n",
      "mean       10.967502\n",
      "std         8.144519\n",
      "min         1.000000\n",
      "25%         4.000000\n",
      "50%         9.000000\n",
      "75%        16.000000\n",
      "max        34.000000\n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "trip_id:\n",
      "trip_id\n",
      "trip_0089    41\n",
      "trip_0090    41\n",
      "trip_0092    41\n",
      "Name: count, dtype: int64\n",
      "\n",
      "stop_id:\n",
      "stop_id\n",
      "stop_0654    69\n",
      "stop_0627    54\n",
      "stop_0071    51\n",
      "Name: count, dtype: int64\n",
      "\n",
      "city:\n",
      "city\n",
      "freetown    6185\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "     trip_id        arrival_time      departure_time    stop_id  \\\n",
      "0  trip_0001 2025-08-11 08:15:14 2025-08-11 08:15:14  stop_0401   \n",
      "1  trip_0001 2025-08-11 08:17:13 2025-08-11 08:17:13  stop_0443   \n",
      "2  trip_0001 2025-08-11 08:21:51 2025-08-11 08:21:51  stop_0308   \n",
      "\n",
      "   stop_sequence      city     source_file  \n",
      "0              1  freetown  stop_times.txt  \n",
      "1              2  freetown  stop_times.txt  \n",
      "2              3  freetown  stop_times.txt  \n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\freetown\\cleaned_stop_times.csv\n",
      "\n",
      "----------------------------------------\n",
      "Processing: freetown/trips.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (404, 9)\n",
      "\n",
      "Cleaning data for freetown/trips.txt...\n",
      "  ✓ Cleaned data shape: (404, 9) → (404, 8)\n",
      "\n",
      "--- Analysis for freetown/trips.txt ---\n",
      "Shape: (404, 8)\n",
      "Memory usage: 0.18 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "object    8\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "trip_id:\n",
      "trip_id\n",
      "trip_0001    1\n",
      "trip_0002    1\n",
      "trip_0003    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "route_id:\n",
      "route_id\n",
      "route_0001    4\n",
      "route_0002    4\n",
      "route_0003    4\n",
      "Name: count, dtype: int64\n",
      "\n",
      "service_id:\n",
      "service_id\n",
      "service_0002    302\n",
      "service_0001    100\n",
      "service_0003      2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "     trip_id    route_id    service_id trip_headsign    trip_short_name  \\\n",
      "0  trip_0001  route_0001  service_0002   Regent Road  Morning peak trip   \n",
      "1  trip_0002  route_0001  service_0002   Regent Road      Off peak trip   \n",
      "2  trip_0003  route_0001  service_0002   Regent Road  Evening peak trip   \n",
      "\n",
      "     shape_id      city source_file  \n",
      "0  shape_0001  freetown   trips.txt  \n",
      "1  shape_0001  freetown   trips.txt  \n",
      "2  shape_0001  freetown   trips.txt  \n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\freetown\\cleaned_trips.csv\n",
      "\n",
      "----------------------------------------\n",
      "COMBINING DATA FOR FREETOWN\n",
      "----------------------------------------\n",
      "⚠ Too few common columns to combine freetown data\n",
      "\n",
      "============================================================\n",
      "PROCESSING CITY: KAMPALA\n",
      "============================================================\n",
      "Found 10 data files in kampala: ['agency.txt', 'calendar.txt', 'calendar_dates.txt', 'feed_info.txt', 'frequencies.txt', 'routes.txt', 'shapes.txt', 'stops.txt', 'stop_times.txt', 'trips.txt']\n",
      "\n",
      "----------------------------------------\n",
      "Processing: kampala/agency.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (2, 7)\n",
      "\n",
      "Cleaning data for kampala/agency.txt...\n",
      "  - Converted 'agency_timezone' to datetime\n",
      "  ✓ Cleaned data shape: (2, 7) → (2, 7)\n",
      "\n",
      "--- Analysis for kampala/agency.txt ---\n",
      "Shape: (2, 7)\n",
      "Memory usage: 0.00 MB\n",
      "\n",
      "Missing values:\n",
      "agency_timezone    2\n",
      "dtype: int64\n",
      "\n",
      "Data types:\n",
      "object            6\n",
      "datetime64[ns]    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "agency_id:\n",
      "agency_id\n",
      "taxi    1\n",
      "bus     1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "agency_name:\n",
      "agency_name\n",
      "14-Seater Paratransit Taxi                    1\n",
      "Buses oprated by Pioneer or Awakula Enumme    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "agency_url:\n",
      "agency_url\n",
      "http://transportforcairo.com/    2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "  agency_id                                 agency_name  \\\n",
      "0      taxi                  14-Seater Paratransit Taxi   \n",
      "1       bus  Buses oprated by Pioneer or Awakula Enumme   \n",
      "\n",
      "                      agency_url agency_timezone agency_lang     city  \\\n",
      "0  http://transportforcairo.com/             NaT          en  kampala   \n",
      "1  http://transportforcairo.com/             NaT          en  kampala   \n",
      "\n",
      "  source_file  \n",
      "0  agency.txt  \n",
      "1  agency.txt  \n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\kampala\\cleaned_agency.csv\n",
      "\n",
      "----------------------------------------\n",
      "Processing: kampala/calendar.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (1, 10)\n",
      "\n",
      "Cleaning data for kampala/calendar.txt...\n",
      "  - Converted 'end_date' to datetime\n",
      "  - Converted 'start_date' to datetime\n",
      "  ✓ Cleaned data shape: (1, 10) → (1, 12)\n",
      "\n",
      "--- Analysis for kampala/calendar.txt ---\n",
      "Shape: (1, 12)\n",
      "Memory usage: 0.00 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "int64             7\n",
      "object            3\n",
      "datetime64[ns]    2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Numeric columns summary:\n",
      "       friday  monday  saturday  sunday  thursday  tuesday  wednesday\n",
      "count     1.0     1.0       1.0     1.0       1.0      1.0        1.0\n",
      "mean      1.0     1.0       1.0     1.0       1.0      1.0        1.0\n",
      "std       NaN     NaN       NaN     NaN       NaN      NaN        NaN\n",
      "min       1.0     1.0       1.0     1.0       1.0      1.0        1.0\n",
      "25%       1.0     1.0       1.0     1.0       1.0      1.0        1.0\n",
      "50%       1.0     1.0       1.0     1.0       1.0      1.0        1.0\n",
      "75%       1.0     1.0       1.0     1.0       1.0      1.0        1.0\n",
      "max       1.0     1.0       1.0     1.0       1.0      1.0        1.0\n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "service_id:\n",
      "service_id\n",
      "Ground_Daily    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "city:\n",
      "city\n",
      "kampala    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "source_file:\n",
      "source_file\n",
      "calendar.txt    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "                       end_date  friday  monday  saturday    service_id  \\\n",
      "0 1970-01-01 00:00:00.020200929       1       1         1  Ground_Daily   \n",
      "\n",
      "                     start_date  sunday  thursday  tuesday  wednesday  \\\n",
      "0 1970-01-01 00:00:00.020191010       1         1        1          1   \n",
      "\n",
      "      city   source_file  \n",
      "0  kampala  calendar.txt  \n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\kampala\\cleaned_calendar.csv\n",
      "\n",
      "----------------------------------------\n",
      "Processing: kampala/calendar_dates.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (1, 3)\n",
      "\n",
      "Cleaning data for kampala/calendar_dates.txt...\n",
      "  - Converted 'date' to datetime\n",
      "  ✓ Cleaned data shape: (1, 3) → (1, 5)\n",
      "\n",
      "--- Analysis for kampala/calendar_dates.txt ---\n",
      "Shape: (1, 5)\n",
      "Memory usage: 0.00 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "object            3\n",
      "datetime64[ns]    1\n",
      "int64             1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Numeric columns summary:\n",
      "       exception_type\n",
      "count             1.0\n",
      "mean              2.0\n",
      "std               NaN\n",
      "min               2.0\n",
      "25%               2.0\n",
      "50%               2.0\n",
      "75%               2.0\n",
      "max               2.0\n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "service_id:\n",
      "service_id\n",
      "Ground_Daily    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "city:\n",
      "city\n",
      "kampala    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "source_file:\n",
      "source_file\n",
      "calendar_dates.txt    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "     service_id                          date  exception_type     city  \\\n",
      "0  Ground_Daily 1970-01-01 00:00:00.020170104               2  kampala   \n",
      "\n",
      "          source_file  \n",
      "0  calendar_dates.txt  \n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\kampala\\cleaned_calendar_dates.csv\n",
      "\n",
      "----------------------------------------\n",
      "Processing: kampala/feed_info.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (1, 6)\n",
      "\n",
      "Cleaning data for kampala/feed_info.txt...\n",
      "  - Converted 'feed_start_date' to datetime\n",
      "  - Converted 'feed_end_date' to datetime\n",
      "  ✓ Cleaned data shape: (1, 6) → (1, 8)\n",
      "\n",
      "--- Analysis for kampala/feed_info.txt ---\n",
      "Shape: (1, 8)\n",
      "Memory usage: 0.00 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "object            5\n",
      "datetime64[ns]    2\n",
      "float64           1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Numeric columns summary:\n",
      "       feed_version\n",
      "count       1.00000\n",
      "mean        1.15042\n",
      "std             NaN\n",
      "min         1.15042\n",
      "25%         1.15042\n",
      "50%         1.15042\n",
      "75%         1.15042\n",
      "max         1.15042\n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "feed_publisher_name:\n",
      "feed_publisher_name\n",
      "Transport for Cairo    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "feed_publisher_url:\n",
      "feed_publisher_url\n",
      "http://transportforcairo.com/    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "feed_lang:\n",
      "feed_lang\n",
      "en    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "   feed_publisher_name             feed_publisher_url feed_lang  \\\n",
      "0  Transport for Cairo  http://transportforcairo.com/        en   \n",
      "\n",
      "                feed_start_date                 feed_end_date  feed_version  \\\n",
      "0 1970-01-01 00:00:00.020200101 1970-01-01 00:00:00.020210101       1.15042   \n",
      "\n",
      "      city    source_file  \n",
      "0  kampala  feed_info.txt  \n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\kampala\\cleaned_feed_info.csv\n",
      "\n",
      "----------------------------------------\n",
      "Processing: kampala/frequencies.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (4696, 4)\n",
      "\n",
      "Cleaning data for kampala/frequencies.txt...\n",
      "  - Converted 'start_time' to datetime\n",
      "  - Converted 'end_time' to datetime\n",
      "  - Capped 1286 outliers in 'headway_secs'\n",
      "  ✓ Cleaned data shape: (4696, 4) → (4696, 6)\n",
      "\n",
      "--- Analysis for kampala/frequencies.txt ---\n",
      "Shape: (4696, 6)\n",
      "Memory usage: 0.97 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "object            3\n",
      "datetime64[ns]    2\n",
      "float64           1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Numeric columns summary:\n",
      "       headway_secs\n",
      "count   4696.000000\n",
      "mean     437.392462\n",
      "std      113.009781\n",
      "min      238.500000\n",
      "25%      375.000000\n",
      "50%      466.000000\n",
      "75%      466.000000\n",
      "max      602.500000\n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "trip_id:\n",
      "trip_id\n",
      "taxi_KA021_R (19-00-00)    1\n",
      "taxi_KA021_R (17-00-00)    1\n",
      "taxi_KA021_R (15-00-00)    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "city:\n",
      "city\n",
      "kampala    4696\n",
      "Name: count, dtype: int64\n",
      "\n",
      "source_file:\n",
      "source_file\n",
      "frequencies.txt    4696\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "                   trip_id          start_time            end_time  \\\n",
      "0  taxi_KA021_R (19-00-00) 2025-08-11 19:00:00 2025-08-11 23:00:00   \n",
      "1  taxi_KA021_R (17-00-00) 2025-08-11 17:00:00 2025-08-11 19:00:00   \n",
      "2  taxi_KA021_R (15-00-00) 2025-08-11 15:00:00 2025-08-11 17:00:00   \n",
      "\n",
      "   headway_secs     city      source_file  \n",
      "0         466.0  kampala  frequencies.txt  \n",
      "1         466.0  kampala  frequencies.txt  \n",
      "2         466.0  kampala  frequencies.txt  \n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\kampala\\cleaned_frequencies.csv\n",
      "\n",
      "----------------------------------------\n",
      "Processing: kampala/routes.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (397, 9)\n",
      "\n",
      "Cleaning data for kampala/routes.txt...\n",
      "  ✓ Cleaned data shape: (397, 9) → (397, 7)\n",
      "\n",
      "--- Analysis for kampala/routes.txt ---\n",
      "Shape: (397, 7)\n",
      "Memory usage: 0.14 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "object    6\n",
      "int64     1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Numeric columns summary:\n",
      "       route_type\n",
      "count       397.0\n",
      "mean          3.0\n",
      "std           0.0\n",
      "min           3.0\n",
      "25%           3.0\n",
      "50%           3.0\n",
      "75%           3.0\n",
      "max           3.0\n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "route_id:\n",
      "route_id\n",
      "bus_IC026    1\n",
      "bus_IC027    1\n",
      "bus_IC030    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "agency_id:\n",
      "agency_id\n",
      "taxi    369\n",
      "bus      28\n",
      "Name: count, dtype: int64\n",
      "\n",
      "route_short_name:\n",
      "route_short_name\n",
      "IC133    2\n",
      "IC026    2\n",
      "IC155    2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "    route_id agency_id route_short_name                 route_long_name  \\\n",
      "0  bus_IC026       bus            IC026  Mukono Taxi Park-Old Taxi Park   \n",
      "1  bus_IC027       bus            IC027  Mukono Taxi Park-Old Taxi Park   \n",
      "2  bus_IC030       bus            IC030       Old Taxi Park-Seeta Stage   \n",
      "\n",
      "   route_type     city source_file  \n",
      "0           3  kampala  routes.txt  \n",
      "1           3  kampala  routes.txt  \n",
      "2           3  kampala  routes.txt  \n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\kampala\\cleaned_routes.csv\n",
      "\n",
      "----------------------------------------\n",
      "Processing: kampala/shapes.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (87791, 4)\n",
      "\n",
      "Cleaning data for kampala/shapes.txt...\n",
      "  - Capped 11220 outliers in 'shape_pt_lat'\n",
      "  - Capped 5515 outliers in 'shape_pt_lon'\n",
      "  - Capped 6148 outliers in 'shape_pt_sequence'\n",
      "  ✓ Cleaned data shape: (87791, 4) → (87791, 6)\n",
      "\n",
      "--- Analysis for kampala/shapes.txt ---\n",
      "Shape: (87791, 6)\n",
      "Memory usage: 17.24 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "object     3\n",
      "float64    3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Numeric columns summary:\n",
      "       shape_pt_lat  shape_pt_lon  shape_pt_sequence\n",
      "count  87791.000000  87791.000000       87791.000000\n",
      "mean       0.324377     32.582079         117.421091\n",
      "std        0.047998      0.042520         102.123635\n",
      "min        0.230410     32.488460           1.000000\n",
      "25%        0.304150     32.559650          39.000000\n",
      "50%        0.326280     32.576810          85.000000\n",
      "75%        0.353310     32.607110         166.000000\n",
      "max        0.427050     32.678300         356.500000\n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "shape_id:\n",
      "shape_id\n",
      "taxi_IC105_R_shape    1052\n",
      "taxi_IC020_R_shape     982\n",
      "taxi_IC073_R_shape     959\n",
      "Name: count, dtype: int64\n",
      "\n",
      "city:\n",
      "city\n",
      "kampala    87791\n",
      "Name: count, dtype: int64\n",
      "\n",
      "source_file:\n",
      "source_file\n",
      "shapes.txt    87791\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "            shape_id  shape_pt_lat  shape_pt_lon  shape_pt_sequence     city  \\\n",
      "0  bus_IC026_R_shape       0.35915       32.6783                1.0  kampala   \n",
      "1  bus_IC026_R_shape       0.35950       32.6783                2.0  kampala   \n",
      "2  bus_IC026_R_shape       0.35978       32.6783                3.0  kampala   \n",
      "\n",
      "  source_file  \n",
      "0  shapes.txt  \n",
      "1  shapes.txt  \n",
      "2  shapes.txt  \n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\kampala\\cleaned_shapes.csv\n",
      "\n",
      "----------------------------------------\n",
      "Processing: kampala/stops.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (1242, 8)\n",
      "\n",
      "Cleaning data for kampala/stops.txt...\n",
      "  - Capped 58 outliers in 'stop_lat'\n",
      "  - Capped 49 outliers in 'stop_lon'\n",
      "  ✓ Cleaned data shape: (1242, 8) → (1242, 6)\n",
      "\n",
      "--- Analysis for kampala/stops.txt ---\n",
      "Shape: (1242, 6)\n",
      "Memory usage: 0.32 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "object     4\n",
      "float64    2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Numeric columns summary:\n",
      "          stop_lat     stop_lon\n",
      "count  1242.000000  1242.000000\n",
      "mean      0.331367    32.583350\n",
      "std       0.041646     0.040408\n",
      "min       0.233162    32.486870\n",
      "25%       0.306625    32.560385\n",
      "50%       0.328630    32.576440\n",
      "75%       0.355600    32.609395\n",
      "max       0.429062    32.682910\n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "stop_id:\n",
      "stop_id\n",
      "rivals.pinches.humidity        1\n",
      "dragonfly.shoelaces.writing    1\n",
      "deck.intruding.copiers         1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "stop_name:\n",
      "stop_name\n",
      "Namungoona                      4\n",
      "Kalerwe Taxi Park Roundabout    4\n",
      "Mambule                         3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "city:\n",
      "city\n",
      "kampala    1242\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "                       stop_id                 stop_name  stop_lat  stop_lon  \\\n",
      "0      rivals.pinches.humidity  Hotel Lavener (Lungujja)   0.31305  32.54460   \n",
      "1  dragonfly.shoelaces.writing                     Mbubi   0.30516  32.54808   \n",
      "2       deck.intruding.copiers             Free Zone Bar   0.30438  32.54613   \n",
      "\n",
      "      city source_file  \n",
      "0  kampala   stops.txt  \n",
      "1  kampala   stops.txt  \n",
      "2  kampala   stops.txt  \n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\kampala\\cleaned_stops.csv\n",
      "\n",
      "----------------------------------------\n",
      "Processing: kampala/stop_times.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (140800, 9)\n",
      "\n",
      "Cleaning data for kampala/stop_times.txt...\n",
      "  - Converted 'arrival_time' to datetime\n",
      "  - Converted 'departure_time' to datetime\n",
      "  - Capped 2512 outliers in 'stop_sequence'\n",
      "  ✓ Cleaned data shape: (140800, 9) → (140800, 7)\n",
      "\n",
      "--- Analysis for kampala/stop_times.txt ---\n",
      "Shape: (140800, 7)\n",
      "Memory usage: 38.47 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "object            4\n",
      "datetime64[ns]    2\n",
      "int64             1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Numeric columns summary:\n",
      "       stop_sequence\n",
      "count  140800.000000\n",
      "mean       20.980455\n",
      "std        15.634606\n",
      "min         1.000000\n",
      "25%         8.000000\n",
      "50%        18.000000\n",
      "75%        30.000000\n",
      "max        63.000000\n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "trip_id:\n",
      "trip_id\n",
      "taxi_IC109_R (11-00-00)    92\n",
      "taxi_IC109_R (09-00-00)    92\n",
      "taxi_IC109_R (08-00-00)    92\n",
      "Name: count, dtype: int64\n",
      "\n",
      "stop_id:\n",
      "stop_id\n",
      "veto.swimsuits.unveils    448\n",
      "loaning.learns.sleepy     384\n",
      "fidgeting.prongs.nerve    384\n",
      "Name: count, dtype: int64\n",
      "\n",
      "city:\n",
      "city\n",
      "kampala    140800\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "                  trip_id                stop_id  stop_sequence  \\\n",
      "0  bus_IC026_R (06-30-00)      doses.wider.warns              1   \n",
      "1  bus_IC026_R (06-30-00)  panting.weeps.backers              2   \n",
      "2  bus_IC026_R (06-30-00)      ramps.sorry.props              3   \n",
      "\n",
      "         arrival_time      departure_time     city     source_file  \n",
      "0 2025-08-11 06:30:00 2025-08-11 06:30:15  kampala  stop_times.txt  \n",
      "1 2025-08-11 06:31:35 2025-08-11 06:31:50  kampala  stop_times.txt  \n",
      "2 2025-08-11 06:34:50 2025-08-11 06:35:05  kampala  stop_times.txt  \n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\kampala\\cleaned_stop_times.csv\n",
      "\n",
      "----------------------------------------\n",
      "Processing: kampala/trips.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (4696, 7)\n",
      "\n",
      "Cleaning data for kampala/trips.txt...\n",
      "  ✓ Cleaned data shape: (4696, 7) → (4696, 9)\n",
      "\n",
      "--- Analysis for kampala/trips.txt ---\n",
      "Shape: (4696, 9)\n",
      "Memory usage: 2.08 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "object    7\n",
      "int64     2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Numeric columns summary:\n",
      "       direction_id  wheelchair_accessible\n",
      "count   4696.000000                 4696.0\n",
      "mean       0.482112                    0.0\n",
      "std        0.499733                    0.0\n",
      "min        0.000000                    0.0\n",
      "25%        0.000000                    0.0\n",
      "50%        0.000000                    0.0\n",
      "75%        1.000000                    0.0\n",
      "max        1.000000                    0.0\n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "route_id:\n",
      "route_id\n",
      "taxi_IC030    16\n",
      "taxi_IC127    16\n",
      "taxi_KA047    16\n",
      "Name: count, dtype: int64\n",
      "\n",
      "service_id:\n",
      "service_id\n",
      "Ground_Daily    4696\n",
      "Name: count, dtype: int64\n",
      "\n",
      "trip_id:\n",
      "trip_id\n",
      "taxi_KA021_R (19-00-00)    1\n",
      "taxi_KA021_R (17-00-00)    1\n",
      "taxi_KA021_R (15-00-00)    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "     route_id    service_id                  trip_id  \\\n",
      "0  taxi_KA021  Ground_Daily  taxi_KA021_R (19-00-00)   \n",
      "1  taxi_KA021  Ground_Daily  taxi_KA021_R (17-00-00)   \n",
      "2  taxi_KA021  Ground_Daily  taxi_KA021_R (15-00-00)   \n",
      "\n",
      "                   trip_short_name  direction_id            shape_id  \\\n",
      "0  Nateete Taxi Park-Old Taxi Park             1  taxi_KA021_R_shape   \n",
      "1  Nateete Taxi Park-Old Taxi Park             1  taxi_KA021_R_shape   \n",
      "2  Nateete Taxi Park-Old Taxi Park             1  taxi_KA021_R_shape   \n",
      "\n",
      "   wheelchair_accessible     city source_file  \n",
      "0                      0  kampala   trips.txt  \n",
      "1                      0  kampala   trips.txt  \n",
      "2                      0  kampala   trips.txt  \n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\kampala\\cleaned_trips.csv\n",
      "\n",
      "----------------------------------------\n",
      "COMBINING DATA FOR KAMPALA\n",
      "----------------------------------------\n",
      "⚠ Too few common columns to combine kampala data\n",
      "\n",
      "============================================================\n",
      "PROCESSING CITY: KUMASI\n",
      "============================================================\n",
      "Found 8 data files in kumasi: ['agency.txt', 'calendar.txt', 'routes.txt', 'routes_enriched.txt', 'shapes.txt', 'stops.txt', 'stop_times.txt', 'trips.txt']\n",
      "\n",
      "----------------------------------------\n",
      "Processing: kumasi/agency.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (1, 6)\n",
      "\n",
      "Cleaning data for kumasi/agency.txt...\n",
      "  - Converted 'agency_timezone' to datetime\n",
      "  ✓ Cleaned data shape: (1, 6) → (1, 7)\n",
      "\n",
      "--- Analysis for kumasi/agency.txt ---\n",
      "Shape: (1, 7)\n",
      "Memory usage: 0.00 MB\n",
      "\n",
      "Missing values:\n",
      "agency_timezone    1\n",
      "dtype: int64\n",
      "\n",
      "Data types:\n",
      "int64             3\n",
      "object            3\n",
      "datetime64[ns]    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Numeric columns summary:\n",
      "       agency_id  agency_name  agency_url\n",
      "count        1.0          1.0         1.0\n",
      "mean         1.0          1.0         1.0\n",
      "std          NaN          NaN         NaN\n",
      "min          1.0          1.0         1.0\n",
      "25%          1.0          1.0         1.0\n",
      "50%          1.0          1.0         1.0\n",
      "75%          1.0          1.0         1.0\n",
      "max          1.0          1.0         1.0\n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "agency_language:\n",
      "agency_language\n",
      "en    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "city:\n",
      "city\n",
      "kumasi    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "source_file:\n",
      "source_file\n",
      "agency.txt    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "   agency_id  agency_name  agency_url agency_timezone agency_language    city  \\\n",
      "0          1            1           1             NaT              en  kumasi   \n",
      "\n",
      "  source_file  \n",
      "0  agency.txt  \n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\kumasi\\cleaned_agency.csv\n",
      "\n",
      "----------------------------------------\n",
      "Processing: kumasi/calendar.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (1, 10)\n",
      "\n",
      "Cleaning data for kumasi/calendar.txt...\n",
      "  - Converted 'start_date' to datetime\n",
      "  - Converted 'end_date' to datetime\n",
      "  ✓ Cleaned data shape: (1, 10) → (1, 12)\n",
      "\n",
      "--- Analysis for kumasi/calendar.txt ---\n",
      "Shape: (1, 12)\n",
      "Memory usage: 0.00 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "int64             8\n",
      "datetime64[ns]    2\n",
      "object            2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Numeric columns summary:\n",
      "       service_id  monday  tuesday  wednesday  thursday  friday  saturday  \\\n",
      "count         1.0     1.0      1.0        1.0       1.0     1.0       1.0   \n",
      "mean          1.0     1.0      1.0        1.0       1.0     1.0       0.0   \n",
      "std           NaN     NaN      NaN        NaN       NaN     NaN       NaN   \n",
      "min           1.0     1.0      1.0        1.0       1.0     1.0       0.0   \n",
      "25%           1.0     1.0      1.0        1.0       1.0     1.0       0.0   \n",
      "50%           1.0     1.0      1.0        1.0       1.0     1.0       0.0   \n",
      "75%           1.0     1.0      1.0        1.0       1.0     1.0       0.0   \n",
      "max           1.0     1.0      1.0        1.0       1.0     1.0       0.0   \n",
      "\n",
      "       sunday  \n",
      "count     1.0  \n",
      "mean      0.0  \n",
      "std       NaN  \n",
      "min       0.0  \n",
      "25%       0.0  \n",
      "50%       0.0  \n",
      "75%       0.0  \n",
      "max       0.0  \n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "city:\n",
      "city\n",
      "kumasi    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "source_file:\n",
      "source_file\n",
      "calendar.txt    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "   service_id  monday  tuesday  wednesday  thursday  friday  saturday  sunday  \\\n",
      "0           1       1        1          1         1       1         0       0   \n",
      "\n",
      "                     start_date                      end_date    city  \\\n",
      "0 1970-01-01 00:00:00.020000101 1970-01-01 00:00:00.020381231  kumasi   \n",
      "\n",
      "    source_file  \n",
      "0  calendar.txt  \n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\kumasi\\cleaned_calendar.csv\n",
      "\n",
      "----------------------------------------\n",
      "Processing: kumasi/routes.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (668, 9)\n",
      "\n",
      "Cleaning data for kumasi/routes.txt...\n",
      "  ✓ Cleaned data shape: (668, 9) → (668, 5)\n",
      "\n",
      "--- Analysis for kumasi/routes.txt ---\n",
      "Shape: (668, 5)\n",
      "Memory usage: 0.12 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "object    3\n",
      "int64     2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Numeric columns summary:\n",
      "          route_id  route_type\n",
      "count   668.000000       668.0\n",
      "mean    635.667665         3.0\n",
      "std     318.228936         0.0\n",
      "min       9.000000         3.0\n",
      "25%     386.750000         3.0\n",
      "50%     662.500000         3.0\n",
      "75%     866.250000         3.0\n",
      "max    1223.000000         3.0\n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "route_short_name:\n",
      "route_short_name\n",
      "3304_AB    1\n",
      "3304_BA    1\n",
      "3305_AB    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "city:\n",
      "city\n",
      "kumasi    668\n",
      "Name: count, dtype: int64\n",
      "\n",
      "source_file:\n",
      "source_file\n",
      "routes.txt    668\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "   route_id route_short_name  route_type    city source_file\n",
      "0         9          3304_AB           3  kumasi  routes.txt\n",
      "1        10          3304_BA           3  kumasi  routes.txt\n",
      "2        11          3305_AB           3  kumasi  routes.txt\n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\kumasi\\cleaned_routes.csv\n",
      "\n",
      "----------------------------------------\n",
      "Processing: kumasi/routes_enriched.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (668, 9)\n",
      "\n",
      "Cleaning data for kumasi/routes_enriched.txt...\n",
      "  ✓ Cleaned data shape: (668, 9) → (668, 6)\n",
      "\n",
      "--- Analysis for kumasi/routes_enriched.txt ---\n",
      "Shape: (668, 6)\n",
      "Memory usage: 0.17 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "object    4\n",
      "int64     2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Numeric columns summary:\n",
      "          route_id  route_type\n",
      "count   668.000000       668.0\n",
      "mean    635.667665         3.0\n",
      "std     318.228936         0.0\n",
      "min       9.000000         3.0\n",
      "25%     386.750000         3.0\n",
      "50%     662.500000         3.0\n",
      "75%     866.250000         3.0\n",
      "max    1223.000000         3.0\n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "route_short_name:\n",
      "route_short_name\n",
      "3304_AB    1\n",
      "3304_BA    1\n",
      "3305_AB    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "route_long_name:\n",
      "route_long_name\n",
      "Suame Roundabout - Abuakwa Terminal    6\n",
      "Abuakwa Terminal - Suame Roundabout    6\n",
      "Santasi Station - Ahenema Kokobin      5\n",
      "Name: count, dtype: int64\n",
      "\n",
      "city:\n",
      "city\n",
      "kumasi    668\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "   route_id route_short_name                 route_long_name  route_type  \\\n",
      "0         9          3304_AB  Nhyieaso Station - Kumasi Mall           3   \n",
      "1        10          3304_BA  Kumasi Mall - Nhyieaso Station           3   \n",
      "2        11          3305_AB       Nhyieaso Station - Sobolo           3   \n",
      "\n",
      "     city          source_file  \n",
      "0  kumasi  routes_enriched.txt  \n",
      "1  kumasi  routes_enriched.txt  \n",
      "2  kumasi  routes_enriched.txt  \n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\kumasi\\cleaned_routes_enriched.csv\n",
      "\n",
      "----------------------------------------\n",
      "Processing: kumasi/shapes.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (114512, 5)\n",
      "\n",
      "Cleaning data for kumasi/shapes.txt...\n",
      "  - Capped 6647 outliers in 'shape_pt_lat'\n",
      "  - Capped 7324 outliers in 'shape_pt_lon'\n",
      "  - Capped 2861 outliers in 'shape_pt_sequence'\n",
      "  - Capped 5832 outliers in 'shape_dist_traveled'\n",
      "  ✓ Cleaned data shape: (114512, 5) → (114512, 7)\n",
      "\n",
      "--- Analysis for kumasi/shapes.txt ---\n",
      "Shape: (114512, 7)\n",
      "Memory usage: 16.82 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "float64    3\n",
      "int64      2\n",
      "object     2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Numeric columns summary:\n",
      "            shape_id   shape_pt_lat   shape_pt_lon  shape_pt_sequence  \\\n",
      "count  114512.000000  114512.000000  114512.000000      114512.000000   \n",
      "mean      353.011283       6.691976      -1.627725         115.494306   \n",
      "std       185.174772       0.035806       0.036222          90.841291   \n",
      "min         1.000000       6.609175      -1.707839           0.000000   \n",
      "25%       209.000000       6.670149      -1.647421          43.000000   \n",
      "50%       363.000000       6.694918      -1.627751          93.000000   \n",
      "75%       510.000000       6.710799      -1.607142         169.000000   \n",
      "max       668.000000       6.771773      -1.546724         358.000000   \n",
      "\n",
      "       shape_dist_traveled  \n",
      "count        114512.000000  \n",
      "mean              0.026857  \n",
      "std               0.019970  \n",
      "min               0.000000  \n",
      "25%               0.011500  \n",
      "50%               0.021452  \n",
      "75%               0.037120  \n",
      "max               0.075550  \n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "city:\n",
      "city\n",
      "kumasi    114512\n",
      "Name: count, dtype: int64\n",
      "\n",
      "source_file:\n",
      "source_file\n",
      "shapes.txt    114512\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "   shape_id  shape_pt_lat  shape_pt_lon  shape_pt_sequence  \\\n",
      "0         1      6.694935     -1.621182                  0   \n",
      "1         1      6.694179     -1.620888                  1   \n",
      "2         1      6.693118     -1.620667                  2   \n",
      "\n",
      "   shape_dist_traveled    city source_file  \n",
      "0             0.000000  kumasi  shapes.txt  \n",
      "1             0.055738  kumasi  shapes.txt  \n",
      "2             0.074473  kumasi  shapes.txt  \n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\kumasi\\cleaned_shapes.csv\n",
      "\n",
      "----------------------------------------\n",
      "Processing: kumasi/stops.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (704, 10)\n",
      "\n",
      "Cleaning data for kumasi/stops.txt...\n",
      "  - Capped 15 outliers in 'stop_lat'\n",
      "  - Capped 38 outliers in 'stop_lon'\n",
      "  ✓ Cleaned data shape: (704, 10) → (704, 7)\n",
      "\n",
      "--- Analysis for kumasi/stops.txt ---\n",
      "Shape: (704, 7)\n",
      "Memory usage: 0.14 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "object     3\n",
      "int64      2\n",
      "float64    2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Numeric columns summary:\n",
      "          stop_id    stop_lat    stop_lon  location_type\n",
      "count  704.000000  704.000000  704.000000          704.0\n",
      "mean   352.500000    6.692918   -1.617378            0.0\n",
      "std    203.371581    0.031443    0.037354            0.0\n",
      "min      1.000000    6.616925   -1.698806            0.0\n",
      "25%    176.750000    6.673199   -1.637424            0.0\n",
      "50%    352.500000    6.695395   -1.619180            0.0\n",
      "75%    528.250000    6.710715   -1.596502            0.0\n",
      "max    704.000000    6.766989   -1.535119            0.0\n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "stop_name:\n",
      "stop_name\n",
      "Wide Ring    15\n",
      "Sofoline     13\n",
      "Nsenie       11\n",
      "Name: count, dtype: int64\n",
      "\n",
      "city:\n",
      "city\n",
      "kumasi    704\n",
      "Name: count, dtype: int64\n",
      "\n",
      "source_file:\n",
      "source_file\n",
      "stops.txt    704\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "   stop_id  stop_name  stop_lat  stop_lon  location_type    city source_file\n",
      "0        1  N10 North  6.766989 -1.645674              0  kumasi   stops.txt\n",
      "1        2   Ampabame  6.745209 -1.671310              0  kumasi   stops.txt\n",
      "2        3   Ampabame  6.745209 -1.671310              0  kumasi   stops.txt\n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\kumasi\\cleaned_stops.csv\n",
      "\n",
      "----------------------------------------\n",
      "Processing: kumasi/stop_times.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (2519, 9)\n",
      "\n",
      "Cleaning data for kumasi/stop_times.txt...\n",
      "  - Converted 'arrival_time' to datetime\n",
      "  - Converted 'departure_time' to datetime\n",
      "  - Capped 6 outliers in 'stop_sequence'\n",
      "  - Capped 68 outliers in 'shape_dist_traveled'\n",
      "  ✓ Cleaned data shape: (2519, 9) → (2519, 8)\n",
      "\n",
      "--- Analysis for kumasi/stop_times.txt ---\n",
      "Shape: (2519, 8)\n",
      "Memory usage: 0.40 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "int64             2\n",
      "datetime64[ns]    2\n",
      "float64           2\n",
      "object            2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Numeric columns summary:\n",
      "           trip_id      stop_id  stop_sequence  shape_dist_traveled\n",
      "count  2519.000000  2519.000000    2519.000000          2519.000000\n",
      "mean    344.040095   177.218738       2.716157             4.413193\n",
      "std     187.739281   146.461274       1.587850             4.009972\n",
      "min       1.000000     1.000000       1.000000             0.001481\n",
      "25%     189.000000    55.000000       1.000000             0.931099\n",
      "50%     347.000000   123.000000       2.000000             3.609454\n",
      "75%     502.000000   280.000000       4.000000             6.657056\n",
      "max     668.000000   535.000000       8.500000            15.245992\n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "city:\n",
      "city\n",
      "kumasi    2519\n",
      "Name: count, dtype: int64\n",
      "\n",
      "source_file:\n",
      "source_file\n",
      "stop_times.txt    2519\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "   trip_id        arrival_time      departure_time  stop_id  stop_sequence  \\\n",
      "0        1 2025-08-11 00:00:04 2025-08-11 00:01:04      146            1.0   \n",
      "1        1 2025-08-11 00:00:03 2025-08-11 00:01:03      179            2.0   \n",
      "2        1 2025-08-11 00:08:57 2025-08-11 00:09:57      173            3.0   \n",
      "\n",
      "   shape_dist_traveled    city     source_file  \n",
      "0             0.032981  kumasi  stop_times.txt  \n",
      "1             0.352566  kumasi  stop_times.txt  \n",
      "2             3.729672  kumasi  stop_times.txt  \n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\kumasi\\cleaned_stop_times.csv\n",
      "\n",
      "----------------------------------------\n",
      "Processing: kumasi/trips.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (668, 8)\n",
      "\n",
      "Cleaning data for kumasi/trips.txt...\n",
      "  ✓ Cleaned data shape: (668, 8) → (668, 7)\n",
      "\n",
      "--- Analysis for kumasi/trips.txt ---\n",
      "Shape: (668, 7)\n",
      "Memory usage: 0.10 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "int64     5\n",
      "object    2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Numeric columns summary:\n",
      "          route_id  service_id     trip_id  direction_id    shape_id\n",
      "count   668.000000       668.0  668.000000         668.0  668.000000\n",
      "mean    635.667665         1.0  334.500000           0.0  334.500000\n",
      "std     318.228936         0.0  192.979273           0.0  192.979273\n",
      "min       9.000000         1.0    1.000000           0.0    1.000000\n",
      "25%     386.750000         1.0  167.750000           0.0  167.750000\n",
      "50%     662.500000         1.0  334.500000           0.0  334.500000\n",
      "75%     866.250000         1.0  501.250000           0.0  501.250000\n",
      "max    1223.000000         1.0  668.000000           0.0  668.000000\n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "city:\n",
      "city\n",
      "kumasi    668\n",
      "Name: count, dtype: int64\n",
      "\n",
      "source_file:\n",
      "source_file\n",
      "trips.txt    668\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "   route_id  service_id  trip_id  direction_id  shape_id    city source_file\n",
      "0         9           1        1             0         1  kumasi   trips.txt\n",
      "1        10           1        2             0         2  kumasi   trips.txt\n",
      "2        11           1        3             0         3  kumasi   trips.txt\n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\kumasi\\cleaned_trips.csv\n",
      "\n",
      "----------------------------------------\n",
      "COMBINING DATA FOR KUMASI\n",
      "----------------------------------------\n",
      "⚠ Too few common columns to combine kumasi data\n",
      "\n",
      "============================================================\n",
      "PROCESSING CITY: LAGOS\n",
      "============================================================\n",
      "Found 10 data files in lagos: ['agency.txt', 'calendar.txt', 'calendar_dates.txt', 'feed_info.txt', 'frequencies.txt', 'routes.txt', 'shapes.txt', 'stops.txt', 'stop_times.txt', 'trips.txt']\n",
      "\n",
      "----------------------------------------\n",
      "Processing: lagos/agency.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (1, 8)\n",
      "\n",
      "Cleaning data for lagos/agency.txt...\n",
      "  - Converted 'agency_timezone' to datetime\n",
      "  ✓ Cleaned data shape: (1, 8) → (1, 9)\n",
      "\n",
      "--- Analysis for lagos/agency.txt ---\n",
      "Shape: (1, 9)\n",
      "Memory usage: 0.00 MB\n",
      "\n",
      "Missing values:\n",
      "agency_timezone    1\n",
      "dtype: int64\n",
      "\n",
      "Data types:\n",
      "object            7\n",
      "datetime64[ns]    1\n",
      "int64             1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Numeric columns summary:\n",
      "       agency_phone\n",
      "count  1.000000e+00\n",
      "mean   2.349100e+12\n",
      "std             NaN\n",
      "min    2.349100e+12\n",
      "25%    2.349100e+12\n",
      "50%    2.349100e+12\n",
      "75%    2.349100e+12\n",
      "max    2.349100e+12\n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "agency_id:\n",
      "agency_id\n",
      "LAMATA    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "agency_name:\n",
      "agency_name\n",
      "Lagos Metropolitan Area Transport Authority    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "agency_url:\n",
      "agency_url\n",
      "https://www.lamata-ng.com/    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "  agency_id                                  agency_name  \\\n",
      "0    LAMATA  Lagos Metropolitan Area Transport Authority   \n",
      "\n",
      "                   agency_url agency_timezone agency_lang   agency_phone  \\\n",
      "0  https://www.lamata-ng.com/             NaT          En  2349099526282   \n",
      "\n",
      "         agency_email   city source_file  \n",
      "0  info@lamata-ng.com  lagos  agency.txt  \n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\lagos\\cleaned_agency.csv\n",
      "\n",
      "----------------------------------------\n",
      "Processing: lagos/calendar.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (1, 10)\n",
      "\n",
      "Cleaning data for lagos/calendar.txt...\n",
      "  - Converted 'start_date' to datetime\n",
      "  - Converted 'end_date' to datetime\n",
      "  ✓ Cleaned data shape: (1, 10) → (1, 12)\n",
      "\n",
      "--- Analysis for lagos/calendar.txt ---\n",
      "Shape: (1, 12)\n",
      "Memory usage: 0.00 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "int64             7\n",
      "object            3\n",
      "datetime64[ns]    2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Numeric columns summary:\n",
      "       monday  tuesday  wednesday  thursday  friday  saturday  sunday\n",
      "count     1.0      1.0        1.0       1.0     1.0       1.0     1.0\n",
      "mean      1.0      1.0        1.0       1.0     1.0       1.0     1.0\n",
      "std       NaN      NaN        NaN       NaN     NaN       NaN     NaN\n",
      "min       1.0      1.0        1.0       1.0     1.0       1.0     1.0\n",
      "25%       1.0      1.0        1.0       1.0     1.0       1.0     1.0\n",
      "50%       1.0      1.0        1.0       1.0     1.0       1.0     1.0\n",
      "75%       1.0      1.0        1.0       1.0     1.0       1.0     1.0\n",
      "max       1.0      1.0        1.0       1.0     1.0       1.0     1.0\n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "service_id:\n",
      "service_id\n",
      "MON-SUN    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "city:\n",
      "city\n",
      "lagos    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "source_file:\n",
      "source_file\n",
      "calendar.txt    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "  service_id  monday  tuesday  wednesday  thursday  friday  saturday  sunday  \\\n",
      "0    MON-SUN       1        1          1         1       1         1       1   \n",
      "\n",
      "                     start_date                      end_date   city  \\\n",
      "0 1970-01-01 00:00:00.020230101 1970-01-01 00:00:00.020241230  lagos   \n",
      "\n",
      "    source_file  \n",
      "0  calendar.txt  \n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\lagos\\cleaned_calendar.csv\n",
      "\n",
      "----------------------------------------\n",
      "Processing: lagos/calendar_dates.txt\n",
      "----------------------------------------\n",
      "  ❌ Could not load trotro\\trotrolive-datasets\\lagos\\calendar_dates.txt with any separator\n",
      "\n",
      "----------------------------------------\n",
      "Processing: lagos/feed_info.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (1, 7)\n",
      "\n",
      "Cleaning data for lagos/feed_info.txt...\n",
      "  - Converted 'feed_start_date' to datetime\n",
      "  - Converted 'feed_end_date' to datetime\n",
      "  ✓ Cleaned data shape: (1, 7) → (1, 9)\n",
      "\n",
      "--- Analysis for lagos/feed_info.txt ---\n",
      "Shape: (1, 9)\n",
      "Memory usage: 0.00 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "object            6\n",
      "datetime64[ns]    2\n",
      "float64           1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Numeric columns summary:\n",
      "       feed_version\n",
      "count           1.0\n",
      "mean            2.0\n",
      "std             NaN\n",
      "min             2.0\n",
      "25%             2.0\n",
      "50%             2.0\n",
      "75%             2.0\n",
      "max             2.0\n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "feed_publisher_name:\n",
      "feed_publisher_name\n",
      "CPCS    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "feed_lang:\n",
      "feed_lang\n",
      "en    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "feed_publisher_url:\n",
      "feed_publisher_url\n",
      "https://www.cpcs.ca    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "  feed_publisher_name               feed_start_date  feed_version  \\\n",
      "0                CPCS 1970-01-01 00:00:00.020220101           2.0   \n",
      "\n",
      "                  feed_end_date feed_lang   feed_publisher_url  \\\n",
      "0 1970-01-01 00:00:00.020241230        en  https://www.cpcs.ca   \n",
      "\n",
      "  feed_contact_email   city    source_file  \n",
      "0       info@cpcs.ca  lagos  feed_info.txt  \n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\lagos\\cleaned_feed_info.csv\n",
      "\n",
      "----------------------------------------\n",
      "Processing: lagos/frequencies.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (8, 5)\n",
      "\n",
      "Cleaning data for lagos/frequencies.txt...\n",
      "  - Converted 'start_time' to datetime\n",
      "  - Converted 'end_time' to datetime\n",
      "  ✓ Cleaned data shape: (8, 5) → (8, 6)\n",
      "\n",
      "--- Analysis for lagos/frequencies.txt ---\n",
      "Shape: (8, 6)\n",
      "Memory usage: 0.00 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "object            3\n",
      "datetime64[ns]    2\n",
      "int64             1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Numeric columns summary:\n",
      "       headway_secs\n",
      "count           8.0\n",
      "mean          600.0\n",
      "std             0.0\n",
      "min           600.0\n",
      "25%           600.0\n",
      "50%           600.0\n",
      "75%           600.0\n",
      "max           600.0\n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "trip_id:\n",
      "trip_id\n",
      "1107D1136    1\n",
      "1107D111     1\n",
      "1107D112     1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "city:\n",
      "city\n",
      "lagos    8\n",
      "Name: count, dtype: int64\n",
      "\n",
      "source_file:\n",
      "source_file\n",
      "frequencies.txt    8\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "     trip_id          start_time            end_time  headway_secs   city  \\\n",
      "0  1107D1136 2025-08-11 05:00:00 2025-08-11 23:00:00           600  lagos   \n",
      "1   1107D111 2025-08-11 05:00:00 2025-08-11 23:00:00           600  lagos   \n",
      "2   1107D112 2025-08-11 05:00:00 2025-08-11 23:00:00           600  lagos   \n",
      "\n",
      "       source_file  \n",
      "0  frequencies.txt  \n",
      "1  frequencies.txt  \n",
      "2  frequencies.txt  \n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\lagos\\cleaned_frequencies.csv\n",
      "\n",
      "----------------------------------------\n",
      "Processing: lagos/routes.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (8, 10)\n",
      "\n",
      "Cleaning data for lagos/routes.txt...\n",
      "  ✓ Cleaned data shape: (8, 10) → (8, 7)\n",
      "\n",
      "--- Analysis for lagos/routes.txt ---\n",
      "Shape: (8, 7)\n",
      "Memory usage: 0.00 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "object    5\n",
      "int64     2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Numeric columns summary:\n",
      "       route_id  route_type\n",
      "count   8.00000         8.0\n",
      "mean   15.50000         3.0\n",
      "std     2.44949         0.0\n",
      "min    12.00000         3.0\n",
      "25%    13.75000         3.0\n",
      "50%    15.50000         3.0\n",
      "75%    17.25000         3.0\n",
      "max    19.00000         3.0\n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "agency_id:\n",
      "agency_id\n",
      "LAMATA    8\n",
      "Name: count, dtype: int64\n",
      "\n",
      "route_short_name:\n",
      "route_short_name\n",
      "1       1\n",
      "QBC2    1\n",
      "QBC3    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "route_long_name:\n",
      "route_long_name\n",
      "Ojuelegba – Idi Araba – Ilasamaja    1\n",
      "Iju Ishaga – Abule Egba              1\n",
      "Igando – Iyana Iba                   1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "   route_id agency_id route_short_name                    route_long_name  \\\n",
      "0        12    LAMATA                1  Ojuelegba – Idi Araba – Ilasamaja   \n",
      "1        13    LAMATA             QBC2            Iju Ishaga – Abule Egba   \n",
      "2        14    LAMATA             QBC3                 Igando – Iyana Iba   \n",
      "\n",
      "   route_type   city source_file  \n",
      "0           3  lagos  routes.txt  \n",
      "1           3  lagos  routes.txt  \n",
      "2           3  lagos  routes.txt  \n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\lagos\\cleaned_routes.csv\n",
      "\n",
      "----------------------------------------\n",
      "Processing: lagos/shapes.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (3774, 5)\n",
      "\n",
      "Cleaning data for lagos/shapes.txt...\n",
      "  - Capped 6 outliers in 'shape_pt_sequence'\n",
      "  - Capped 54 outliers in 'shape_dist_traveled'\n",
      "  ✓ Cleaned data shape: (3774, 5) → (3774, 7)\n",
      "\n",
      "--- Analysis for lagos/shapes.txt ---\n",
      "Shape: (3774, 7)\n",
      "Memory usage: 0.55 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "float64    4\n",
      "object     2\n",
      "int64      1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Numeric columns summary:\n",
      "          shape_id  shape_pt_lat  shape_pt_lon  shape_pt_sequence  \\\n",
      "count  3774.000000   3774.000000   3774.000000        3774.000000   \n",
      "mean      4.206942      6.558849      3.307616         300.487215   \n",
      "std       1.873651      0.052121      0.068223         224.750481   \n",
      "min       1.000000      6.461217      3.199097           1.000000   \n",
      "25%       2.000000      6.512456      3.241138         118.250000   \n",
      "50%       4.000000      6.547752      3.324156         250.000000   \n",
      "75%       5.000000      6.601817      3.361820         447.000000   \n",
      "max       8.000000      6.667942      3.400698         940.125000   \n",
      "\n",
      "       shape_dist_traveled  \n",
      "count          3774.000000  \n",
      "mean           3738.130470  \n",
      "std            2882.498972  \n",
      "min               0.000000  \n",
      "25%            1446.096146  \n",
      "50%            3078.554388  \n",
      "75%            5512.294277  \n",
      "max           11611.591472  \n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "city:\n",
      "city\n",
      "lagos    3774\n",
      "Name: count, dtype: int64\n",
      "\n",
      "source_file:\n",
      "source_file\n",
      "shapes.txt    3774\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "   shape_id  shape_pt_lat  shape_pt_lon  shape_pt_sequence  \\\n",
      "0         1      6.510493      3.363141                1.0   \n",
      "1         1      6.510382      3.363046                2.0   \n",
      "2         1      6.510302      3.362994                3.0   \n",
      "\n",
      "   shape_dist_traveled   city source_file  \n",
      "0             0.000000  lagos  shapes.txt  \n",
      "1            16.214464  lagos  shapes.txt  \n",
      "2            26.746660  lagos  shapes.txt  \n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\lagos\\cleaned_shapes.csv\n",
      "\n",
      "----------------------------------------\n",
      "Processing: lagos/stops.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (171, 11)\n",
      "\n",
      "Cleaning data for lagos/stops.txt...\n",
      "  ✓ Cleaned data shape: (171, 11) → (171, 12)\n",
      "\n",
      "--- Analysis for lagos/stops.txt ---\n",
      "Shape: (171, 12)\n",
      "Memory usage: 0.07 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "object     6\n",
      "float64    4\n",
      "int64      2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Numeric columns summary:\n",
      "       esrignss_latitude  esrignss_longitude     stop_id    shape_id  \\\n",
      "count         171.000000          171.000000  171.000000  171.000000   \n",
      "mean            6.560934            3.315509  222.409357  222.409357   \n",
      "std             0.050523            0.062443   54.082363   54.082363   \n",
      "min             6.461283            3.199118  121.000000  121.000000   \n",
      "25%             6.512644            3.258051  179.000000  179.000000   \n",
      "50%             6.551365            3.336172  225.000000  225.000000   \n",
      "75%             6.601707            3.362109  268.500000  268.500000   \n",
      "max             6.668228            3.400652  311.000000  311.000000   \n",
      "\n",
      "         stop_lat    stop_lon  \n",
      "count  171.000000  171.000000  \n",
      "mean     6.560934    3.315513  \n",
      "std      0.050526    0.062446  \n",
      "min      6.461283    3.199118  \n",
      "25%      6.512644    3.258051  \n",
      "50%      6.551365    3.336172  \n",
      "75%      6.601708    3.362109  \n",
      "max      6.668228    3.400696  \n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "name:\n",
      "name\n",
      "ANTHONY         7\n",
      "IYANA SCHOOL    3\n",
      "OSHODI          3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "globalid:\n",
      "globalid\n",
      "{89856929-D9D4-42E3-837D-8DFDA0AFE1EE}    1\n",
      "{D3C32A35-3691-450D-81E0-1E00D026B904}    1\n",
      "{67D5C21A-E684-415C-BB70-4A01E9300715}    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "corridorname:\n",
      "corridorname\n",
      "Iyana Ipaja-Ayobo     32\n",
      "Iyana Iba-Igando      30\n",
      "Yaba-Lawanson-Cele    29\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "   esrignss_latitude  esrignss_longitude       name  \\\n",
      "0           6.510450            3.363065  OJUELEGBA   \n",
      "1           6.510343            3.361760    AYILARA   \n",
      "2           6.511603            3.359387     ISHAGA   \n",
      "\n",
      "                                 globalid                    corridorname  \\\n",
      "0  {89856929-D9D4-42E3-837D-8DFDA0AFE1EE}  Ojuelegba-Idi Araba -Ilasamaja   \n",
      "1  {D3C32A35-3691-450D-81E0-1E00D026B904}  Ojuelegba-Idi Araba -Ilasamaja   \n",
      "2  {67D5C21A-E684-415C-BB70-4A01E9300715}              Yaba-Lawanson-Cele   \n",
      "\n",
      "   stop_id  stop_name  shape_id  stop_lat  stop_lon   city source_file  \n",
      "0      121  Ojuelegba       121  6.510450  3.363065  lagos   stops.txt  \n",
      "1      122    Ayilara       122  6.510343  3.361760  lagos   stops.txt  \n",
      "2      123     Ishaga       123  6.511603  3.359387  lagos   stops.txt  \n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\lagos\\cleaned_stops.csv\n",
      "\n",
      "----------------------------------------\n",
      "Processing: lagos/stop_times.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (171, 7)\n",
      "\n",
      "Cleaning data for lagos/stop_times.txt...\n",
      "  - Converted 'arrival_time' to datetime\n",
      "  - Converted 'departure_time' to datetime\n",
      "  - Converted 'timepoint' to datetime\n",
      "  ✓ Cleaned data shape: (171, 7) → (171, 8)\n",
      "\n",
      "--- Analysis for lagos/stop_times.txt ---\n",
      "Shape: (171, 8)\n",
      "Memory usage: 0.04 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "object            3\n",
      "datetime64[ns]    3\n",
      "int64             2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Numeric columns summary:\n",
      "          stop_id  stop_sequence\n",
      "count  171.000000     171.000000\n",
      "mean   222.409357      24.719298\n",
      "std     54.082363      15.314838\n",
      "min    121.000000       1.000000\n",
      "25%    179.000000      11.000000\n",
      "50%    225.000000      24.000000\n",
      "75%    268.500000      38.000000\n",
      "max    311.000000      55.000000\n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "trip_id:\n",
      "trip_id\n",
      "1107D115    32\n",
      "1107D112    30\n",
      "1107D116    29\n",
      "Name: count, dtype: int64\n",
      "\n",
      "city:\n",
      "city\n",
      "lagos    171\n",
      "Name: count, dtype: int64\n",
      "\n",
      "source_file:\n",
      "source_file\n",
      "stop_times.txt    171\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "     trip_id        arrival_time      departure_time  stop_id  stop_sequence  \\\n",
      "0  1107D1136 2025-08-11 06:00:00 2025-08-11 06:00:20      121              1   \n",
      "1  1107D1136 2025-08-11 06:00:34 2025-08-11 06:00:54      122              2   \n",
      "2   1107D116 2025-08-11 06:01:10 2025-08-11 06:01:30      123              3   \n",
      "\n",
      "   timepoint   city     source_file  \n",
      "0 1970-01-01  lagos  stop_times.txt  \n",
      "1 1970-01-01  lagos  stop_times.txt  \n",
      "2 1970-01-01  lagos  stop_times.txt  \n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\lagos\\cleaned_stop_times.csv\n",
      "\n",
      "----------------------------------------\n",
      "Processing: lagos/trips.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (8, 6)\n",
      "\n",
      "Cleaning data for lagos/trips.txt...\n",
      "  ✓ Cleaned data shape: (8, 6) → (8, 8)\n",
      "\n",
      "--- Analysis for lagos/trips.txt ---\n",
      "Shape: (8, 8)\n",
      "Memory usage: 0.00 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "object    5\n",
      "int64     3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Numeric columns summary:\n",
      "       route_id  direction_id  shape_id\n",
      "count   8.00000           8.0   8.00000\n",
      "mean   15.50000           1.0   4.50000\n",
      "std     2.44949           0.0   2.44949\n",
      "min    12.00000           1.0   1.00000\n",
      "25%    13.75000           1.0   2.75000\n",
      "50%    15.50000           1.0   4.50000\n",
      "75%    17.25000           1.0   6.25000\n",
      "max    19.00000           1.0   8.00000\n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "service_id:\n",
      "service_id\n",
      "MON-SUN    8\n",
      "Name: count, dtype: int64\n",
      "\n",
      "trip_id:\n",
      "trip_id\n",
      "1107D1136    1\n",
      "1107D111     1\n",
      "1107D112     1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "trip_headsign:\n",
      "trip_headsign\n",
      "Ojuelegba     1\n",
      "Iju Ishaga    1\n",
      "Igando        1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "   route_id service_id    trip_id trip_headsign  direction_id  shape_id  \\\n",
      "0        12    MON-SUN  1107D1136     Ojuelegba             1         1   \n",
      "1        13    MON-SUN   1107D111    Iju Ishaga             1         3   \n",
      "2        14    MON-SUN   1107D112        Igando             1         5   \n",
      "\n",
      "    city source_file  \n",
      "0  lagos   trips.txt  \n",
      "1  lagos   trips.txt  \n",
      "2  lagos   trips.txt  \n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\lagos\\cleaned_trips.csv\n",
      "\n",
      "----------------------------------------\n",
      "COMBINING DATA FOR LAGOS\n",
      "----------------------------------------\n",
      "⚠ Too few common columns to combine lagos data\n",
      "\n",
      "============================================================\n",
      "PROCESSING CITY: NAIROBI\n",
      "============================================================\n",
      "Found 10 data files in nairobi: ['agency.txt', 'calendar.txt', 'calendar_dates.txt', 'feed_info.txt', 'frequencies.txt', 'routes.txt', 'shapes.txt', 'stops.txt', 'stop_times.txt', 'trips.txt']\n",
      "\n",
      "----------------------------------------\n",
      "Processing: nairobi/agency.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (1, 6)\n",
      "\n",
      "Cleaning data for nairobi/agency.txt...\n",
      "  - Converted 'agency_timezone' to datetime\n",
      "  ✓ Cleaned data shape: (1, 6) → (1, 8)\n",
      "\n",
      "--- Analysis for nairobi/agency.txt ---\n",
      "Shape: (1, 8)\n",
      "Memory usage: 0.00 MB\n",
      "\n",
      "Missing values:\n",
      "agency_timezone    1\n",
      "dtype: int64\n",
      "\n",
      "Data types:\n",
      "object            7\n",
      "datetime64[ns]    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "agency_id:\n",
      "agency_id\n",
      "UON    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "agency_name:\n",
      "agency_name\n",
      "Approved SACCOs - University of Nairobi C4D Lab reporting/sharing    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "agency_url:\n",
      "agency_url\n",
      "http://www.digitalmatatus.com    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "  agency_id                                        agency_name  \\\n",
      "0       UON  Approved SACCOs - University of Nairobi C4D La...   \n",
      "\n",
      "                      agency_url agency_timezone agency_lang   agency_phone  \\\n",
      "0  http://www.digitalmatatus.com             NaT          en  020 - 2729200   \n",
      "\n",
      "      city source_file  \n",
      "0  nairobi  agency.txt  \n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\nairobi\\cleaned_agency.csv\n",
      "\n",
      "----------------------------------------\n",
      "Processing: nairobi/calendar.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (1, 10)\n",
      "\n",
      "Cleaning data for nairobi/calendar.txt...\n",
      "  - Converted 'start_date' to datetime\n",
      "  - Converted 'end_date' to datetime\n",
      "  ✓ Cleaned data shape: (1, 10) → (1, 12)\n",
      "\n",
      "--- Analysis for nairobi/calendar.txt ---\n",
      "Shape: (1, 12)\n",
      "Memory usage: 0.00 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "int64             7\n",
      "object            3\n",
      "datetime64[ns]    2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Numeric columns summary:\n",
      "       monday  tuesday  wednesday  thursday  friday  saturday  sunday\n",
      "count     1.0      1.0        1.0       1.0     1.0       1.0     1.0\n",
      "mean      1.0      1.0        1.0       1.0     1.0       1.0     1.0\n",
      "std       NaN      NaN        NaN       NaN     NaN       NaN     NaN\n",
      "min       1.0      1.0        1.0       1.0     1.0       1.0     1.0\n",
      "25%       1.0      1.0        1.0       1.0     1.0       1.0     1.0\n",
      "50%       1.0      1.0        1.0       1.0     1.0       1.0     1.0\n",
      "75%       1.0      1.0        1.0       1.0     1.0       1.0     1.0\n",
      "max       1.0      1.0        1.0       1.0     1.0       1.0     1.0\n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "service_id:\n",
      "service_id\n",
      "DAILY    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "city:\n",
      "city\n",
      "nairobi    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "source_file:\n",
      "source_file\n",
      "calendar.txt    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "  service_id  monday  tuesday  wednesday  thursday  friday  saturday  sunday  \\\n",
      "0      DAILY       1        1          1         1       1         1       1   \n",
      "\n",
      "                     start_date                      end_date     city  \\\n",
      "0 1970-01-01 00:00:00.020190101 1970-01-01 00:00:00.020201231  nairobi   \n",
      "\n",
      "    source_file  \n",
      "0  calendar.txt  \n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\nairobi\\cleaned_calendar.csv\n",
      "\n",
      "----------------------------------------\n",
      "Processing: nairobi/calendar_dates.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (4, 3)\n",
      "\n",
      "Cleaning data for nairobi/calendar_dates.txt...\n",
      "  - Converted 'date' to datetime\n",
      "  ✓ Cleaned data shape: (4, 3) → (4, 5)\n",
      "\n",
      "--- Analysis for nairobi/calendar_dates.txt ---\n",
      "Shape: (4, 5)\n",
      "Memory usage: 0.00 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "object            3\n",
      "datetime64[ns]    1\n",
      "int64             1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Numeric columns summary:\n",
      "       exception_type\n",
      "count             4.0\n",
      "mean              1.0\n",
      "std               0.0\n",
      "min               1.0\n",
      "25%               1.0\n",
      "50%               1.0\n",
      "75%               1.0\n",
      "max               1.0\n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "service_id:\n",
      "service_id\n",
      "DAILY    4\n",
      "Name: count, dtype: int64\n",
      "\n",
      "city:\n",
      "city\n",
      "nairobi    4\n",
      "Name: count, dtype: int64\n",
      "\n",
      "source_file:\n",
      "source_file\n",
      "calendar_dates.txt    4\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "  service_id                          date  exception_type     city  \\\n",
      "0      DAILY 1970-01-01 00:00:00.020191020               1  nairobi   \n",
      "1      DAILY 1970-01-01 00:00:00.020191212               1  nairobi   \n",
      "2      DAILY 1970-01-01 00:00:00.020201020               1  nairobi   \n",
      "\n",
      "          source_file  \n",
      "0  calendar_dates.txt  \n",
      "1  calendar_dates.txt  \n",
      "2  calendar_dates.txt  \n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\nairobi\\cleaned_calendar_dates.csv\n",
      "\n",
      "----------------------------------------\n",
      "Processing: nairobi/feed_info.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (1, 5)\n",
      "\n",
      "Cleaning data for nairobi/feed_info.txt...\n",
      "  - Converted 'feed_start_date' to datetime\n",
      "  - Converted 'feed_end_date' to datetime\n",
      "  ✓ Cleaned data shape: (1, 5) → (1, 7)\n",
      "\n",
      "--- Analysis for nairobi/feed_info.txt ---\n",
      "Shape: (1, 7)\n",
      "Memory usage: 0.00 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "object            5\n",
      "datetime64[ns]    2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "feed_publisher_name:\n",
      "feed_publisher_name\n",
      "TRAINING    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "feed_publisher_url:\n",
      "feed_publisher_url\n",
      "http://www.digitalmatatus.com/    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "feed_lang:\n",
      "feed_lang\n",
      "en    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "  feed_publisher_name              feed_publisher_url feed_lang  \\\n",
      "0            TRAINING  http://www.digitalmatatus.com/        en   \n",
      "\n",
      "                feed_start_date                 feed_end_date     city  \\\n",
      "0 1970-01-01 00:00:00.020120302 1970-01-01 00:00:00.020201231  nairobi   \n",
      "\n",
      "     source_file  \n",
      "0  feed_info.txt  \n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\nairobi\\cleaned_feed_info.csv\n",
      "\n",
      "----------------------------------------\n",
      "Processing: nairobi/frequencies.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (792, 4)\n",
      "\n",
      "Cleaning data for nairobi/frequencies.txt...\n",
      "  - Converted 'start_time' to datetime\n",
      "  - Converted 'end_time' to datetime\n",
      "  ✓ Cleaned data shape: (792, 4) → (792, 6)\n",
      "\n",
      "--- Analysis for nairobi/frequencies.txt ---\n",
      "Shape: (792, 6)\n",
      "Memory usage: 0.15 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "object            3\n",
      "datetime64[ns]    2\n",
      "int64             1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Numeric columns summary:\n",
      "       headway_secs\n",
      "count    792.000000\n",
      "mean     500.000000\n",
      "std      283.021444\n",
      "min      300.000000\n",
      "25%      300.000000\n",
      "50%      300.000000\n",
      "75%      900.000000\n",
      "max      900.000000\n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "trip_id:\n",
      "trip_id\n",
      "10106110    3\n",
      "10106111    3\n",
      "10107110    3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "city:\n",
      "city\n",
      "nairobi    792\n",
      "Name: count, dtype: int64\n",
      "\n",
      "source_file:\n",
      "source_file\n",
      "frequencies.txt    792\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "    trip_id          start_time            end_time  headway_secs     city  \\\n",
      "0  10106110 2025-08-11 06:00:00 2025-08-11 09:00:00           300  nairobi   \n",
      "1  10106110 2025-08-11 09:00:00 2025-08-11 15:00:00           900  nairobi   \n",
      "2  10106110 2025-08-11 15:00:00 2025-08-11 21:00:00           300  nairobi   \n",
      "\n",
      "       source_file  \n",
      "0  frequencies.txt  \n",
      "1  frequencies.txt  \n",
      "2  frequencies.txt  \n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\nairobi\\cleaned_frequencies.csv\n",
      "\n",
      "----------------------------------------\n",
      "Processing: nairobi/routes.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (136, 5)\n",
      "\n",
      "Cleaning data for nairobi/routes.txt...\n",
      "  ✓ Cleaned data shape: (136, 5) → (136, 7)\n",
      "\n",
      "--- Analysis for nairobi/routes.txt ---\n",
      "Shape: (136, 7)\n",
      "Memory usage: 0.05 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "object    6\n",
      "int64     1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Numeric columns summary:\n",
      "       route_type\n",
      "count       136.0\n",
      "mean          3.0\n",
      "std           0.0\n",
      "min           3.0\n",
      "25%           3.0\n",
      "50%           3.0\n",
      "75%           3.0\n",
      "max           3.0\n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "route_id:\n",
      "route_id\n",
      "10000107D11    1\n",
      "10000114011    1\n",
      "10000116011    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "agency_id:\n",
      "agency_id\n",
      "UON    136\n",
      "Name: count, dtype: int64\n",
      "\n",
      "route_short_name:\n",
      "route_short_name\n",
      "107D    1\n",
      "114R    1\n",
      "116     1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "      route_id agency_id route_short_name              route_long_name  \\\n",
      "0  10000107D11       UON             107D                  Ruaka-Ruiru   \n",
      "1  10000114011       UON             114R  Ngara-Rwaka-Ndenderu-Limuru   \n",
      "2  10000116011       UON              116     Koja-Ngara-Banana-Limuru   \n",
      "\n",
      "   route_type     city source_file  \n",
      "0           3  nairobi  routes.txt  \n",
      "1           3  nairobi  routes.txt  \n",
      "2           3  nairobi  routes.txt  \n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\nairobi\\cleaned_routes.csv\n",
      "\n",
      "----------------------------------------\n",
      "Processing: nairobi/shapes.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (36483, 4)\n",
      "\n",
      "Cleaning data for nairobi/shapes.txt...\n",
      "  - Capped 4354 outliers in 'shape_pt_lat'\n",
      "  - Capped 1724 outliers in 'shape_pt_lon'\n",
      "  - Capped 2827 outliers in 'shape_pt_sequence'\n",
      "  ✓ Cleaned data shape: (36483, 4) → (36483, 6)\n",
      "\n",
      "--- Analysis for nairobi/shapes.txt ---\n",
      "Shape: (36483, 6)\n",
      "Memory usage: 6.82 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "object     3\n",
      "float64    2\n",
      "int64      1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Numeric columns summary:\n",
      "       shape_pt_lat  shape_pt_lon  shape_pt_sequence\n",
      "count  36483.000000  36483.000000       36483.000000\n",
      "mean      -1.272266     36.831952          94.687060\n",
      "std        0.038674      0.070877          77.974584\n",
      "min       -1.353856     36.673816           1.000000\n",
      "25%       -1.294335     36.799328          35.000000\n",
      "50%       -1.277171     36.830367          73.000000\n",
      "75%       -1.254654     36.883003         131.000000\n",
      "max       -1.195133     37.008514         275.000000\n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "shape_id:\n",
      "shape_id\n",
      "33738110    1239\n",
      "80114111     817\n",
      "10116110     623\n",
      "Name: count, dtype: int64\n",
      "\n",
      "city:\n",
      "city\n",
      "nairobi    36483\n",
      "Name: count, dtype: int64\n",
      "\n",
      "source_file:\n",
      "source_file\n",
      "shapes.txt    36483\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "   shape_id  shape_pt_lat  shape_pt_lon  shape_pt_sequence     city  \\\n",
      "0  10106110     -1.195133     36.758779                  1  nairobi   \n",
      "1  10106110     -1.195133     36.759025                  2  nairobi   \n",
      "2  10106110     -1.195133     36.759267                  3  nairobi   \n",
      "\n",
      "  source_file  \n",
      "0  shapes.txt  \n",
      "1  shapes.txt  \n",
      "2  shapes.txt  \n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\nairobi\\cleaned_shapes.csv\n",
      "\n",
      "----------------------------------------\n",
      "Processing: nairobi/stops.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (4284, 6)\n",
      "\n",
      "Cleaning data for nairobi/stops.txt...\n",
      "  - Dropped column 'location_type' (>70.0% missing)\n",
      "  - Dropped column 'parent_station' (>70.0% missing)\n",
      "  - Capped 349 outliers in 'stop_lat'\n",
      "  - Capped 4 outliers in 'stop_lon'\n",
      "  ✓ Cleaned data shape: (4284, 6) → (4284, 6)\n",
      "\n",
      "--- Analysis for nairobi/stops.txt ---\n",
      "Shape: (4284, 6)\n",
      "Memory usage: 1.01 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "object     4\n",
      "float64    2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Numeric columns summary:\n",
      "          stop_lat     stop_lon\n",
      "count  4284.000000  4284.000000\n",
      "mean     -1.273121    36.824192\n",
      "std       0.051552     0.081622\n",
      "min      -1.388754    36.629579\n",
      "25%      -1.303790    36.765932\n",
      "50%      -1.275724    36.827780\n",
      "75%      -1.247147    36.887114\n",
      "max      -1.162182    37.068887\n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "stop_id:\n",
      "stop_id\n",
      "0001RLW    1\n",
      "0002KOJ    1\n",
      "0003NGR    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "stop_name:\n",
      "stop_name\n",
      "Tuskys      23\n",
      "Car Wash    22\n",
      "Naivas      21\n",
      "Name: count, dtype: int64\n",
      "\n",
      "city:\n",
      "city\n",
      "nairobi    4284\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "   stop_id stop_name  stop_lat   stop_lon     city source_file\n",
      "0  0001RLW  Railways -1.290884  36.828242  nairobi   stops.txt\n",
      "1  0002KOJ      Koja -1.281230  36.822596  nairobi   stops.txt\n",
      "2  0003NGR     Ngara -1.274395  36.823806  nairobi   stops.txt\n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\nairobi\\cleaned_stops.csv\n",
      "\n",
      "----------------------------------------\n",
      "Processing: nairobi/stop_times.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (7533, 5)\n",
      "\n",
      "Cleaning data for nairobi/stop_times.txt...\n",
      "  - Converted 'arrival_time' to datetime\n",
      "  - Converted 'departure_time' to datetime\n",
      "  - Capped 146 outliers in 'stop_sequence'\n",
      "  ✓ Cleaned data shape: (7533, 5) → (7533, 7)\n",
      "\n",
      "--- Analysis for nairobi/stop_times.txt ---\n",
      "Shape: (7533, 7)\n",
      "Memory usage: 1.84 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "object            4\n",
      "datetime64[ns]    2\n",
      "float64           1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Numeric columns summary:\n",
      "       stop_sequence\n",
      "count    7533.000000\n",
      "mean       18.137528\n",
      "std        13.336553\n",
      "min         1.000000\n",
      "25%         7.000000\n",
      "50%        15.000000\n",
      "75%        26.000000\n",
      "max        54.500000\n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "trip_id:\n",
      "trip_id\n",
      "70103110    81\n",
      "10114111    81\n",
      "10114110    70\n",
      "Name: count, dtype: int64\n",
      "\n",
      "stop_id:\n",
      "stop_id\n",
      "0212SNT    28\n",
      "0102KHJ    22\n",
      "0510AGP    20\n",
      "Name: count, dtype: int64\n",
      "\n",
      "city:\n",
      "city\n",
      "nairobi    7533\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "    trip_id        arrival_time      departure_time  stop_id  stop_sequence  \\\n",
      "0  10106110 2025-08-11 06:00:00 2025-08-11 06:00:20  0110BAA            1.0   \n",
      "1  10106110 2025-08-11 06:00:34 2025-08-11 06:00:54  0110BNI            2.0   \n",
      "2  10106110 2025-08-11 06:01:10 2025-08-11 06:01:30  0110UMK            3.0   \n",
      "\n",
      "      city     source_file  \n",
      "0  nairobi  stop_times.txt  \n",
      "1  nairobi  stop_times.txt  \n",
      "2  nairobi  stop_times.txt  \n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\nairobi\\cleaned_stop_times.csv\n",
      "\n",
      "----------------------------------------\n",
      "Processing: nairobi/trips.txt\n",
      "----------------------------------------\n",
      "  ✓ Loaded with separator ',': (272, 6)\n",
      "\n",
      "Cleaning data for nairobi/trips.txt...\n",
      "  ✓ Cleaned data shape: (272, 6) → (272, 8)\n",
      "\n",
      "--- Analysis for nairobi/trips.txt ---\n",
      "Shape: (272, 8)\n",
      "Memory usage: 0.11 MB\n",
      "\n",
      "✓ No missing values!\n",
      "\n",
      "Data types:\n",
      "object    7\n",
      "int64     1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Numeric columns summary:\n",
      "       direction_id\n",
      "count    272.000000\n",
      "mean       0.496324\n",
      "std        0.500908\n",
      "min        0.000000\n",
      "25%        0.000000\n",
      "50%        0.000000\n",
      "75%        1.000000\n",
      "max        1.000000\n",
      "\n",
      "Categorical columns (top 3 each):\n",
      "\n",
      "route_id:\n",
      "route_id\n",
      "10000107D11    2\n",
      "10000114011    2\n",
      "10000116011    2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "service_id:\n",
      "service_id\n",
      "DAILY    272\n",
      "Name: count, dtype: int64\n",
      "\n",
      "trip_id:\n",
      "trip_id\n",
      "1107D110    1\n",
      "1107D111    1\n",
      "10114110    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data (first 3 rows):\n",
      "      route_id service_id   trip_id trip_headsign  direction_id  shape_id  \\\n",
      "0  10000107D11      DAILY  1107D110         Ruaka             0  1107D110   \n",
      "1  10000107D11      DAILY  1107D111         Ruiru             1  1107D111   \n",
      "2  10000114011      DAILY  10114110         Ngara             0  10114110   \n",
      "\n",
      "      city source_file  \n",
      "0  nairobi   trips.txt  \n",
      "1  nairobi   trips.txt  \n",
      "2  nairobi   trips.txt  \n",
      "✓ Saved to: cleaned_trotro_data\\by_city\\nairobi\\cleaned_trips.csv\n",
      "\n",
      "----------------------------------------\n",
      "COMBINING DATA FOR NAIROBI\n",
      "----------------------------------------\n",
      "⚠ Too few common columns to combine nairobi data\n",
      "\n",
      "============================================================\n",
      "CREATING GLOBAL COMBINED DATASET\n",
      "============================================================\n",
      "⚠ Too few common columns to create global combined dataset\n",
      "\n",
      "============================================================\n",
      "CLEANING COMPLETE!\n",
      "============================================================\n",
      "Output directory: cleaned_trotro_data\n",
      "- Individual city files: cleaned_trotro_data\\by_city\n",
      "- Combined datasets: cleaned_trotro_data\\combined\n",
      "- Reports: cleaned_trotro_data\\reports\n",
      "Finished at: 2025-08-11 08:44:44\n"
     ]
    }
   ],
   "source": [
    "#TroTro Multi-City Dataset Cleaning \n",
    "import opendatasets as od\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import re\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=== TroTro Multi-City Dataset Cleaning and Analysis ===\")\n",
    "print(f\"Starting analysis at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# 1. Download dataset with error handling\n",
    "def download_dataset():\n",
    "    \"\"\"Download the TroTro dataset from Kaggle\"\"\"\n",
    "    try:\n",
    "        dataset_url = 'https://www.kaggle.com/datasets/godfredaddaiamoako/trotro'\n",
    "        print(\"Downloading dataset...\")\n",
    "        od.download(dataset_url)\n",
    "        print(\"✓ Dataset downloaded successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠ Error downloading dataset: {e}\")\n",
    "        print(\"Please ensure you have Kaggle credentials configured\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# 2. Enhanced data cleaning function\n",
    "def clean_data(df, filename=\"\", city=\"\"):\n",
    "    \"\"\"Comprehensive data cleaning function\"\"\"\n",
    "    print(f\"\\nCleaning data for {city}/{filename}...\")\n",
    "    original_shape = df.shape\n",
    "    \n",
    "    if df.empty:\n",
    "        print(f\"  ⚠ Warning: {filename} is empty, skipping...\")\n",
    "        return None\n",
    "    \n",
    "    # Create a copy to avoid modifying original\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # 1. Remove completely empty rows and columns\n",
    "    df_clean = df_clean.dropna(how='all').dropna(axis=1, how='all')\n",
    "    \n",
    "    # 2. Clean column names\n",
    "    df_clean.columns = df_clean.columns.str.strip().str.lower()\n",
    "    df_clean.columns = df_clean.columns.str.replace(' ', '_').str.replace(r'[^\\w]', '_', regex=True)\n",
    "    \n",
    "    # 3. Handle duplicates\n",
    "    initial_rows = len(df_clean)\n",
    "    df_clean = df_clean.drop_duplicates()\n",
    "    duplicates_removed = initial_rows - len(df_clean)\n",
    "    if duplicates_removed > 0:\n",
    "        print(f\"  - Removed {duplicates_removed} duplicate rows\")\n",
    "    \n",
    "    # 4. Clean text columns\n",
    "    text_columns = df_clean.select_dtypes(include=['object']).columns\n",
    "    for col in text_columns:\n",
    "        if col in df_clean.columns:\n",
    "            # Strip whitespace and handle common issues\n",
    "            df_clean[col] = df_clean[col].astype(str).str.strip()\n",
    "            df_clean[col] = df_clean[col].replace(['nan', 'NaN', 'None', ''], np.nan)\n",
    "            \n",
    "            # Clean special characters and normalize text\n",
    "            df_clean[col] = df_clean[col].str.replace(r'\\s+', ' ', regex=True)\n",
    "            \n",
    "    # 5. Handle missing values intelligently\n",
    "    missing_threshold = 0.7  # Drop columns with >70% missing data\n",
    "    columns_to_drop = []\n",
    "    for col in df_clean.columns:\n",
    "        missing_pct = df_clean[col].isnull().sum() / len(df_clean)\n",
    "        if missing_pct > missing_threshold:\n",
    "            columns_to_drop.append(col)\n",
    "            print(f\"  - Dropped column '{col}' (>{missing_threshold*100}% missing)\")\n",
    "    \n",
    "    if columns_to_drop:\n",
    "        df_clean = df_clean.drop(columns=columns_to_drop)\n",
    "    \n",
    "    # 6. Fill remaining missing values based on data type\n",
    "    for col in df_clean.columns:\n",
    "        if df_clean[col].isnull().any():\n",
    "            if df_clean[col].dtype in ['int64', 'float64']:\n",
    "                # For numeric columns, use median\n",
    "                df_clean[col] = df_clean[col].fillna(df_clean[col].median())\n",
    "            else:\n",
    "                # For categorical columns, use mode or 'Unknown'\n",
    "                mode_val = df_clean[col].mode()\n",
    "                if len(mode_val) > 0:\n",
    "                    df_clean[col] = df_clean[col].fillna(mode_val[0])\n",
    "                else:\n",
    "                    df_clean[col] = df_clean[col].fillna('Unknown')\n",
    "    \n",
    "    # 7. Detect and handle potential date columns\n",
    "    potential_date_cols = [col for col in df_clean.columns if 'date' in col.lower() or 'time' in col.lower()]\n",
    "    for col in potential_date_cols:\n",
    "        try:\n",
    "            df_clean[col] = pd.to_datetime(df_clean[col], errors='coerce')\n",
    "            print(f\"  - Converted '{col}' to datetime\")\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # 8. Clean numeric columns\n",
    "    numeric_cols = df_clean.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_cols:\n",
    "        if len(df_clean[col].dropna()) > 0:  # Only if we have numeric data\n",
    "            # Remove outliers using IQR method\n",
    "            Q1 = df_clean[col].quantile(0.25)\n",
    "            Q3 = df_clean[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            \n",
    "            if IQR > 0:  # Only apply if there's variation in the data\n",
    "                lower_bound = Q1 - 1.5 * IQR\n",
    "                upper_bound = Q3 + 1.5 * IQR\n",
    "                \n",
    "                outliers_before = len(df_clean[(df_clean[col] < lower_bound) | (df_clean[col] > upper_bound)])\n",
    "                df_clean[col] = df_clean[col].clip(lower=lower_bound, upper=upper_bound)\n",
    "                if outliers_before > 0:\n",
    "                    print(f\"  - Capped {outliers_before} outliers in '{col}'\")\n",
    "    \n",
    "    # 9. Add city identifier\n",
    "    df_clean['city'] = city\n",
    "    df_clean['source_file'] = filename\n",
    "    \n",
    "    print(f\"  ✓ Cleaned data shape: {original_shape} → {df_clean.shape}\")\n",
    "    return df_clean\n",
    "\n",
    "# 3. Data analysis function\n",
    "def analyze_data(df, filename=\"\", city=\"\"):\n",
    "    \"\"\"Perform comprehensive data analysis\"\"\"\n",
    "    print(f\"\\n--- Analysis for {city}/{filename} ---\")\n",
    "    if df is None or df.empty:\n",
    "        print(\"No data to analyze\")\n",
    "        return df\n",
    "        \n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    \n",
    "    # Missing values analysis\n",
    "    missing_data = df.isnull().sum()\n",
    "    if missing_data.sum() > 0:\n",
    "        print(\"\\nMissing values:\")\n",
    "        print(missing_data[missing_data > 0])\n",
    "    else:\n",
    "        print(\"\\n✓ No missing values!\")\n",
    "    \n",
    "    # Data types\n",
    "    print(f\"\\nData types:\")\n",
    "    print(df.dtypes.value_counts())\n",
    "    \n",
    "    # Numeric columns summary\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    if len(numeric_cols) > 0:\n",
    "        print(f\"\\nNumeric columns summary:\")\n",
    "        print(df[numeric_cols].describe())\n",
    "    \n",
    "    # Categorical columns info\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "    if len(categorical_cols) > 0:\n",
    "        print(f\"\\nCategorical columns (top 3 each):\")\n",
    "        for col in categorical_cols[:3]:  # Limit to first 3 to avoid clutter\n",
    "            print(f\"\\n{col}:\")\n",
    "            print(df[col].value_counts().head(3))\n",
    "    \n",
    "    # Display sample\n",
    "    print(f\"\\nSample data (first 3 rows):\")\n",
    "    print(df.head(3))\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 4. Load data with multiple separator attempts\n",
    "def load_data_file(file_path):\n",
    "    \"\"\"Try to load a data file with different separators\"\"\"\n",
    "    file_extension = os.path.splitext(file_path)[1].lower()\n",
    "    \n",
    "    separators_to_try = [',', '\\t', ';', '|', r'\\s+']\n",
    "    \n",
    "    for sep in separators_to_try:\n",
    "        try:\n",
    "            if sep == r'\\s+':\n",
    "                df = pd.read_csv(file_path, sep=sep, engine='python')\n",
    "            else:\n",
    "                df = pd.read_csv(file_path, sep=sep)\n",
    "            \n",
    "            # Check if data loaded properly (more than just headers)\n",
    "            if not df.empty and len(df.columns) > 1:\n",
    "                print(f\"  ✓ Loaded with separator '{sep}': {df.shape}\")\n",
    "                return df\n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    print(f\"  ❌ Could not load {file_path} with any separator\")\n",
    "    return None\n",
    "\n",
    "# 5. Create output directories\n",
    "def create_output_structure():\n",
    "    \"\"\"Create clean directory structure\"\"\"\n",
    "    # Create main output directory\n",
    "    output_dir = Path('cleaned_trotro_data')\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Create subdirectories\n",
    "    (output_dir / 'by_city').mkdir(exist_ok=True)\n",
    "    (output_dir / 'combined').mkdir(exist_ok=True)\n",
    "    (output_dir / 'reports').mkdir(exist_ok=True)\n",
    "    \n",
    "    return output_dir\n",
    "\n",
    "# 6. Main execution function\n",
    "def main():\n",
    "    \"\"\"Main execution function for multi-city cleaning\"\"\"\n",
    "    \n",
    "    # Download dataset\n",
    "    if not download_dataset():\n",
    "        print(\"Failed to download dataset. Please check your Kaggle credentials.\")\n",
    "        return\n",
    "    \n",
    "    # Find data directory structure\n",
    "    base_data_dir = Path('trotro')\n",
    "    if not base_data_dir.exists():\n",
    "        # Try alternative directory names\n",
    "        possible_dirs = [d for d in Path('.').iterdir() if d.is_dir() and 'trotro' in d.name.lower()]\n",
    "        if possible_dirs:\n",
    "            base_data_dir = possible_dirs[0]\n",
    "        else:\n",
    "            print(\"Could not find dataset directory\")\n",
    "            return\n",
    "    \n",
    "    # Look for the nested structure\n",
    "    datasets_dir = base_data_dir / 'trotrolive-datasets'\n",
    "    if not datasets_dir.exists():\n",
    "        datasets_dir = base_data_dir\n",
    "    \n",
    "    print(f\"Using data directory: {datasets_dir}\")\n",
    "    \n",
    "    # Create output structure\n",
    "    output_dir = create_output_structure()\n",
    "    \n",
    "    # Find all city directories\n",
    "    city_dirs = [d for d in datasets_dir.iterdir() if d.is_dir()]\n",
    "    print(f\"Found {len(city_dirs)} city directories: {[d.name for d in city_dirs]}\")\n",
    "    \n",
    "    if not city_dirs:\n",
    "        print(\"No city directories found\")\n",
    "        return\n",
    "    \n",
    "    # Storage for all cleaned data\n",
    "    all_city_data = {}\n",
    "    all_files_data = []\n",
    "    \n",
    "    # Process each city\n",
    "    for city_dir in city_dirs:\n",
    "        city_name = city_dir.name\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"PROCESSING CITY: {city_name.upper()}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Find data files in city directory\n",
    "        data_files = []\n",
    "        for ext in ['*.txt', '*.csv']:\n",
    "            data_files.extend(city_dir.glob(ext))\n",
    "        \n",
    "        if not data_files:\n",
    "            print(f\"No data files found in {city_name}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"Found {len(data_files)} data files in {city_name}: {[f.name for f in data_files]}\")\n",
    "        \n",
    "        # Storage for this city's data\n",
    "        city_cleaned_data = {}\n",
    "        \n",
    "        # Process each file in the city\n",
    "        for data_file in data_files:\n",
    "            try:\n",
    "                print(f\"\\n{'-'*40}\")\n",
    "                print(f\"Processing: {city_name}/{data_file.name}\")\n",
    "                print(f\"{'-'*40}\")\n",
    "                \n",
    "                # Load data\n",
    "                df = load_data_file(data_file)\n",
    "                if df is None:\n",
    "                    continue\n",
    "                \n",
    "                # Clean data\n",
    "                df_clean = clean_data(df, data_file.name, city_name)\n",
    "                if df_clean is None:\n",
    "                    continue\n",
    "                \n",
    "                # Analyze cleaned data\n",
    "                df_analyzed = analyze_data(df_clean, data_file.name, city_name)\n",
    "                \n",
    "                # Save individual cleaned file\n",
    "                city_output_dir = output_dir / 'by_city' / city_name\n",
    "                city_output_dir.mkdir(exist_ok=True)\n",
    "                \n",
    "                cleaned_filename = city_output_dir / f'cleaned_{data_file.stem}.csv'\n",
    "                df_clean.to_csv(cleaned_filename, index=False)\n",
    "                print(f\"✓ Saved to: {cleaned_filename}\")\n",
    "                \n",
    "                # Store for city-level combination\n",
    "                city_cleaned_data[data_file.name] = df_clean\n",
    "                \n",
    "                # Store for global combination\n",
    "                all_files_data.append(df_clean)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error processing {city_name}/{data_file.name}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Combine all files for this city\n",
    "        if city_cleaned_data:\n",
    "            print(f\"\\n{'-'*40}\")\n",
    "            print(f\"COMBINING DATA FOR {city_name.upper()}\")\n",
    "            print(f\"{'-'*40}\")\n",
    "            \n",
    "            try:\n",
    "                # Find common columns across all files in this city\n",
    "                all_columns = [set(df.columns) for df in city_cleaned_data.values()]\n",
    "                common_columns = set.intersection(*all_columns) if all_columns else set()\n",
    "                \n",
    "                if len(common_columns) > 2:  # More than just city and source_file\n",
    "                    print(f\"Found {len(common_columns)} common columns for {city_name}\")\n",
    "                    \n",
    "                    # Combine city data\n",
    "                    city_combined = pd.concat([df[list(common_columns)] for df in city_cleaned_data.values()], \n",
    "                                            ignore_index=True)\n",
    "                    \n",
    "                    # Save city combined data\n",
    "                    city_combined_file = output_dir / 'by_city' / f'{city_name}_combined.csv'\n",
    "                    city_combined.to_csv(city_combined_file, index=False)\n",
    "                    print(f\"✓ City combined data saved: {city_combined_file}\")\n",
    "                    \n",
    "                    # Store for global analysis\n",
    "                    all_city_data[city_name] = city_combined\n",
    "                    \n",
    "                else:\n",
    "                    print(f\"⚠ Too few common columns to combine {city_name} data\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error combining {city_name} data: {e}\")\n",
    "    \n",
    "    # Create global combined dataset\n",
    "    if all_files_data:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"CREATING GLOBAL COMBINED DATASET\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        try:\n",
    "            # Find common columns across ALL files from ALL cities\n",
    "            all_columns = [set(df.columns) for df in all_files_data]\n",
    "            global_common_columns = set.intersection(*all_columns) if all_columns else set()\n",
    "            \n",
    "            if len(global_common_columns) > 2:\n",
    "                print(f\"Found {len(global_common_columns)} common columns across all cities\")\n",
    "                \n",
    "                # Create global combined dataset\n",
    "                global_combined = pd.concat([df[list(global_common_columns)] for df in all_files_data], \n",
    "                                          ignore_index=True)\n",
    "                \n",
    "                # Save global combined data\n",
    "                global_combined_file = output_dir / 'combined' / 'all_cities_combined.csv'\n",
    "                global_combined.to_csv(global_combined_file, index=False)\n",
    "                print(f\"✓ Global combined data saved: {global_combined_file}\")\n",
    "                \n",
    "                # Generate summary report\n",
    "                summary_report = f\"\"\"\n",
    "TroTro Dataset Cleaning Summary Report\n",
    "Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "Cities Processed: {len(all_city_data)}\n",
    "Total Files Processed: {len(all_files_data)}\n",
    "Global Combined Dataset Shape: {global_combined.shape}\n",
    "\n",
    "City Breakdown:\n",
    "\"\"\"\n",
    "                for city, data in all_city_data.items():\n",
    "                    summary_report += f\"- {city}: {data.shape[0]} records, {data.shape[1]} columns\\n\"\n",
    "                \n",
    "                summary_report += f\"\\nCommon Columns Across All Data:\\n\"\n",
    "                for col in sorted(global_common_columns):\n",
    "                    summary_report += f\"- {col}\\n\"\n",
    "                \n",
    "                # Save summary report\n",
    "                report_file = output_dir / 'reports' / 'cleaning_summary.txt'\n",
    "                with open(report_file, 'w') as f:\n",
    "                    f.write(summary_report)\n",
    "                \n",
    "                print(f\"✓ Summary report saved: {report_file}\")\n",
    "                \n",
    "            else:\n",
    "                print(\"⚠ Too few common columns to create global combined dataset\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error creating global combined dataset: {e}\")\n",
    "    \n",
    "    # Final summary\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"CLEANING COMPLETE!\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Output directory: {output_dir}\")\n",
    "    print(f\"- Individual city files: {output_dir / 'by_city'}\")\n",
    "    print(f\"- Combined datasets: {output_dir / 'combined'}\")\n",
    "    print(f\"- Reports: {output_dir / 'reports'}\")\n",
    "    print(f\"Finished at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# Execute the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f903865a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streamlined TroTro Unified ML Pipeline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import joblib\n",
    "\n",
    "# ML Libraries - Only the essentials\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import (mean_squared_error, r2_score, accuracy_score, \n",
    "                           classification_report, confusion_matrix)\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Try XGBoost (optional)\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGBOOST_AVAILABLE = True\n",
    "except ImportError:\n",
    "    XGBOOST_AVAILABLE = False\n",
    "    print(\"XGBoost not available - using sklearn alternatives\")\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class StreamlinedTroTroML:\n",
    "    \"\"\"Streamlined TroTro ML Pipeline - Unified Model Approach\"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir=\"cleaned_trotro_data\"):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.output_dir = Path('streamlined_trotro_results')\n",
    "        self.output_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Setup logging\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "            handlers=[\n",
    "                logging.FileHandler('trotro_ml.log'),\n",
    "                logging.StreamHandler()\n",
    "            ]\n",
    "        )\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "        # Initialize storage\n",
    "        self.combined_data = None\n",
    "        self.models = {}\n",
    "        self.best_models = {}\n",
    "        \n",
    "        print(\"🚀 Streamlined TroTro ML Pipeline Initialized\")\n",
    "        print(f\"📁 Data directory: {self.data_dir}\")\n",
    "        print(f\"📊 Output directory: {self.output_dir}\")\n",
    "    \n",
    "    def load_and_combine_data(self):\n",
    "        \"\"\"Load all city data and combine into unified dataset\"\"\"\n",
    "        self.logger.info(\"Loading and combining all city data...\")\n",
    "        \n",
    "        all_dataframes = []\n",
    "        city_data_dir = self.data_dir / 'by_city'\n",
    "        \n",
    "        if not city_data_dir.exists():\n",
    "            self.logger.error(f\"City data directory not found: {city_data_dir}\")\n",
    "            return None\n",
    "        \n",
    "        # Load data from all cities\n",
    "        for city_path in city_data_dir.iterdir():\n",
    "            if city_path.is_dir():\n",
    "                city_name = city_path.name\n",
    "                self.logger.info(f\"Loading data for {city_name}\")\n",
    "                \n",
    "                # Find CSV files in city directory\n",
    "                csv_files = list(city_path.glob('*.csv'))\n",
    "                \n",
    "                for csv_file in csv_files:\n",
    "                    try:\n",
    "                        # Clean encoding handling\n",
    "                        df = pd.read_csv(csv_file, encoding='utf-8', low_memory=False)\n",
    "                        if df.empty:\n",
    "                            continue\n",
    "                            \n",
    "                        # Add city identifier\n",
    "                        df['city'] = city_name\n",
    "                        df['source_file'] = csv_file.stem\n",
    "                        \n",
    "                        # Clean column names\n",
    "                        df.columns = df.columns.str.strip().str.lower()\n",
    "                        df.columns = df.columns.str.replace(r'[^\\w]', '_', regex=True)\n",
    "                        df.columns = df.columns.str.replace(r'_+', '_', regex=True)\n",
    "                        \n",
    "                        all_dataframes.append(df)\n",
    "                        self.logger.info(f\"  ✅ Loaded {csv_file.name}: {df.shape}\")\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        self.logger.warning(f\"  ❌ Failed to load {csv_file}: {e}\")\n",
    "        \n",
    "        # Also check for combined files directly in by_city\n",
    "        for csv_file in city_data_dir.glob('*_combined.csv'):\n",
    "            try:\n",
    "                df = pd.read_csv(csv_file, encoding='utf-8', low_memory=False)\n",
    "                city_name = csv_file.stem.replace('_combined', '')\n",
    "                df['city'] = city_name\n",
    "                df['source_file'] = csv_file.stem\n",
    "                \n",
    "                df.columns = df.columns.str.strip().str.lower()\n",
    "                df.columns = df.columns.str.replace(r'[^\\w]', '_', regex=True)\n",
    "                \n",
    "                all_dataframes.append(df)\n",
    "                self.logger.info(f\"✅ Loaded combined file for {city_name}: {df.shape}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"❌ Failed to load {csv_file}: {e}\")\n",
    "        \n",
    "        if not all_dataframes:\n",
    "            self.logger.error(\"No data files loaded successfully!\")\n",
    "            return None\n",
    "        \n",
    "        # Combine all dataframes\n",
    "        self.logger.info(\"Combining all datasets...\")\n",
    "        \n",
    "        # Find common columns across all dataframes\n",
    "        common_columns = set(all_dataframes[0].columns)\n",
    "        for df in all_dataframes[1:]:\n",
    "            common_columns = common_columns.intersection(set(df.columns))\n",
    "        \n",
    "        if len(common_columns) < 3:\n",
    "            self.logger.warning(\"Few common columns found. Using all available data.\")\n",
    "            self.combined_data = pd.concat(all_dataframes, ignore_index=True, sort=False)\n",
    "        else:\n",
    "            # Use only common columns for consistency\n",
    "            combined_dfs = [df[list(common_columns)] for df in all_dataframes]\n",
    "            self.combined_data = pd.concat(combined_dfs, ignore_index=True)\n",
    "        \n",
    "        # Basic cleaning\n",
    "        self.combined_data = self.combined_data.dropna(how='all')  # Remove empty rows\n",
    "        self.combined_data = self.combined_data.loc[:, self.combined_data.notna().any()]  # Remove empty columns\n",
    "        \n",
    "        self.logger.info(f\"✅ Combined dataset created: {self.combined_data.shape}\")\n",
    "        self.logger.info(f\"📊 Cities included: {self.combined_data['city'].nunique()}\")\n",
    "        \n",
    "        return self.combined_data\n",
    "    \n",
    "    def visualize_dataset(self):\n",
    "        \"\"\"Create comprehensive visualizations of the combined dataset\"\"\"\n",
    "        if self.combined_data is None:\n",
    "            self.logger.error(\"No data loaded for visualization\")\n",
    "            return\n",
    "        \n",
    "        self.logger.info(\"Creating dataset visualizations...\")\n",
    "        \n",
    "        # Set up plotting style\n",
    "        plt.style.use('default')\n",
    "        sns.set_palette(\"husl\")\n",
    "        \n",
    "        # Create visualization directory\n",
    "        viz_dir = self.output_dir / 'visualizations'\n",
    "        viz_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # 1. Dataset Overview\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        fig.suptitle('TroTro Dataset Overview', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # Dataset size by city\n",
    "        city_counts = self.combined_data['city'].value_counts()\n",
    "        axes[0, 0].bar(city_counts.index, city_counts.values, color='skyblue', edgecolor='navy')\n",
    "        axes[0, 0].set_title('Records per City')\n",
    "        axes[0, 0].set_xlabel('City')\n",
    "        axes[0, 0].set_ylabel('Number of Records')\n",
    "        axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Missing data heatmap\n",
    "        missing_data = self.combined_data.isnull().sum().sort_values(ascending=False)\n",
    "        top_missing = missing_data.head(10)\n",
    "        if len(top_missing) > 0:\n",
    "            axes[0, 1].bar(range(len(top_missing)), top_missing.values, color='lightcoral')\n",
    "            axes[0, 1].set_title('Top 10 Columns with Missing Data')\n",
    "            axes[0, 1].set_xlabel('Columns')\n",
    "            axes[0, 1].set_ylabel('Missing Values Count')\n",
    "            axes[0, 1].set_xticks(range(len(top_missing)))\n",
    "            axes[0, 1].set_xticklabels(top_missing.index, rotation=45, ha='right')\n",
    "        \n",
    "        # Data types distribution\n",
    "        dtype_counts = self.combined_data.dtypes.value_counts()\n",
    "        axes[1, 0].pie(dtype_counts.values, labels=dtype_counts.index, autopct='%1.1f%%')\n",
    "        axes[1, 0].set_title('Data Types Distribution')\n",
    "        \n",
    "        # Column count by type\n",
    "        numeric_cols = self.combined_data.select_dtypes(include=[np.number]).columns\n",
    "        categorical_cols = self.combined_data.select_dtypes(include=['object', 'category']).columns\n",
    "        \n",
    "        col_types = {'Numeric': len(numeric_cols), 'Categorical': len(categorical_cols)}\n",
    "        axes[1, 1].bar(col_types.keys(), col_types.values(), color=['lightgreen', 'orange'])\n",
    "        axes[1, 1].set_title('Column Types Count')\n",
    "        axes[1, 1].set_ylabel('Number of Columns')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(viz_dir / 'dataset_overview.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # 2. Numeric Features Analysis\n",
    "        if len(numeric_cols) > 0:\n",
    "            # Exclude city-related columns for better analysis\n",
    "            analysis_numeric = [col for col in numeric_cols if col not in ['city', 'source_file']]\n",
    "            \n",
    "            if len(analysis_numeric) > 0:\n",
    "                n_cols = min(4, len(analysis_numeric))\n",
    "                n_rows = (len(analysis_numeric) + n_cols - 1) // n_cols\n",
    "                \n",
    "                fig, axes = plt.subplots(n_rows, n_cols, figsize=(16, 4 * n_rows))\n",
    "                fig.suptitle('Numeric Features Distribution', fontsize=16, fontweight='bold')\n",
    "                \n",
    "                if n_rows == 1:\n",
    "                    axes = axes if n_cols > 1 else [axes]\n",
    "                else:\n",
    "                    axes = axes.flatten()\n",
    "                \n",
    "                for i, col in enumerate(analysis_numeric[:len(axes)]):\n",
    "                    try:\n",
    "                        data_to_plot = self.combined_data[col].dropna()\n",
    "                        if len(data_to_plot) > 0:\n",
    "                            axes[i].hist(data_to_plot, bins=30, alpha=0.7, color='skyblue', edgecolor='navy')\n",
    "                            axes[i].set_title(f'{col}\\n(n={len(data_to_plot)})')\n",
    "                            axes[i].set_xlabel(col)\n",
    "                            axes[i].set_ylabel('Frequency')\n",
    "                    except Exception as e:\n",
    "                        axes[i].text(0.5, 0.5, f'Error plotting {col}', \n",
    "                                   ha='center', va='center', transform=axes[i].transAxes)\n",
    "                \n",
    "                # Hide empty subplots\n",
    "                for i in range(len(analysis_numeric), len(axes)):\n",
    "                    axes[i].set_visible(False)\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.savefig(viz_dir / 'numeric_features.png', dpi=300, bbox_inches='tight')\n",
    "                plt.close()\n",
    "        \n",
    "        # 3. Categorical Features Analysis\n",
    "        if len(categorical_cols) > 0:\n",
    "            analysis_categorical = [col for col in categorical_cols \n",
    "                                  if col not in ['city', 'source_file'] and \n",
    "                                  self.combined_data[col].nunique() <= 20]\n",
    "            \n",
    "            if len(analysis_categorical) > 0:\n",
    "                n_cols = min(3, len(analysis_categorical))\n",
    "                n_rows = (len(analysis_categorical) + n_cols - 1) // n_cols\n",
    "                \n",
    "                fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5 * n_rows))\n",
    "                fig.suptitle('Categorical Features Distribution', fontsize=16, fontweight='bold')\n",
    "                \n",
    "                if n_rows == 1:\n",
    "                    axes = axes if n_cols > 1 else [axes]\n",
    "                else:\n",
    "                    axes = axes.flatten()\n",
    "                \n",
    "                for i, col in enumerate(analysis_categorical[:len(axes)]):\n",
    "                    try:\n",
    "                        value_counts = self.combined_data[col].value_counts().head(15)\n",
    "                        axes[i].bar(range(len(value_counts)), value_counts.values, color='lightgreen')\n",
    "                        axes[i].set_title(f'{col}\\n(Unique: {self.combined_data[col].nunique()})')\n",
    "                        axes[i].set_xlabel('Categories')\n",
    "                        axes[i].set_ylabel('Count')\n",
    "                        axes[i].set_xticks(range(len(value_counts)))\n",
    "                        axes[i].set_xticklabels(value_counts.index, rotation=45, ha='right')\n",
    "                    except Exception as e:\n",
    "                        axes[i].text(0.5, 0.5, f'Error plotting {col}', \n",
    "                                   ha='center', va='center', transform=axes[i].transAxes)\n",
    "                \n",
    "                # Hide empty subplots\n",
    "                for i in range(len(analysis_categorical), len(axes)):\n",
    "                    axes[i].set_visible(False)\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.savefig(viz_dir / 'categorical_features.png', dpi=300, bbox_inches='tight')\n",
    "                plt.close()\n",
    "        \n",
    "        # 4. Correlation Analysis\n",
    "        numeric_data = self.combined_data.select_dtypes(include=[np.number])\n",
    "        if len(numeric_data.columns) > 1:\n",
    "            plt.figure(figsize=(12, 10))\n",
    "            correlation_matrix = numeric_data.corr()\n",
    "            \n",
    "            # Create mask for upper triangle\n",
    "            mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "            \n",
    "            sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='coolwarm', \n",
    "                       center=0, square=True, fmt='.2f')\n",
    "            plt.title('Feature Correlation Matrix', fontsize=16, fontweight='bold')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(viz_dir / 'correlation_matrix.png', dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "        \n",
    "        self.logger.info(f\"✅ Visualizations saved to {viz_dir}\")\n",
    "    \n",
    "    def identify_ml_targets(self):\n",
    "        \"\"\"Identify potential ML targets in the dataset\"\"\"\n",
    "        if self.combined_data is None:\n",
    "            self.logger.error(\"No data available for target identification\")\n",
    "            return []\n",
    "        \n",
    "        self.logger.info(\"Identifying potential ML targets...\")\n",
    "        \n",
    "        targets = []\n",
    "        \n",
    "        # Transport-specific keywords for targets\n",
    "        regression_keywords = ['fare', 'price', 'cost', 'duration', 'time', 'distance', 'speed']\n",
    "        classification_keywords = ['route', 'destination', 'origin', 'station', 'vehicle_type', 'status']\n",
    "        \n",
    "        # Check each column\n",
    "        for col in self.combined_data.columns:\n",
    "            if col in ['city', 'source_file']:\n",
    "                continue\n",
    "            \n",
    "            col_lower = col.lower()\n",
    "            non_null_count = self.combined_data[col].notna().sum()\n",
    "            total_count = len(self.combined_data)\n",
    "            \n",
    "            # Skip columns with too much missing data\n",
    "            if non_null_count < total_count * 0.5:\n",
    "                continue\n",
    "            \n",
    "            # Check for regression targets\n",
    "            for keyword in regression_keywords:\n",
    "                if keyword in col_lower:\n",
    "                    if (col in self.combined_data.select_dtypes(include=[np.number]).columns and \n",
    "                        self.combined_data[col].nunique() > 10):\n",
    "                        targets.append(('regression', col))\n",
    "                        self.logger.info(f\"  📈 Regression target found: {col}\")\n",
    "                        break\n",
    "            \n",
    "            # Check for classification targets\n",
    "            for keyword in classification_keywords:\n",
    "                if keyword in col_lower:\n",
    "                    unique_count = self.combined_data[col].nunique()\n",
    "                    if 2 <= unique_count <= 50:\n",
    "                        targets.append(('classification', col))\n",
    "                        self.logger.info(f\"  📊 Classification target found: {col}\")\n",
    "                        break\n",
    "        \n",
    "        # If no specific targets found, use general heuristics\n",
    "        if not targets:\n",
    "            self.logger.info(\"No transport-specific targets found. Using general heuristics...\")\n",
    "            \n",
    "            # General regression targets (numeric with enough variety)\n",
    "            for col in self.combined_data.select_dtypes(include=[np.number]).columns:\n",
    "                if col not in ['city', 'source_file'] and self.combined_data[col].nunique() > 10:\n",
    "                    targets.append(('regression', col))\n",
    "                    self.logger.info(f\"  📈 General regression target: {col}\")\n",
    "            \n",
    "            # General classification targets\n",
    "            for col in self.combined_data.columns:\n",
    "                if col not in ['city', 'source_file']:\n",
    "                    unique_count = self.combined_data[col].nunique()\n",
    "                    if 2 <= unique_count <= 20:\n",
    "                        targets.append(('classification', col))\n",
    "                        self.logger.info(f\"  📊 General classification target: {col}\")\n",
    "        \n",
    "        # Limit to top 5 most promising targets\n",
    "        targets = targets[:5]\n",
    "        self.logger.info(f\"✅ Identified {len(targets)} ML targets\")\n",
    "        \n",
    "        return targets\n",
    "    \n",
    "    def prepare_data(self, target_col, task_type):\n",
    "        \"\"\"Prepare data for ML with efficient preprocessing\"\"\"\n",
    "        self.logger.info(f\"Preparing data for {target_col} ({task_type})\")\n",
    "        \n",
    "        # Create feature matrix\n",
    "        X = self.combined_data.drop(columns=[target_col, 'source_file'], errors='ignore')\n",
    "        y = self.combined_data[target_col].copy()\n",
    "        \n",
    "        # Remove samples with missing target\n",
    "        mask = y.notna()\n",
    "        X = X[mask]\n",
    "        y = y[mask]\n",
    "        \n",
    "        if len(X) == 0:\n",
    "            self.logger.error(\"No valid samples after removing missing targets\")\n",
    "            return None, None, None, None, None\n",
    "        \n",
    "        # Prepare target for classification\n",
    "        if task_type == 'classification' and y.dtype == 'object':\n",
    "            le = LabelEncoder()\n",
    "            y = le.fit_transform(y.astype(str))\n",
    "            self.target_encoder = le\n",
    "        \n",
    "        # Identify feature types\n",
    "        numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "        \n",
    "        # Create preprocessing pipeline\n",
    "        preprocessors = []\n",
    "        \n",
    "        if numeric_features:\n",
    "            numeric_pipeline = Pipeline([\n",
    "                ('imputer', SimpleImputer(strategy='median')),\n",
    "                ('scaler', StandardScaler())\n",
    "            ])\n",
    "            preprocessors.append(('numeric', numeric_pipeline, numeric_features))\n",
    "        \n",
    "        if categorical_features:\n",
    "            categorical_pipeline = Pipeline([\n",
    "                ('imputer', SimpleImputer(strategy='constant', fill_value='unknown')),\n",
    "                ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "            ])\n",
    "            preprocessors.append(('categorical', categorical_pipeline, categorical_features))\n",
    "        \n",
    "        if preprocessors:\n",
    "            self.preprocessor = ColumnTransformer(preprocessors, remainder='drop')\n",
    "            X_processed = self.preprocessor.fit_transform(X)\n",
    "        else:\n",
    "            self.logger.warning(\"No valid features for preprocessing\")\n",
    "            return None, None, None, None, None\n",
    "        \n",
    "        # Split data\n",
    "        test_size = min(0.3, max(0.1, 100 / len(X_processed)))\n",
    "        stratify = y if task_type == 'classification' else None\n",
    "        \n",
    "        try:\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X_processed, y, test_size=test_size, random_state=42, stratify=stratify\n",
    "            )\n",
    "        except ValueError:\n",
    "            # Fallback without stratification\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X_processed, y, test_size=test_size, random_state=42\n",
    "            )\n",
    "        \n",
    "        self.logger.info(f\"✅ Data prepared: Train={X_train.shape}, Test={X_test.shape}\")\n",
    "        \n",
    "        return X_train, X_test, y_train, y_test, (numeric_features + categorical_features)\n",
    "    \n",
    "    def train_models(self, X_train, X_test, y_train, y_test, task_type, target_col):\n",
    "        \"\"\"Train best-performing models efficiently\"\"\"\n",
    "        self.logger.info(f\"Training models for {target_col} ({task_type})\")\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        if task_type == 'regression':\n",
    "            models = {\n",
    "                'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "                'Linear Regression': LinearRegression(),\n",
    "                'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "            }\n",
    "            \n",
    "            if XGBOOST_AVAILABLE:\n",
    "                models['XGBoost'] = xgb.XGBRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "            \n",
    "            # Hyperparameter grids for key models\n",
    "            param_grids = {\n",
    "                'Random Forest': {\n",
    "                    'n_estimators': [50, 100, 200],\n",
    "                    'max_depth': [10, 20, None],\n",
    "                    'min_samples_split': [2, 5, 10]\n",
    "                },\n",
    "                'Gradient Boosting': {\n",
    "                    'n_estimators': [50, 100, 200],\n",
    "                    'learning_rate': [0.05, 0.1, 0.2],\n",
    "                    'max_depth': [3, 5, 7]\n",
    "                }\n",
    "            }\n",
    "            \n",
    "        else:  # classification\n",
    "            models = {\n",
    "                'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "                'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "                'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "            }\n",
    "            \n",
    "            if XGBOOST_AVAILABLE:\n",
    "                models['XGBoost'] = xgb.XGBClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "            \n",
    "            param_grids = {\n",
    "                'Random Forest': {\n",
    "                    'n_estimators': [50, 100, 200],\n",
    "                    'max_depth': [10, 20, None],\n",
    "                    'min_samples_split': [2, 5, 10]\n",
    "                },\n",
    "                'Logistic Regression': {\n",
    "                    'C': [0.1, 1.0, 10.0],\n",
    "                    'penalty': ['l1', 'l2'],\n",
    "                    'solver': ['liblinear', 'saga']\n",
    "                }\n",
    "            }\n",
    "        \n",
    "        # Train and tune models\n",
    "        for name, model in models.items():\n",
    "            try:\n",
    "                self.logger.info(f\"  Training {name}...\")\n",
    "                \n",
    "                # Hyperparameter tuning for select models\n",
    "                if name in param_grids and len(X_train) > 100:\n",
    "                    grid_search = GridSearchCV(\n",
    "                        model, param_grids[name], cv=3, \n",
    "                        scoring='r2' if task_type == 'regression' else 'accuracy',\n",
    "                        n_jobs=-1\n",
    "                    )\n",
    "                    grid_search.fit(X_train, y_train)\n",
    "                    best_model = grid_search.best_estimator_\n",
    "                    self.logger.info(f\"    Best params: {grid_search.best_params_}\")\n",
    "                else:\n",
    "                    best_model = model\n",
    "                    best_model.fit(X_train, y_train)\n",
    "                \n",
    "                # Make predictions\n",
    "                train_pred = best_model.predict(X_train)\n",
    "                test_pred = best_model.predict(X_test)\n",
    "                \n",
    "                # Evaluate\n",
    "                if task_type == 'regression':\n",
    "                    train_r2 = r2_score(y_train, train_pred)\n",
    "                    test_r2 = r2_score(y_test, test_pred)\n",
    "                    test_mse = mean_squared_error(y_test, test_pred)\n",
    "                    \n",
    "                    results[name] = {\n",
    "                        'model': best_model,\n",
    "                        'train_r2': train_r2,\n",
    "                        'test_r2': test_r2,\n",
    "                        'test_mse': test_mse,\n",
    "                        'train_pred': train_pred,\n",
    "                        'test_pred': test_pred\n",
    "                    }\n",
    "                    \n",
    "                    self.logger.info(f\"    {name} - R²: {test_r2:.4f}, MSE: {test_mse:.4f}\")\n",
    "                    \n",
    "                else:  # classification\n",
    "                    train_acc = accuracy_score(y_train, train_pred)\n",
    "                    test_acc = accuracy_score(y_test, test_pred)\n",
    "                    \n",
    "                    results[name] = {\n",
    "                        'model': best_model,\n",
    "                        'train_accuracy': train_acc,\n",
    "                        'test_accuracy': test_acc,\n",
    "                        'train_pred': train_pred,\n",
    "                        'test_pred': test_pred\n",
    "                    }\n",
    "                    \n",
    "                    self.logger.info(f\"    {name} - Accuracy: {test_acc:.4f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"    Error training {name}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def create_model_visualizations(self, results, task_type, target_col, X_test, y_test):\n",
    "        \"\"\"Create visualizations for model results\"\"\"\n",
    "        viz_dir = self.output_dir / 'model_results'\n",
    "        viz_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Model comparison\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        fig.suptitle(f'Model Results - {target_col} ({task_type})', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        model_names = list(results.keys())\n",
    "        \n",
    "        if task_type == 'regression':\n",
    "            test_scores = [results[name]['test_r2'] for name in model_names]\n",
    "            metric_name = 'R² Score'\n",
    "        else:\n",
    "            test_scores = [results[name]['test_accuracy'] for name in model_names]\n",
    "            metric_name = 'Accuracy'\n",
    "        \n",
    "        # Performance comparison\n",
    "        bars = axes[0].bar(model_names, test_scores, color='skyblue', edgecolor='navy')\n",
    "        axes[0].set_title(f'Model {metric_name} Comparison')\n",
    "        axes[0].set_ylabel(metric_name)\n",
    "        axes[0].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, score in zip(bars, test_scores):\n",
    "            axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                        f'{score:.3f}', ha='center', va='bottom')\n",
    "        \n",
    "        # Best model predictions vs actual\n",
    "        best_model_name = max(model_names, key=lambda x: test_scores[model_names.index(x)])\n",
    "        best_predictions = results[best_model_name]['test_pred']\n",
    "        \n",
    "        if task_type == 'regression':\n",
    "            axes[1].scatter(y_test, best_predictions, alpha=0.6, color='green')\n",
    "            axes[1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n",
    "                        'r--', linewidth=2, label='Perfect Prediction')\n",
    "            axes[1].set_xlabel('Actual Values')\n",
    "            axes[1].set_ylabel('Predicted Values')\n",
    "            axes[1].legend()\n",
    "        else:\n",
    "            from sklearn.metrics import confusion_matrix\n",
    "            cm = confusion_matrix(y_test, best_predictions)\n",
    "            sns.heatmap(cm, annot=True, fmt='d', ax=axes[1], cmap='Blues')\n",
    "            axes[1].set_xlabel('Predicted')\n",
    "            axes[1].set_ylabel('Actual')\n",
    "        \n",
    "        axes[1].set_title(f'Best Model: {best_model_name}')\n",
    "        axes[1].grid(alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(viz_dir / f'{target_col}_{task_type}_results.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        self.logger.info(f\"✅ Model visualization saved for {target_col}\")\n",
    "    \n",
    "    def save_best_model(self, results, task_type, target_col):\n",
    "        \"\"\"Save the best performing model\"\"\"\n",
    "        if not results:\n",
    "            return\n",
    "        \n",
    "        model_dir = self.output_dir / 'models'\n",
    "        model_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Find best model\n",
    "        if task_type == 'regression':\n",
    "            best_model_name = max(results.keys(), key=lambda x: results[x]['test_r2'])\n",
    "        else:\n",
    "            best_model_name = max(results.keys(), key=lambda x: results[x]['test_accuracy'])\n",
    "        \n",
    "        best_model = results[best_model_name]['model']\n",
    "        \n",
    "        # Save model and preprocessor\n",
    "        model_info = {\n",
    "            'model': best_model,\n",
    "            'preprocessor': self.preprocessor,\n",
    "            'target_encoder': getattr(self, 'target_encoder', None),\n",
    "            'task_type': task_type,\n",
    "            'target_column': target_col,\n",
    "            'best_model_name': best_model_name,\n",
    "            'results': results[best_model_name]\n",
    "        }\n",
    "        \n",
    "        model_file = model_dir / f'{target_col}_{task_type}_best_model.pkl'\n",
    "        joblib.dump(model_info, model_file)\n",
    "        \n",
    "        self.best_models[target_col] = model_info\n",
    "        self.logger.info(f\"✅ Best model saved: {best_model_name} for {target_col}\")\n",
    "    \n",
    "    def generate_summary_report(self):\n",
    "        \"\"\"Generate comprehensive summary report\"\"\"\n",
    "        report_file = self.output_dir / 'summary_report.md'\n",
    "        \n",
    "        with open(report_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(\"# TroTro Unified ML Pipeline - Summary Report\\n\\n\")\n",
    "            f.write(f\"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "            \n",
    "            # Dataset Overview\n",
    "            f.write(\"## Dataset Overview\\n\\n\")\n",
    "            if self.combined_data is not None:\n",
    "                f.write(f\"- **Total Records**: {len(self.combined_data):,}\\n\")\n",
    "                f.write(f\"- **Total Features**: {len(self.combined_data.columns)}\\n\")\n",
    "                f.write(f\"- **Cities Included**: {self.combined_data['city'].nunique()}\\n\")\n",
    "                f.write(f\"- **City List**: {', '.join(self.combined_data['city'].unique())}\\n\\n\")\n",
    "                \n",
    "                # Missing data summary\n",
    "                missing_pct = (self.combined_data.isnull().sum().sum() / \n",
    "                             (len(self.combined_data) * len(self.combined_data.columns))) * 100\n",
    "                f.write(f\"- **Overall Missing Data**: {missing_pct:.2f}%\\n\\n\")\n",
    "            \n",
    "            # Model Results\n",
    "            f.write(\"## Model Results Summary\\n\\n\")\n",
    "            \n",
    "            if self.best_models:\n",
    "                f.write(\"| Target | Task Type | Best Model | Performance |\\n\")\n",
    "                f.write(\"|--------|-----------|------------|-------------|\\n\")\n",
    "                \n",
    "                for target, model_info in self.best_models.items():\n",
    "                    task_type = model_info['task_type']\n",
    "                    best_model = model_info['best_model_name']\n",
    "                    \n",
    "                    if task_type == 'regression':\n",
    "                        score = model_info['results']['test_r2']\n",
    "                        metric = 'R²'\n",
    "                    else:\n",
    "                        score = model_info['results']['test_accuracy']\n",
    "                        metric = 'Accuracy'\n",
    "                    \n",
    "                    f.write(f\"| {target} | {task_type} | {best_model} | {score:.4f} ({metric}) |\\n\")\n",
    "                \n",
    "                f.write(\"\\n\")\n",
    "            \n",
    "            # Key Insights\n",
    "            f.write(\"## Key Insights\\n\\n\")\n",
    "            \n",
    "            if self.best_models:\n",
    "                # Best performing model overall\n",
    "                all_scores = []\n",
    "                for model_info in self.best_models.values():\n",
    "                    if model_info['task_type'] == 'regression':\n",
    "                        all_scores.append((model_info['results']['test_r2'], \n",
    "                                         model_info['best_model_name'], 'regression'))\n",
    "                    else:\n",
    "                        all_scores.append((model_info['results']['test_accuracy'], \n",
    "                                         model_info['best_model_name'], 'classification'))\n",
    "                \n",
    "                if all_scores:\n",
    "                    best_overall = max(all_scores, key=lambda x: x[0])\n",
    "                    f.write(f\"- **Best Overall Performance**: {best_overall[1]} \"\n",
    "                          f\"({best_overall[2]}) with score {best_overall[0]:.4f}\\n\")\n",
    "                \n",
    "                # Algorithm popularity\n",
    "                algorithm_counts = {}\n",
    "                for model_info in self.best_models.values():\n",
    "                    algo = model_info['best_model_name']\n",
    "                    algorithm_counts[algo] = algorithm_counts.get(algo, 0) + 1\n",
    "                \n",
    "                most_successful = max(algorithm_counts.items(), key=lambda x: x[1])\n",
    "                f.write(f\"- **Most Successful Algorithm**: {most_successful[0]} \"\n",
    "                      f\"(won {most_successful[1]} targets)\\n\")\n",
    "            \n",
    "            f.write(\"\\n## Recommendations\\n\\n\")\n",
    "            f.write(\"1. **Data Quality**: Focus on improving data collection consistency across cities\\n\")\n",
    "            f.write(\"2. **Feature Engineering**: Consider creating city-specific features or interactions\\n\")\n",
    "            f.write(\"3. **Model Deployment**: Prioritize models with highest performance for production\\n\")\n",
    "            f.write(\"4. **Monitoring**: Set up performance monitoring for deployed models\\n\\n\")\n",
    "            \n",
    "            f.write(\"---\\n\")\n",
    "            f.write(\"*Report generated by Streamlined TroTro ML Pipeline*\\n\")\n",
    "        \n",
    "        self.logger.info(f\"✅ Summary report saved: {report_file}\")\n",
    "    \n",
    "    def run_complete_pipeline(self):\n",
    "        \"\"\"Run the complete streamlined ML pipeline\"\"\"\n",
    "        print(\"\\n🚀 Starting Streamlined TroTro ML Pipeline\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        try:\n",
    "            # Step 1: Load and combine data\n",
    "            print(\"\\n📊 Step 1: Loading and combining data...\")\n",
    "            if self.load_and_combine_data() is None:\n",
    "                print(\"❌ Failed to load data. Exiting.\")\n",
    "                return\n",
    "            \n",
    "            # Step 2: Visualize dataset\n",
    "            print(\"\\n📈 Step 2: Creating dataset visualizations...\")\n",
    "            self.visualize_dataset()\n",
    "            \n",
    "            # Step 3: Identify ML targets\n",
    "            print(\"\\n🎯 Step 3: Identifying ML targets...\")\n",
    "            targets = self.identify_ml_targets()\n",
    "            \n",
    "            if not targets:\n",
    "                print(\"❌ No ML targets identified. Exiting.\")\n",
    "                return\n",
    "            \n",
    "            # Step 4: Train models for each target\n",
    "            print(f\"\\n🤖 Step 4: Training models for {len(targets)} targets...\")\n",
    "            \n",
    "            for i, (task_type, target_col) in enumerate(targets, 1):\n",
    "                print(f\"\\n  Target {i}/{len(targets)}: {target_col} ({task_type})\")\n",
    "                \n",
    "                # Prepare data\n",
    "                data_result = self.prepare_data(target_col, task_type)\n",
    "                if data_result[0] is None:\n",
    "                    print(f\"    ❌ Data preparation failed for {target_col}\")\n",
    "                    continue\n",
    "                \n",
    "                X_train, X_test, y_train, y_test, features = data_result\n",
    "                \n",
    "                # Train models\n",
    "                results = self.train_models(X_train, X_test, y_train, y_test, task_type, target_col)\n",
    "                \n",
    "                if results:\n",
    "                    # Create visualizations\n",
    "                    self.create_model_visualizations(results, task_type, target_col, X_test, y_test)\n",
    "                    \n",
    "                    # Save best model\n",
    "                    self.save_best_model(results, task_type, target_col)\n",
    "                    \n",
    "                    print(f\"    ✅ Completed {target_col}\")\n",
    "                else:\n",
    "                    print(f\"    ❌ No models trained successfully for {target_col}\")\n",
    "            \n",
    "            # Step 5: Generate report\n",
    "            print(\"\\n📋 Step 5: Generating summary report...\")\n",
    "            self.generate_summary_report()\n",
    "            \n",
    "            # Final summary\n",
    "            print(f\"\\n🎉 Pipeline completed successfully!\")\n",
    "            print(f\"📁 Results saved in: {self.output_dir}\")\n",
    "            print(f\"📊 Visualizations: {self.output_dir}/visualizations/\")\n",
    "            print(f\"🤖 Models: {self.output_dir}/models/\")\n",
    "            print(f\"📈 Model Results: {self.output_dir}/model_results/\")\n",
    "            print(f\"📋 Summary Report: {self.output_dir}/summary_report.md\")\n",
    "            \n",
    "            # Print quick results\n",
    "            if self.best_models:\n",
    "                print(f\"\\n📊 Quick Results Summary:\")\n",
    "                print(\"-\" * 40)\n",
    "                for target, model_info in self.best_models.items():\n",
    "                    task_type = model_info['task_type']\n",
    "                    best_model = model_info['best_model_name']\n",
    "                    \n",
    "                    if task_type == 'regression':\n",
    "                        score = model_info['results']['test_r2']\n",
    "                        metric = 'R²'\n",
    "                    else:\n",
    "                        score = model_info['results']['test_accuracy']\n",
    "                        metric = 'Accuracy'\n",
    "                    \n",
    "                    print(f\"{target:20} | {best_model:15} | {metric}: {score:.4f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Pipeline failed: {e}\")\n",
    "            print(f\"❌ Pipeline failed: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "\n",
    "# Usage and execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Create and run the streamlined pipeline\n",
    "    pipeline = StreamlinedTroTroML()\n",
    "    pipeline.run_complete_pipeline()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
